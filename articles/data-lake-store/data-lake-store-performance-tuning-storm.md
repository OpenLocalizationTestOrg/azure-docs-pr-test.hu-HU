---
title: "aaaAzure Data Lake Store Storm teljesítményének hangolása irányelvek |} Microsoft Docs"
description: "Azure Data Lake Store Storm teljesítményének hangolása irányelvek"
services: data-lake-store
documentationcenter: 
author: stewu
manager: amitkul
editor: stewu
ms.assetid: ebde7b9f-2e51-4d43-b7ab-566417221335
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 12/19/2016
ms.author: stewu
ms.openlocfilehash: 5412fd46cf2373f5877030913df4fe1fc6f5473a
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 10/06/2017
---
# <a name="performance-tuning-guidance-for-storm-on-hdinsight-and-azure-data-lake-store"></a><span data-ttu-id="50f13-103">Útmutató a Storm on HDInsight és az Azure Data Lake Store teljesítményhangolása</span><span class="sxs-lookup"><span data-stu-id="50f13-103">Performance tuning guidance for Storm on HDInsight and Azure Data Lake Store</span></span>

<span data-ttu-id="50f13-104">Ismerje meg, hello tényezőket kell figyelembe venni, amikor egy Azure Storm-topológia hello teljesítményének beállításakor.</span><span class="sxs-lookup"><span data-stu-id="50f13-104">Understand hello factors that should be considered when you tune hello performance of an Azure Storm topology.</span></span> <span data-ttu-id="50f13-105">Például akkor fontos toounderstand hello jellemzői hello által végzett munka hello spoutokkal kapcsolatban és hello boltokhoz (hello munkahelyi-e i/o- vagy memóriaigényes).</span><span class="sxs-lookup"><span data-stu-id="50f13-105">For example, it's important toounderstand hello characteristics of hello work done by hello spouts and hello bolts (whether hello work is I/O or memory intensive).</span></span> <span data-ttu-id="50f13-106">Ez a cikk foglalkozik számos különböző teljesítményének hangolása útmutatást, beleértve a gyakori problémák elhárításához.</span><span class="sxs-lookup"><span data-stu-id="50f13-106">This article covers a range of performance tuning guidelines, including troubleshooting common issues.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="50f13-107">Előfeltételek</span><span class="sxs-lookup"><span data-stu-id="50f13-107">Prerequisites</span></span>

* <span data-ttu-id="50f13-108">**Azure-előfizetés**.</span><span class="sxs-lookup"><span data-stu-id="50f13-108">**An Azure subscription**.</span></span> <span data-ttu-id="50f13-109">Lásd: [Ingyenes Azure-fiók létrehozása](https://azure.microsoft.com/pricing/free-trial/).</span><span class="sxs-lookup"><span data-stu-id="50f13-109">See [Get Azure free trial](https://azure.microsoft.com/pricing/free-trial/).</span></span>
* <span data-ttu-id="50f13-110">**Egy Azure Data Lake Store-fiók**.</span><span class="sxs-lookup"><span data-stu-id="50f13-110">**An Azure Data Lake Store account**.</span></span> <span data-ttu-id="50f13-111">Útmutatást toocreate egy, lásd: [Ismerkedés az Azure Data Lake Store](data-lake-store-get-started-portal.md).</span><span class="sxs-lookup"><span data-stu-id="50f13-111">For instructions on how toocreate one, see [Get started with Azure Data Lake Store](data-lake-store-get-started-portal.md).</span></span>
* <span data-ttu-id="50f13-112">**Egy Azure HDInsight fürt** a Data Lake Store-fiók hozzáférési tooa.</span><span class="sxs-lookup"><span data-stu-id="50f13-112">**An Azure HDInsight cluster** with access tooa Data Lake Store account.</span></span> <span data-ttu-id="50f13-113">Lásd: [HDInsight-fürtök létrehozása a Data Lake Store](data-lake-store-hdinsight-hadoop-use-portal.md).</span><span class="sxs-lookup"><span data-stu-id="50f13-113">See [Create an HDInsight cluster with Data Lake Store](data-lake-store-hdinsight-hadoop-use-portal.md).</span></span> <span data-ttu-id="50f13-114">Ellenőrizze, hogy engedélyezte a távoli asztal hello fürt.</span><span class="sxs-lookup"><span data-stu-id="50f13-114">Make sure you enable Remote Desktop for hello cluster.</span></span>
* <span data-ttu-id="50f13-115">**A Data Lake Store egy Storm-fürt futtató**.</span><span class="sxs-lookup"><span data-stu-id="50f13-115">**Running a Storm cluster on Data Lake Store**.</span></span> <span data-ttu-id="50f13-116">További információkért lásd: [HDInsight alatt futó Storm](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview).</span><span class="sxs-lookup"><span data-stu-id="50f13-116">For more information, see [Storm on HDInsight](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-storm-overview).</span></span>
* <span data-ttu-id="50f13-117">**Teljesítményhangolás irányelvek a Data Lake Store**.</span><span class="sxs-lookup"><span data-stu-id="50f13-117">**Performance tuning guidelines on Data Lake Store**.</span></span>  <span data-ttu-id="50f13-118">Általános teljesítmény fogalmakat, lásd: [Data Lake Store teljesítmény hangolása útmutatást](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance).</span><span class="sxs-lookup"><span data-stu-id="50f13-118">For general performance concepts, see [Data Lake Store Performance Tuning Guidance](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance).</span></span>  

## <a name="tune-hello-parallelism-of-hello-topology"></a><span data-ttu-id="50f13-119">Hello topológia párhuzamosságát hello hangolása</span><span class="sxs-lookup"><span data-stu-id="50f13-119">Tune hello parallelism of hello topology</span></span>

<span data-ttu-id="50f13-120">Előfordulhat, hogy képes tooimprove teljesítmény szerint növekvő hello párhuzamossági az i/o-tooand hello Data Lake Store-ból.</span><span class="sxs-lookup"><span data-stu-id="50f13-120">You might be able tooimprove performance by increasing hello concurrency of hello I/O tooand from Data Lake Store.</span></span> <span data-ttu-id="50f13-121">A Storm-topológia tartozik egy hello párhuzamossági meghatározó konfigurációk:</span><span class="sxs-lookup"><span data-stu-id="50f13-121">A Storm topology has a set of configurations that determine hello parallelism:</span></span>
* <span data-ttu-id="50f13-122">Munkavégző folyamatok (hello munkavállalók egyenlően elosztott virtuális gépek hello) száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-122">Number of worker processes (hello workers are evenly distributed across hello VMs).</span></span>
* <span data-ttu-id="50f13-123">Spout végrehajtó példányok száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-123">Number of spout executor instances.</span></span>
* <span data-ttu-id="50f13-124">Bolt végrehajtó példányok száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-124">Number of bolt executor instances.</span></span>
* <span data-ttu-id="50f13-125">Spout feladatok száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-125">Number of spout tasks.</span></span>
* <span data-ttu-id="50f13-126">Bolt feladatok száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-126">Number of bolt tasks.</span></span>

<span data-ttu-id="50f13-127">Például egy fürtön 4 virtuális gépek és 4 munkavégző folyamatok, 32 spout végrehajtója és 32 spout feladatokat, és 256 bolt végrehajtója és 512 bolt feladatok, vegye figyelembe a következőket hello:</span><span class="sxs-lookup"><span data-stu-id="50f13-127">For example, on a cluster with 4 VMs and 4 worker processes, 32 spout executors and 32 spout tasks, and 256 bolt executors and 512 bolt tasks, consider hello following:</span></span>

<span data-ttu-id="50f13-128">Minden egyes felügyelő, amely a munkavégző csomópont, a Java virtuális gép (JVM) folyamat egyetlen worker rendelkezik.</span><span class="sxs-lookup"><span data-stu-id="50f13-128">Each supervisor, which is a worker node, has a single worker Java virtual machine (JVM) process.</span></span> <span data-ttu-id="50f13-129">A JVM folyamat 4 spout szál és 64 bolt szál kezeli.</span><span class="sxs-lookup"><span data-stu-id="50f13-129">This JVM process manages 4 spout threads and 64 bolt threads.</span></span> <span data-ttu-id="50f13-130">Minden egyes szál belül feladatok egymás után futnak.</span><span class="sxs-lookup"><span data-stu-id="50f13-130">Within each thread, tasks are run sequentially.</span></span> <span data-ttu-id="50f13-131">Konfigurációs megelőző hello minden spout szál 1 tevékenység rendelkezik, és minden egyes bolt szál 2 tevékenységet tartalmaz.</span><span class="sxs-lookup"><span data-stu-id="50f13-131">With hello preceding configuration, each spout thread has 1 task, and each bolt thread has 2 tasks.</span></span>

<span data-ttu-id="50f13-132">A Storm az alábbiakban hello érintett összetevők, és rendelkezik párhuzamossági hello szint hatásuk:</span><span class="sxs-lookup"><span data-stu-id="50f13-132">In Storm, here are hello various components involved, and how they affect hello level of parallelism you have:</span></span>
* <span data-ttu-id="50f13-133">hello átjárócsomópont (a Storm hívott Nimbus) használt toosubmit és feladatok kezelése.</span><span class="sxs-lookup"><span data-stu-id="50f13-133">hello head node (called Nimbus in Storm) is used toosubmit and manage jobs.</span></span> <span data-ttu-id="50f13-134">Ezek a csomópontok befolyásolni párhuzamossági hello fokát.</span><span class="sxs-lookup"><span data-stu-id="50f13-134">These nodes have no impact on hello degree of parallelism.</span></span>
* <span data-ttu-id="50f13-135">hello felügyelő csomópontok.</span><span class="sxs-lookup"><span data-stu-id="50f13-135">hello supervisor nodes.</span></span> <span data-ttu-id="50f13-136">A Hdinsightban Ez megfelel a tooa munkavégző csomópont Azure virtuális Gépen.</span><span class="sxs-lookup"><span data-stu-id="50f13-136">In HDInsight, this corresponds tooa worker node Azure VM.</span></span>
* <span data-ttu-id="50f13-137">hello munkavégző feladatokat az hello virtuális gépeken futó Storm folyamatokat is.</span><span class="sxs-lookup"><span data-stu-id="50f13-137">hello worker tasks are Storm processes running in hello VMs.</span></span> <span data-ttu-id="50f13-138">Minden egyes munkavégző feladat tooa JVM-példány megfelel.</span><span class="sxs-lookup"><span data-stu-id="50f13-138">Each worker task corresponds tooa JVM instance.</span></span> <span data-ttu-id="50f13-139">Storm hello szám osztja el a munkavégző folyamatok adhat meg toohello munkavégző csomópontokhoz lehetőség szerint egyenletes.</span><span class="sxs-lookup"><span data-stu-id="50f13-139">Storm distributes hello number of worker processes you specify toohello worker nodes as evenly as possible.</span></span>
* <span data-ttu-id="50f13-140">Spout és boltok végrehajtó példányok.</span><span class="sxs-lookup"><span data-stu-id="50f13-140">Spout and bolt executor instances.</span></span> <span data-ttu-id="50f13-141">Minden egyes végrehajtó példány hello munkavállalók (JVMs) belül futó tooa szál felel meg.</span><span class="sxs-lookup"><span data-stu-id="50f13-141">Each executor instance corresponds tooa thread running within hello workers (JVMs).</span></span>
* <span data-ttu-id="50f13-142">A Storm feladatok.</span><span class="sxs-lookup"><span data-stu-id="50f13-142">Storm tasks.</span></span> <span data-ttu-id="50f13-143">Ezek a logikai feladatok, hogy minden egyes szálait futtatható.</span><span class="sxs-lookup"><span data-stu-id="50f13-143">These are logical tasks that each of these threads run.</span></span> <span data-ttu-id="50f13-144">Ez nem módosítja hello szintű párhuzamosság, így ki kell értékelnie, hogy több feladat végrehajtója / kell-e.</span><span class="sxs-lookup"><span data-stu-id="50f13-144">This does not change hello level of parallelism, so you should evaluate if you need multiple tasks per executor or not.</span></span>

### <a name="get-hello-best-performance-from-data-lake-store"></a><span data-ttu-id="50f13-145">Hello legjobb teljesítmény eléréséhez a Data Lake Store-ból</span><span class="sxs-lookup"><span data-stu-id="50f13-145">Get hello best performance from Data Lake Store</span></span>

<span data-ttu-id="50f13-146">A Data Lake Store használatakor hello legjobb teljesítmény érdekében elérhetővé Ha hello a következő:</span><span class="sxs-lookup"><span data-stu-id="50f13-146">When working with Data Lake Store, you get hello best performance if you do hello following:</span></span>
* <span data-ttu-id="50f13-147">Nyílt meg kis hozzáfűzi a nagyobb méretű (ideális esetben 4 MB).</span><span class="sxs-lookup"><span data-stu-id="50f13-147">Coalesce your small appends into larger sizes (ideally 4 MB).</span></span>
* <span data-ttu-id="50f13-148">Hajtsa végre, mint a sok egyidejű kéréssel.</span><span class="sxs-lookup"><span data-stu-id="50f13-148">Do as many concurrent requests as you can.</span></span> <span data-ttu-id="50f13-149">Minden egyes bolt szál blokkoló olvasások végez műveletet, mert kívánt toohave valahol 8 – 12 szálak száma core hello tartományán belül.</span><span class="sxs-lookup"><span data-stu-id="50f13-149">Because each bolt thread is doing blocking reads, you want toohave somewhere in hello range of 8-12 threads per core.</span></span> <span data-ttu-id="50f13-150">Így hello hálózati adapter és hello CPU is alkalmazhatja.</span><span class="sxs-lookup"><span data-stu-id="50f13-150">This keeps hello NIC and hello CPU well utilized.</span></span> <span data-ttu-id="50f13-151">A nagyobb virtuális gépek lehetővé teszi több egyidejű kérelmet.</span><span class="sxs-lookup"><span data-stu-id="50f13-151">A larger VM enables more concurrent requests.</span></span>  

### <a name="example-topology"></a><span data-ttu-id="50f13-152">Példa topológia</span><span class="sxs-lookup"><span data-stu-id="50f13-152">Example topology</span></span>

<span data-ttu-id="50f13-153">Tegyük fel, hogy rendelkezik egy 8 munkavégző csomópontot tartalmazó fürtben D13v2 Azure virtuális gépen.</span><span class="sxs-lookup"><span data-stu-id="50f13-153">Let’s assume you have an 8 worker node cluster with a D13v2 Azure VM.</span></span> <span data-ttu-id="50f13-154">A virtuális gép rendelkezik 8 magos, így között hello 8 munkavégző csomópontokhoz, 64 teljes mag van.</span><span class="sxs-lookup"><span data-stu-id="50f13-154">This VM has 8 cores, so among hello 8 worker nodes, you have 64 total cores.</span></span>

<span data-ttu-id="50f13-155">Tegyük fel, 8 bolt szálak száma core végezzük.</span><span class="sxs-lookup"><span data-stu-id="50f13-155">Let’s say we do 8 bolt threads per core.</span></span> <span data-ttu-id="50f13-156">A megadott 64 mag, amely azt jelenti, hogy azt szeretnénk, ha 512 teljes bolt végrehajtó példányok (Ez azt jelenti, hogy a szál).</span><span class="sxs-lookup"><span data-stu-id="50f13-156">Given 64 cores, that means we want 512 total bolt executor instances (that is, threads).</span></span> <span data-ttu-id="50f13-157">Ebben az esetben tegyük fel, azt egy JVM-et virtuális gépenként kezdődnie, és főként a hello szál párhuzamossági hello JVM tooachieve egyidejűségi belül használja.</span><span class="sxs-lookup"><span data-stu-id="50f13-157">In this case, let’s say we start with one JVM per VM, and mainly use hello thread concurrency within hello JVM tooachieve concurrency.</span></span> <span data-ttu-id="50f13-158">Ez azt jelenti, hogy 8 munkavégző feladatokat (egy Azure virtuális gépenként), és 512 bolt végrehajtója kell.</span><span class="sxs-lookup"><span data-stu-id="50f13-158">That means we need 8 worker tasks (one per Azure VM), and 512 bolt executors.</span></span> <span data-ttu-id="50f13-159">Ebben a konfigurációban a Storm megpróbál toodistribute hello munkavállalók egyenletesen munkavégző csomópontokhoz (más néven a felügyelő csomópontok) keresztül jogosultságot ad az egyes feldolgozó csomópontok 1 JVM-et.</span><span class="sxs-lookup"><span data-stu-id="50f13-159">Given this configuration, Storm tries toodistribute hello workers evenly across worker nodes (also known as supervisor nodes), giving each worker node 1 JVM.</span></span> <span data-ttu-id="50f13-160">Most belül hello felügyelők, Storm megpróbál toodistribute hello végrehajtója egyenletesen felügyelők között, minden felügyelő (Ez azt jelenti, hogy JVM) amely 8 szálait egyes.</span><span class="sxs-lookup"><span data-stu-id="50f13-160">Now within hello supervisors, Storm tries toodistribute hello executors evenly between supervisors, giving each supervisor (that is, JVM) 8 threads each.</span></span>

## <a name="tune-additional-parameters"></a><span data-ttu-id="50f13-161">További paraméterek hangolása</span><span class="sxs-lookup"><span data-stu-id="50f13-161">Tune additional parameters</span></span>
<span data-ttu-id="50f13-162">Miután hello alapszintű topológia, érdemes lehet dönthet a tootweak hello paraméterek egyikét:</span><span class="sxs-lookup"><span data-stu-id="50f13-162">After you have hello basic topology, you can consider whether you want tootweak any of hello parameters:</span></span>
* <span data-ttu-id="50f13-163">**Egyes feldolgozó csomópontok JVMs száma.**</span><span class="sxs-lookup"><span data-stu-id="50f13-163">**Number of JVMs per worker node.**</span></span> <span data-ttu-id="50f13-164">Ha nagy adatszerkezet (például egy keresési tábla) a memóriában, minden egyes JVM állomás szükséges egy külön példányát.</span><span class="sxs-lookup"><span data-stu-id="50f13-164">If you have a large data structure (for example, a lookup table) that you host in memory, each JVM requires a separate copy.</span></span> <span data-ttu-id="50f13-165">Azt is megteheti használhatja hello adatszerkezet sok szálakon Ha kevesebb JVMs rendelkezik.</span><span class="sxs-lookup"><span data-stu-id="50f13-165">Alternatively, you can use hello data structure across many threads if you have fewer JVMs.</span></span> <span data-ttu-id="50f13-166">Hello bolt i/o JVMs hello száma tegye az ugyanennyi egy között ezen JVMs hozzáadott szálak számának hello különbség.</span><span class="sxs-lookup"><span data-stu-id="50f13-166">For hello bolt’s I/O, hello number of JVMs does not make as much of a difference as hello number of threads added across those JVMs.</span></span> <span data-ttu-id="50f13-167">Az egyszerűség kedvéért egy jó ötlet toohave egy / munkavégző JVM-et.</span><span class="sxs-lookup"><span data-stu-id="50f13-167">For simplicity, it's a good idea toohave one JVM per worker.</span></span> <span data-ttu-id="50f13-168">Attól függően, hogy milyen módon tartalmaz a bolt vagy milyen alkalmazás feldolgozása az Ön igényel, ha esetleg toochange ezt a számot.</span><span class="sxs-lookup"><span data-stu-id="50f13-168">Depending on what your bolt is doing or what application processing you require, though, you may need toochange this number.</span></span>
* <span data-ttu-id="50f13-169">**Spout végrehajtója száma.**</span><span class="sxs-lookup"><span data-stu-id="50f13-169">**Number of spout executors.**</span></span> <span data-ttu-id="50f13-170">Hello előző példában boltokhoz tooData Lake Store írásra, mert hello spoutokkal kapcsolatban nincs közvetlenül érintett toohello bolt teljesítményét.</span><span class="sxs-lookup"><span data-stu-id="50f13-170">Because hello preceding example uses bolts for writing tooData Lake Store, hello number of spouts is not directly relevant toohello bolt performance.</span></span> <span data-ttu-id="50f13-171">Azonban attól függően, hogy feldolgozás vagy i/o történik a hello spout jó ötlet hello mennyisége tootune hello spoutok a legjobb teljesítmény érdekében.</span><span class="sxs-lookup"><span data-stu-id="50f13-171">However, depending on hello amount of processing or I/O happening in hello spout, it's a good idea tootune hello spouts for best performance.</span></span> <span data-ttu-id="50f13-172">Győződjön meg arról, hogy rendelkezik-e elegendő spoutokkal kapcsolatban toobe képes tookeep hello boltokhoz foglalt.</span><span class="sxs-lookup"><span data-stu-id="50f13-172">Ensure that you have enough spouts toobe able tookeep hello bolts busy.</span></span> <span data-ttu-id="50f13-173">hello kimeneti sebességű hello spoutokkal kapcsolatban meg kell felelnie hello boltokhoz hello átviteli sebességgel.</span><span class="sxs-lookup"><span data-stu-id="50f13-173">hello output rates of hello spouts should match hello throughput of hello bolts.</span></span> <span data-ttu-id="50f13-174">hello tényleges konfiguráció hello spout függ.</span><span class="sxs-lookup"><span data-stu-id="50f13-174">hello actual configuration depends on hello spout.</span></span>
* <span data-ttu-id="50f13-175">**Feladatok száma.**</span><span class="sxs-lookup"><span data-stu-id="50f13-175">**Number of tasks.**</span></span> <span data-ttu-id="50f13-176">Minden egyes bolt egyetlen szálon futtatja.</span><span class="sxs-lookup"><span data-stu-id="50f13-176">Each bolt runs as a single thread.</span></span> <span data-ttu-id="50f13-177">További feladatok / bolt nem ad meg semmilyen további egyidejűségi.</span><span class="sxs-lookup"><span data-stu-id="50f13-177">Additional tasks per bolt don't provide any additional concurrency.</span></span> <span data-ttu-id="50f13-178">hello csak azok előnyös, ha a folyamat hello rekordot igazolása időt vesz igénybe a bolt végrehajtási idő nagy részét.</span><span class="sxs-lookup"><span data-stu-id="50f13-178">hello only time they are of benefit is if your process of acknowledging hello tuple takes a large proportion of your bolt execution time.</span></span> <span data-ttu-id="50f13-179">Egy jó ötlet toogroup sok rekordokat a nagyobb hozzáfűzése a hello bolt nyugtázást elküldése előtt.</span><span class="sxs-lookup"><span data-stu-id="50f13-179">It's a good idea toogroup many tuples into a larger append before you send an acknowledgement from hello bolt.</span></span> <span data-ttu-id="50f13-180">Így a legtöbb esetben több feladat nincs további előnye adja meg.</span><span class="sxs-lookup"><span data-stu-id="50f13-180">So, in most cases, multiple tasks provide no additional benefit.</span></span>
* <span data-ttu-id="50f13-181">**Helyi vagy véletlen csoportosítása.**</span><span class="sxs-lookup"><span data-stu-id="50f13-181">**Local or shuffle grouping.**</span></span> <span data-ttu-id="50f13-182">Ha ez a beállítás engedélyezve van, rekordokat küldött toobolts hello belül azonos munkavégző folyamatot.</span><span class="sxs-lookup"><span data-stu-id="50f13-182">When this setting is enabled, tuples are sent toobolts within hello same worker process.</span></span> <span data-ttu-id="50f13-183">Ez csökkenti a folyamatok közötti kommunikációt és a hálózati hívások.</span><span class="sxs-lookup"><span data-stu-id="50f13-183">This reduces inter-process communication and network calls.</span></span> <span data-ttu-id="50f13-184">Ez a legtöbb topológiák javasolt.</span><span class="sxs-lookup"><span data-stu-id="50f13-184">This is recommended for most topologies.</span></span>

<span data-ttu-id="50f13-185">A alapvető forgatókönyv, az jó kiindulási pont.</span><span class="sxs-lookup"><span data-stu-id="50f13-185">This basic scenario is a good starting point.</span></span> <span data-ttu-id="50f13-186">A saját adatok tootweak hello megelőző paraméterek tooachieve optimális teljesítményének tesztelése</span><span class="sxs-lookup"><span data-stu-id="50f13-186">Test with your own data tootweak hello preceding parameters tooachieve optimal performance.</span></span>

## <a name="tune-hello-spout"></a><span data-ttu-id="50f13-187">Hello spout hangolása</span><span class="sxs-lookup"><span data-stu-id="50f13-187">Tune hello spout</span></span>

<span data-ttu-id="50f13-188">A következő beállítások tootune hello spout hello módosíthatja.</span><span class="sxs-lookup"><span data-stu-id="50f13-188">You can modify hello following settings tootune hello spout.</span></span>

- <span data-ttu-id="50f13-189">**Rekord időtúllépés: topology.message.timeout.secs**.</span><span class="sxs-lookup"><span data-stu-id="50f13-189">**Tuple timeout: topology.message.timeout.secs**.</span></span> <span data-ttu-id="50f13-190">Ez a beállítás hello mennyiségű üzenet toocomplete szükséges idő határozza meg, és fogadni nyugtázása, nem sikertelen volt.</span><span class="sxs-lookup"><span data-stu-id="50f13-190">This setting determines hello amount of time a message takes toocomplete, and receive acknowledgement, before it is considered failed.</span></span>

- <span data-ttu-id="50f13-191">**Munkavégző folyamatok maximális memória: worker.childopts**.</span><span class="sxs-lookup"><span data-stu-id="50f13-191">**Max memory per worker process: worker.childopts**.</span></span> <span data-ttu-id="50f13-192">Ezzel a beállítással adható meg a további parancssori paraméterek toohello Java munkavállalók.</span><span class="sxs-lookup"><span data-stu-id="50f13-192">This setting lets you specify additional command-line parameters toohello Java workers.</span></span> <span data-ttu-id="50f13-193">a beállítás itt leggyakrabban használt hello XmX, amely megadja, hogy hello lefoglalt memória maximális tooa JVM halommemória.</span><span class="sxs-lookup"><span data-stu-id="50f13-193">hello most commonly used setting here is XmX, which determines hello maximum memory allocated tooa JVM’s heap.</span></span>

- <span data-ttu-id="50f13-194">**Függőben lévő maximális spout: topology.max.spout.pending**.</span><span class="sxs-lookup"><span data-stu-id="50f13-194">**Max spout pending: topology.max.spout.pending**.</span></span> <span data-ttu-id="50f13-195">Ez a beállítás meghatározza, hogy rekordokat, amelyek a lehetnek repülési (hello topológia összes csomópontjának, még nem nyugtázott) spout szálankénti bármikor hello száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-195">This setting determines hello number of tuples that can in be flight (not yet acknowledged at all nodes in hello topology) per spout thread at any time.</span></span>

 <span data-ttu-id="50f13-196">Egy jó számítási toodo mérete tooestimate hello egyes a rekordokat.</span><span class="sxs-lookup"><span data-stu-id="50f13-196">A good calculation toodo is tooestimate hello size of each of your tuples.</span></span> <span data-ttu-id="50f13-197">Majd mérje fel, hogy mennyi memória egy spout szál.</span><span class="sxs-lookup"><span data-stu-id="50f13-197">Then figure out how much memory one spout thread has.</span></span> <span data-ttu-id="50f13-198">hello tooa szál, ez az érték osztva lefoglalt teljes memória biztosítani fogja hello maximális spout paraméter függőben lévő hello felső korlátja.</span><span class="sxs-lookup"><span data-stu-id="50f13-198">hello total memory allocated tooa thread, divided by this value, should give you hello upper bound for hello max spout pending parameter.</span></span>

## <a name="tune-hello-bolt"></a><span data-ttu-id="50f13-199">Hello bolt hangolása</span><span class="sxs-lookup"><span data-stu-id="50f13-199">Tune hello bolt</span></span>
<span data-ttu-id="50f13-200">TooData Lake Store ír be, amikor mérete szinkronizálási szabály (puffer hello ügyféloldalon) beállításához too4 MB.</span><span class="sxs-lookup"><span data-stu-id="50f13-200">When you're writing tooData Lake Store, set a size sync policy (buffer on hello client side) too4 MB.</span></span> <span data-ttu-id="50f13-201">A kiürítési vagy hsync() csak akkor, ha hello puffer mérete hello ezt az értéket, majd történik.</span><span class="sxs-lookup"><span data-stu-id="50f13-201">A flushing or hsync() is then performed only when hello buffer size is hello at this value.</span></span> <span data-ttu-id="50f13-202">a virtuális gép hello munkavégző hello Data Lake Store illesztőprogramja automatikusan végzi a pufferelés, kivéve, ha explicit módon hajtsa végre egy hsync().</span><span class="sxs-lookup"><span data-stu-id="50f13-202">hello Data Lake Store driver on hello worker VM automatically does this buffering, unless you explicitly perform an hsync().</span></span>

<span data-ttu-id="50f13-203">hello alapértelmezett Data Lake Store Storm bolt mérete szinkronizálási házirend paraméter tartozik (fileBufferSize), amely használt tootune ezt a paramétert.</span><span class="sxs-lookup"><span data-stu-id="50f13-203">hello default Data Lake Store Storm bolt has a size sync policy parameter (fileBufferSize) that can be used tootune this parameter.</span></span>

<span data-ttu-id="50f13-204">I/O-igényes topológia esetén a jó ötlet toohave minden egyes bolt szál tooits saját fájlt, és tooset egy fájl Elforgatás-házirend írása (fileRotationSize).</span><span class="sxs-lookup"><span data-stu-id="50f13-204">In I/O-intensive topologies, it's a good idea toohave each bolt thread write tooits own file, and tooset a file rotation policy (fileRotationSize).</span></span> <span data-ttu-id="50f13-205">Amikor hello fájl elér egy adott méretet, hello adatfolyam automatikusan ki van ürítve nevével, és egy új fájlt.</span><span class="sxs-lookup"><span data-stu-id="50f13-205">When hello file reaches a certain size, hello stream is automatically flushed and a new file is written to.</span></span> <span data-ttu-id="50f13-206">hello ajánlott Elforgatás mérete 1 GB-os.</span><span class="sxs-lookup"><span data-stu-id="50f13-206">hello recommended file size for rotation is 1 GB.</span></span>

### <a name="handle-tuple-data"></a><span data-ttu-id="50f13-207">Rekord adatok kezelése</span><span class="sxs-lookup"><span data-stu-id="50f13-207">Handle tuple data</span></span>

<span data-ttu-id="50f13-208">A Storm egy spout érvényes tooa rekordot a amíg explicit módon nyugtázta hello bolt.</span><span class="sxs-lookup"><span data-stu-id="50f13-208">In Storm, a spout holds on tooa tuple until it is explicitly acknowledged by hello bolt.</span></span> <span data-ttu-id="50f13-209">Ha egy rekord hello bolt által beolvasása megtörtént, de még nem nyugtázták, hello spout előfordulhat, hogy nem rendelkezik állandóként létrehozni a Data Lake Store háttérből.</span><span class="sxs-lookup"><span data-stu-id="50f13-209">If a tuple has been read by hello bolt but has not been acknowledged yet, hello spout might not have persisted into Data Lake Store back end.</span></span> <span data-ttu-id="50f13-210">A nyugtázott egy rekord, miután hello spout hello bolt a adatmegőrzési garantálható, és is törölje hello forrásadatok bármilyen forrásból történt a olvasásakor.</span><span class="sxs-lookup"><span data-stu-id="50f13-210">After a tuple is acknowledged, hello spout can be guaranteed persistence by hello bolt, and can then delete hello source data from whatever source it is reading from.</span></span>  

<span data-ttu-id="50f13-211">A legjobb teljesítmény elérése érdekében a Data Lake Store rendelkezik hello bolt rekordot adatok 4 MB-os puffer.</span><span class="sxs-lookup"><span data-stu-id="50f13-211">For best performance on Data Lake Store, have hello bolt buffer 4 MB of tuple data.</span></span> <span data-ttu-id="50f13-212">Jegyezze meg egy 4 MB-os írhatóként end vissza a Data Lake Store toohello.</span><span class="sxs-lookup"><span data-stu-id="50f13-212">Then write toohello Data Lake Store back end as one 4-MB write.</span></span> <span data-ttu-id="50f13-213">Hello adatok sikeresen írásbeli toohello tároló után (hívó hflush()) hello bolt is jelenti hello adatok hátsó toohello spout.</span><span class="sxs-lookup"><span data-stu-id="50f13-213">After hello data has been successfully written toohello store (by calling hflush()), hello bolt can acknowledge hello data back toohello spout.</span></span> <span data-ttu-id="50f13-214">Ez az, hogy milyen hello példa boltok esetében megadott ide biztosítja.</span><span class="sxs-lookup"><span data-stu-id="50f13-214">This is what hello example bolt supplied here does.</span></span> <span data-ttu-id="50f13-215">Egyúttal elfogadható toohold előtt hello hflush() kezdeményezték a rekordokat és hello nagyobb számú nyugtázott rekordokat.</span><span class="sxs-lookup"><span data-stu-id="50f13-215">It is also acceptable toohold a larger number of tuples before hello hflush() call is made and hello tuples acknowledged.</span></span> <span data-ttu-id="50f13-216">Azonban ez növeli a felhőszolgáltató közötti átviteléhez hello spout toohold van szüksége, és ezért növekszik hello JVM szükséges memóriamennyiség rekordjainak hello számát.</span><span class="sxs-lookup"><span data-stu-id="50f13-216">However, this increases hello number of tuples in flight that hello spout needs toohold, and therefore increases hello amount of memory required per JVM.</span></span>

> [!NOTE]
<span data-ttu-id="50f13-217">Alkalmazások esetében gyakrabban (a adatok mérete MB-nál kisebb 4) a követelmény tooacknowledge rekordokat más nem teljesítményének javítására szolgál.</span><span class="sxs-lookup"><span data-stu-id="50f13-217">Applications might have a requirement tooacknowledge tuples more frequently (at data sizes less than 4 MB) for other non-performance reasons.</span></span> <span data-ttu-id="50f13-218">Azonban, amely hatással lehet a hello i/o átviteli toohello tároló háttér.</span><span class="sxs-lookup"><span data-stu-id="50f13-218">However, that might affect hello I/O throughput toohello storage back end.</span></span> <span data-ttu-id="50f13-219">Gondosan mérjük ez kompromisszumot hello bolt i/o-teljesítmény ellen.</span><span class="sxs-lookup"><span data-stu-id="50f13-219">Carefully weigh this tradeoff against hello bolt’s I/O performance.</span></span>

<span data-ttu-id="50f13-220">Ha hello beérkezési sebessége rekordokat nem magas, hello 4 MB-os puffer tart egy hosszú ideig toofill, érdemes kiküszöböléséhez ezt úgy:</span><span class="sxs-lookup"><span data-stu-id="50f13-220">If hello incoming rate of tuples is not high, so hello 4-MB buffer takes a long time toofill, consider mitigating this by:</span></span>
* <span data-ttu-id="50f13-221">Szögek hello számának csökkentése, így nincsenek kevesebb pufferek toofill.</span><span class="sxs-lookup"><span data-stu-id="50f13-221">Reducing hello number of bolts, so there are fewer buffers toofill.</span></span>
* <span data-ttu-id="50f13-222">Minden kiürítéseinek x vagy minden y ezredmásodperc rendelkező időalapú vagy count-alapú szabályzat, ahol egy hflush() van elindítva, és eddig halmozott hello rekordokat ismernek vissza.</span><span class="sxs-lookup"><span data-stu-id="50f13-222">Having a time-based or count-based policy, where an hflush() is triggered every x flushes or every y milliseconds, and hello tuples accumulated so far are acknowledged back.</span></span>

<span data-ttu-id="50f13-223">Fontos megjegyezni, hogy hello átviteli ebben az esetben nem éri az események lassú sebesség, maximális átviteli sebesség célja nem hello legnagyobb ennek ellenére is.</span><span class="sxs-lookup"><span data-stu-id="50f13-223">Note that hello throughput in this case is lower, but with a slow rate of events, maximum throughput is not hello biggest objective anyway.</span></span> <span data-ttu-id="50f13-224">Ezek azok mérséklési hello egy rekordot tooflow – toohello tárolják a szükséges teljes idő csökkentése érdekében.</span><span class="sxs-lookup"><span data-stu-id="50f13-224">These mitigations help you reduce hello total time that it takes for a tuple tooflow through toohello store.</span></span> <span data-ttu-id="50f13-225">Ez lehet, hogy számít, ha azt szeretné, hogy még egy kis események száma a valós idejű folyamat.</span><span class="sxs-lookup"><span data-stu-id="50f13-225">This might matter if you want a real-time pipeline even with a low event rate.</span></span> <span data-ttu-id="50f13-226">Is vegye figyelembe, hogy ha a bejövő rekordot aránya alacsony, kell módosíthatja hello topology.message.timeout_secs paraméter, így hello rekordokat nem túllépi az időkorlátot, miközben azok első pufferelt vagy nem dolgozható fel.</span><span class="sxs-lookup"><span data-stu-id="50f13-226">Also note that if your incoming tuple rate is low, you should adjust hello topology.message.timeout_secs parameter, so hello tuples don’t time out while they are getting buffered or processed.</span></span>

## <a name="monitor-your-topology-in-storm"></a><span data-ttu-id="50f13-227">A topológia a Storm figyelése</span><span class="sxs-lookup"><span data-stu-id="50f13-227">Monitor your topology in Storm</span></span>  
<span data-ttu-id="50f13-228">A topológia futása közben, a figyelheti hello Storm felhasználói felületén.</span><span class="sxs-lookup"><span data-stu-id="50f13-228">While your topology is running, you can monitor it in hello Storm user interface.</span></span> <span data-ttu-id="50f13-229">Az alábbiakban hello fő paraméterek toolook::</span><span class="sxs-lookup"><span data-stu-id="50f13-229">Here are hello main parameters toolook at:</span></span>

* <span data-ttu-id="50f13-230">**Teljes folyamat végrehajtása késés.**</span><span class="sxs-lookup"><span data-stu-id="50f13-230">**Total process execution latency.**</span></span> <span data-ttu-id="50f13-231">Ez az hello átlagos idő, egy rekord toobe hello spout által kibocsátott, hello bolt által feldolgozott és nyugtázott vesz igénybe.</span><span class="sxs-lookup"><span data-stu-id="50f13-231">This is hello average time one tuple takes toobe emitted by hello spout, processed by hello bolt, and acknowledged.</span></span>

* <span data-ttu-id="50f13-232">**Teljes bolt folyamat várakozási ideje**</span><span class="sxs-lookup"><span data-stu-id="50f13-232">**Total bolt process latency.**</span></span> <span data-ttu-id="50f13-233">Ez az hello átlagos feldolgozási idő által hello rekordot hello bolt, amíg nem kap nyugtázást.</span><span class="sxs-lookup"><span data-stu-id="50f13-233">This is hello average time spent by hello tuple at hello bolt until it receives an acknowledgement.</span></span>

* <span data-ttu-id="50f13-234">**Teljes bolt késés hajtható végre.**</span><span class="sxs-lookup"><span data-stu-id="50f13-234">**Total bolt execute latency.**</span></span> <span data-ttu-id="50f13-235">Ez a hello átlagos a hello hello bolt által felhasznált idő metódus hajtható végre.</span><span class="sxs-lookup"><span data-stu-id="50f13-235">This is hello average time spent by hello bolt in hello execute method.</span></span>

* <span data-ttu-id="50f13-236">**Hibák száma.**</span><span class="sxs-lookup"><span data-stu-id="50f13-236">**Number of failures.**</span></span> <span data-ttu-id="50f13-237">Ez hivatkozik, amely nem sikerült a teljes feldolgozása előtt azok túllépte az időkorlátot toobe rekordokat toohello száma.</span><span class="sxs-lookup"><span data-stu-id="50f13-237">This refers toohello number of tuples that failed toobe fully processed before they timed out.</span></span>

* <span data-ttu-id="50f13-238">**A kapacitás.**</span><span class="sxs-lookup"><span data-stu-id="50f13-238">**Capacity.**</span></span> <span data-ttu-id="50f13-239">Ez a méri, hogy foglalt a rendszer.</span><span class="sxs-lookup"><span data-stu-id="50f13-239">This is a measure of how busy your system is.</span></span> <span data-ttu-id="50f13-240">Ha ez a szám 1, a boltokhoz gyors a következőkre dolgozik.</span><span class="sxs-lookup"><span data-stu-id="50f13-240">If this number is 1, your bolts are working as fast as they can.</span></span> <span data-ttu-id="50f13-241">Ha kevesebb mint 1, növelje a hello párhuzamosságát.</span><span class="sxs-lookup"><span data-stu-id="50f13-241">If it is less than 1, increase hello parallelism.</span></span> <span data-ttu-id="50f13-242">Ha nagyobb, mint 1, csökkentse a hello párhuzamossági.</span><span class="sxs-lookup"><span data-stu-id="50f13-242">If it is greater than 1, reduce hello parallelism.</span></span>

## <a name="troubleshoot-common-problems"></a><span data-ttu-id="50f13-243">Gyakori hibák elhárítása</span><span class="sxs-lookup"><span data-stu-id="50f13-243">Troubleshoot common problems</span></span>
<span data-ttu-id="50f13-244">Az alábbiakban néhány gyakori hibaelhárítási forgatókönyveket.</span><span class="sxs-lookup"><span data-stu-id="50f13-244">Here are a few common troubleshooting scenarios.</span></span>
* <span data-ttu-id="50f13-245">**Sok listának vannak időtúllépés miatt.** Tekintse meg minden egyes csomópontja hello topológia toodetermine hello szűk esetén.</span><span class="sxs-lookup"><span data-stu-id="50f13-245">**Many tuples are timing out.** Look at each node in hello topology toodetermine where hello bottleneck is.</span></span> <span data-ttu-id="50f13-246">hello leggyakoribb ennek oka, hogy hello boltokhoz nem tud tookeep mentése hello spoutokkal kapcsolatban.</span><span class="sxs-lookup"><span data-stu-id="50f13-246">hello most common reason for this is that hello bolts are not able tookeep up with hello spouts.</span></span> <span data-ttu-id="50f13-247">Ennek eredménye tootuples eltömődés hello belső puffer várakozási toobe feldolgozása közben.</span><span class="sxs-lookup"><span data-stu-id="50f13-247">This leads tootuples clogging hello internal buffers while waiting toobe processed.</span></span> <span data-ttu-id="50f13-248">Vegye figyelembe a hello időtúllépési érték növelésével vagy csökkentésével hello maximális spout függőben.</span><span class="sxs-lookup"><span data-stu-id="50f13-248">Consider increasing hello timeout value or decreasing hello max spout pending.</span></span>

* <span data-ttu-id="50f13-249">**Nincs, a magas teljes folyamat végrehajtása késés, de egy kis bolt folyamat várakozási ideje.**</span><span class="sxs-lookup"><span data-stu-id="50f13-249">**There is a high total process execution latency, but a low bolt process latency.**</span></span> <span data-ttu-id="50f13-250">Ebben az esetben is lehet, hogy hello rekordokat nem alatt ismernek elég gyorsan.</span><span class="sxs-lookup"><span data-stu-id="50f13-250">In this case, it is possible that hello tuples are not being acknowledged fast enough.</span></span> <span data-ttu-id="50f13-251">Ellenőrizze, hogy vannak-e elegendő acknowledgers.</span><span class="sxs-lookup"><span data-stu-id="50f13-251">Check that there are a sufficient number of acknowledgers.</span></span> <span data-ttu-id="50f13-252">Egy másik lehetőség, hogy várnak hello várólistában túl hosszú a hello boltok feldolgozás indítása előtt.</span><span class="sxs-lookup"><span data-stu-id="50f13-252">Another possibility is that they are waiting in hello queue for too long before hello bolts start processing them.</span></span> <span data-ttu-id="50f13-253">Hello maximális spout függőben lévő csökkentése.</span><span class="sxs-lookup"><span data-stu-id="50f13-253">Decrease hello max spout pending.</span></span>

* <span data-ttu-id="50f13-254">**Nincs a magas bolt késés hajtható végre.**</span><span class="sxs-lookup"><span data-stu-id="50f13-254">**There is a high bolt execute latency.**</span></span> <span data-ttu-id="50f13-255">Ez azt jelenti, hogy a bolt metódusában hello az execute() metódus túl sokáig tart.</span><span class="sxs-lookup"><span data-stu-id="50f13-255">This means that hello execute() method of your bolt is taking too long.</span></span> <span data-ttu-id="50f13-256">Optimalizálni hello kódot, vagy írási méretek tekintse meg, és ürítse ki a viselkedést.</span><span class="sxs-lookup"><span data-stu-id="50f13-256">Optimize hello code, or look at write sizes and flush behavior.</span></span>

### <a name="data-lake-store-throttling"></a><span data-ttu-id="50f13-257">Data Lake Store-szabályozás</span><span class="sxs-lookup"><span data-stu-id="50f13-257">Data Lake Store throttling</span></span>
<span data-ttu-id="50f13-258">Kattintson a Data Lake Store által biztosított sávszélesség hello korlátai, ha a feladat sikertelen jelenhet meg.</span><span class="sxs-lookup"><span data-stu-id="50f13-258">If you hit hello limits of bandwidth provided by Data Lake Store, you might see task failures.</span></span> <span data-ttu-id="50f13-259">Tekintse meg a feladat naplókat hibák szabályozás.</span><span class="sxs-lookup"><span data-stu-id="50f13-259">Check task logs for throttling errors.</span></span> <span data-ttu-id="50f13-260">Hello párhuzamossági tároló méretének növelésével csökkenthető.</span><span class="sxs-lookup"><span data-stu-id="50f13-260">You can decrease hello parallelism by increasing container size.</span></span>    

<span data-ttu-id="50f13-261">Ha Ön első szabályozott, toocheck hello hibakeresési naplózás hello ügyféloldalon engedélyezése:</span><span class="sxs-lookup"><span data-stu-id="50f13-261">toocheck if you are getting throttled, enable hello debug logging on hello client side:</span></span>

1. <span data-ttu-id="50f13-262">A **Ambari** > **Storm** > **Config** > **storm-worker-log4j speciális**, módosítása  **&lt;gyökér szintű = "Infó"&gt;**  túl**&lt;gyökér szintű = "debug"&gt;**.</span><span class="sxs-lookup"><span data-stu-id="50f13-262">In **Ambari** > **Storm** > **Config** > **Advanced storm-worker-log4j**, change **&lt;root level="info"&gt;** too**&lt;root level=”debug”&gt;**.</span></span> <span data-ttu-id="50f13-263">Indítsa újra az összes hello csomópontok/szolgáltatást hello konfigurációs tootake hatást.</span><span class="sxs-lookup"><span data-stu-id="50f13-263">Restart all hello nodes/service for hello configuration tootake effect.</span></span>
2. <span data-ttu-id="50f13-264">A figyelő hello Storm-topológia bejelentkezik a feldolgozó csomópontok (alatt /var/log/storm/worker-artifacts /&lt;TopologyName&gt;/&lt;port&gt;/worker.log) a Data Lake Store szabályozási kivételeket.</span><span class="sxs-lookup"><span data-stu-id="50f13-264">Monitor hello Storm topology logs on worker nodes (under /var/log/storm/worker-artifacts/&lt;TopologyName&gt;/&lt;port&gt;/worker.log) for Data Lake Store throttling exceptions.</span></span>

## <a name="next-steps"></a><span data-ttu-id="50f13-265">Következő lépések</span><span class="sxs-lookup"><span data-stu-id="50f13-265">Next steps</span></span>
<span data-ttu-id="50f13-266">További teljesítményhangolás, a Storm hivatkozható [ebben a blogban](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/).</span><span class="sxs-lookup"><span data-stu-id="50f13-266">Additional performance tuning for Storm can be referenced in [this blog](https://blogs.msdn.microsoft.com/shanyu/2015/05/14/performance-tuning-for-hdinsight-storm-and-microsoft-azure-eventhubs/).</span></span>

<span data-ttu-id="50f13-267">Egy további példa toorun, lásd: [erre a Githubon](https://github.com/hdinsight/storm-performance-automation).</span><span class="sxs-lookup"><span data-stu-id="50f13-267">For an additional example toorun, see [this one on GitHub](https://github.com/hdinsight/storm-performance-automation).</span></span>
