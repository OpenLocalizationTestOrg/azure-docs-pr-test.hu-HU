---
title: "Application Insights naplóinak elemzése a Spark - Azure HDInsight |} Microsoft Docs"
description: "Megtudhatja, hogyan exportálják a blob-tároló, és a naplóinak majd elemzése a Spark on HDInsight az Application Insights-naplókat."
services: hdinsight
documentationcenter: 
author: Blackmist
manager: jhubbard
editor: cgronlun
ms.assetid: 883beae6-9839-45b5-94f7-7eb0f4534ad5
ms.service: hdinsight
ms.custom: hdinsightactive
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 08/15/2017
ms.author: larryfr
ms.openlocfilehash: d98e403683618ef6115372f99e4949af87af4490
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 08/18/2017
---
# <a name="analyze-application-insights-telemetry-logs-with-spark-on-hdinsight"></a><span data-ttu-id="6e547-103">A Spark on HDInsight az Application Insights telemetria naplóinak elemzése</span><span class="sxs-lookup"><span data-stu-id="6e547-103">Analyze Application Insights telemetry logs with Spark on HDInsight</span></span>

<span data-ttu-id="6e547-104">Megtudhatja, hogyan válassza a Spark on HDInsight Application Insights telemetriai adatok elemzését.</span><span class="sxs-lookup"><span data-stu-id="6e547-104">Learn how to use Spark on HDInsight to analyze Application Insight telemetry data.</span></span>

<span data-ttu-id="6e547-105">[A Visual Studio Application Insights](../application-insights/app-insights-overview.md) analytics szolgáltatás, amely figyeli a webes alkalmazások.</span><span class="sxs-lookup"><span data-stu-id="6e547-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) is an analytics service that monitors your web applications.</span></span> <span data-ttu-id="6e547-106">Az Application Insights által létrehozott telemetriai adatok Azure Storage exportálhatja.</span><span class="sxs-lookup"><span data-stu-id="6e547-106">Telemetry data generated by Application Insights can be exported to Azure Storage.</span></span> <span data-ttu-id="6e547-107">Ha az adatokat az Azure Storage, HDInsight segítségével elemezze.</span><span class="sxs-lookup"><span data-stu-id="6e547-107">Once the data is in Azure Storage, HDInsight can be used to analyze it.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="6e547-108">Előfeltételek</span><span class="sxs-lookup"><span data-stu-id="6e547-108">Prerequisites</span></span>

* <span data-ttu-id="6e547-109">Egy alkalmazás, amely az Application Insights használatára van konfigurálva.</span><span class="sxs-lookup"><span data-stu-id="6e547-109">An application that is configured to use Application Insights.</span></span>

* <span data-ttu-id="6e547-110">Linux-alapú HDInsight-fürt létrehozása ismeretét.</span><span class="sxs-lookup"><span data-stu-id="6e547-110">Familiarity with creating a Linux-based HDInsight cluster.</span></span> <span data-ttu-id="6e547-111">További információkért lásd: [hozzon létre a Spark on HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="6e547-111">For more information, see [Create Spark on HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

  > [!IMPORTANT]
  > <span data-ttu-id="6e547-112">A jelen dokumentumban leírt lépések egy HDInsight-fürt által használt Linux igényelnek.</span><span class="sxs-lookup"><span data-stu-id="6e547-112">The steps in this document require an HDInsight cluster that uses Linux.</span></span> <span data-ttu-id="6e547-113">A Linux az egyetlen operációs rendszer, amely a HDInsight 3.4-es vagy újabb verziói esetében használható.</span><span class="sxs-lookup"><span data-stu-id="6e547-113">Linux is the only operating system used on HDInsight version 3.4 or greater.</span></span> <span data-ttu-id="6e547-114">További tudnivalókért lásd: [A HDInsight elavulása Windows rendszeren](hdinsight-component-versioning.md#hdinsight-windows-retirement).</span><span class="sxs-lookup"><span data-stu-id="6e547-114">For more information, see [HDInsight retirement on Windows](hdinsight-component-versioning.md#hdinsight-windows-retirement).</span></span>

* <span data-ttu-id="6e547-115">Egy webböngészőben.</span><span class="sxs-lookup"><span data-stu-id="6e547-115">A web browser.</span></span>

<span data-ttu-id="6e547-116">A következő források a fejlesztés és tesztelés Ez a dokumentum használták:</span><span class="sxs-lookup"><span data-stu-id="6e547-116">The following resources were used in developing and testing this document:</span></span>

* <span data-ttu-id="6e547-117">Application Insights telemetria adatok generálta egy [Application Insights használatára konfigurált Node.js webalkalmazás](../application-insights/app-insights-nodejs.md).</span><span class="sxs-lookup"><span data-stu-id="6e547-117">Application Insights telemetry data was generated using a [Node.js web app configured to use Application Insights](../application-insights/app-insights-nodejs.md).</span></span>

* <span data-ttu-id="6e547-118">A Linux-alapú a Spark on HDInsight-fürt verziószáma 3.5 használatával elemezheti az adatokat.</span><span class="sxs-lookup"><span data-stu-id="6e547-118">A Linux-based Spark on HDInsight cluster version 3.5 was used to analyze the data.</span></span>

## <a name="architecture-and-planning"></a><span data-ttu-id="6e547-119">Architektúra és tervezése</span><span class="sxs-lookup"><span data-stu-id="6e547-119">Architecture and planning</span></span>

<span data-ttu-id="6e547-120">A következő diagram azt ábrázolja, ebben a példában a service-architektúra:</span><span class="sxs-lookup"><span data-stu-id="6e547-120">The following diagram illustrates the service architecture of this example:</span></span>

![a blob storage az Application Insights áramló adatok jelennek meg, majd a Spark on HDInsight által feldolgozott diagramja](./media/hdinsight-spark-analyze-application-insight-logs/appinsightshdinsight.png)

### <a name="azure-storage"></a><span data-ttu-id="6e547-122">Azure Storage</span><span class="sxs-lookup"><span data-stu-id="6e547-122">Azure storage</span></span>

<span data-ttu-id="6e547-123">Az Application Insights beállítható úgy, hogy folyamatosan telemetriai adatainak exportálása blobokat.</span><span class="sxs-lookup"><span data-stu-id="6e547-123">Application Insights can be configured to continuously export telemetry information to blobs.</span></span> <span data-ttu-id="6e547-124">HDInsight majd elolvashatják a blobok tárolt adatokat.</span><span class="sxs-lookup"><span data-stu-id="6e547-124">HDInsight can then read data stored in the blobs.</span></span> <span data-ttu-id="6e547-125">Vannak azonban olyan követelményekkel, amelyeket kell követnie:</span><span class="sxs-lookup"><span data-stu-id="6e547-125">However, there are some requirements that you must follow:</span></span>

* <span data-ttu-id="6e547-126">**Hely**: Ha a Tárfiók és a HDInsight különböző helyeken vannak, megnövelheti késés.</span><span class="sxs-lookup"><span data-stu-id="6e547-126">**Location**: If the Storage Account and HDInsight are in different locations, it may increase latency.</span></span> <span data-ttu-id="6e547-127">Kimenő forgalom díjak adatok régiók közötti áthelyezése is vonatkozik, költség, is növeli.</span><span class="sxs-lookup"><span data-stu-id="6e547-127">It also increases cost, as egress charges are applied to data moving between regions.</span></span>

    > [!WARNING]
    > <span data-ttu-id="6e547-128">A Storage-fiók egy másik helyen, mint a HDInsight használata nem támogatott.</span><span class="sxs-lookup"><span data-stu-id="6e547-128">Using a Storage Account in a different location than HDInsight is not supported.</span></span>

* <span data-ttu-id="6e547-129">**BLOB-típusú**: HDInsight csak blokk blobokat támogat.</span><span class="sxs-lookup"><span data-stu-id="6e547-129">**Blob type**: HDInsight only supports block blobs.</span></span> <span data-ttu-id="6e547-130">Alkalmazás Insights alapértelmezés szerint használja a blokkblobok, úgy kell működnie alapértelmezés szerint a HDInsight.</span><span class="sxs-lookup"><span data-stu-id="6e547-130">Application Insights defaults to using block blobs, so should work by default with HDInsight.</span></span>

<span data-ttu-id="6e547-131">A további tárhely hozzáadása egy meglévő HDInsight-fürt információkért lásd: a [adja hozzá a további tárfiókok](hdinsight-hadoop-add-storage.md) dokumentum.</span><span class="sxs-lookup"><span data-stu-id="6e547-131">For information on adding additional storage to an existing HDInsight cluster, see the [Add additional storage accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

### <a name="data-schema"></a><span data-ttu-id="6e547-132">Adatséma</span><span class="sxs-lookup"><span data-stu-id="6e547-132">Data schema</span></span>

<span data-ttu-id="6e547-133">Az Application Insights biztosít [exportálja az adatokat az adatmodellbe](../application-insights/app-insights-export-data-model.md) blobok exportálják a telemetriai adatok formátuma.</span><span class="sxs-lookup"><span data-stu-id="6e547-133">Application Insights provides [export data model](../application-insights/app-insights-export-data-model.md) information for the telemetry data format exported to blobs.</span></span> <span data-ttu-id="6e547-134">A jelen dokumentumban leírt lépések Spark SQL használatával az adatokat.</span><span class="sxs-lookup"><span data-stu-id="6e547-134">The steps in this document use Spark SQL to work with the data.</span></span> <span data-ttu-id="6e547-135">Spark SQL automatikusan hozhat létre a JSON-adatszerkezet az Application Insights által naplózott sémát.</span><span class="sxs-lookup"><span data-stu-id="6e547-135">Spark SQL can automatically generate a schema for the JSON data structure logged by Application Insights.</span></span>

## <a name="export-telemetry-data"></a><span data-ttu-id="6e547-136">Telemetriai adatok exportálása</span><span class="sxs-lookup"><span data-stu-id="6e547-136">Export telemetry data</span></span>

<span data-ttu-id="6e547-137">Kövesse a [konfigurálása a folyamatos exportálás](../application-insights/app-insights-export-telemetry.md) konfigurálása az Application Insights telemetria adatok exportálása egy Azure storage-blobba.</span><span class="sxs-lookup"><span data-stu-id="6e547-137">Follow the steps in [Configure Continuous Export](../application-insights/app-insights-export-telemetry.md) to configure your Application Insights to export telemetry information to an Azure storage blob.</span></span>

## <a name="configure-hdinsight-to-access-the-data"></a><span data-ttu-id="6e547-138">Az adatok eléréséhez HDInsight konfigurálása</span><span class="sxs-lookup"><span data-stu-id="6e547-138">Configure HDInsight to access the data</span></span>

<span data-ttu-id="6e547-139">HDInsight-fürtöt hoz létre, ha a tárfiók hozzáadása a fürt létrehozása során.</span><span class="sxs-lookup"><span data-stu-id="6e547-139">If you are creating an HDInsight cluster, add the storage account during cluster creation.</span></span>

<span data-ttu-id="6e547-140">Az Azure Storage-fiók hozzáadása egy meglévő fürthöz, olvassa el a a [adja hozzá a további Tárfiókok](hdinsight-hadoop-add-storage.md) dokumentum.</span><span class="sxs-lookup"><span data-stu-id="6e547-140">To add the Azure Storage Account to an existing cluster, use the information in the [Add additional Storage Accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

## <a name="analyze-the-data-pyspark"></a><span data-ttu-id="6e547-141">Az adatok elemzése: PySpark</span><span class="sxs-lookup"><span data-stu-id="6e547-141">Analyze the data: PySpark</span></span>

1. <span data-ttu-id="6e547-142">Az a [Azure-portálon](https://portal.azure.com), válassza ki a Spark on HDInsight-fürt.</span><span class="sxs-lookup"><span data-stu-id="6e547-142">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="6e547-143">Az a **Gyorshivatkozások** szakaszban jelölje be **fürt irányítópultok**, majd válassza ki **Jupyter Notebook** a fürt Dashboard__ paneljéről.</span><span class="sxs-lookup"><span data-stu-id="6e547-143">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![A fürt irányítópultok](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)

2. <span data-ttu-id="6e547-145">Válassza ki a Jupyter oldal jobb felső sarkában **új**, majd **PySpark**.</span><span class="sxs-lookup"><span data-stu-id="6e547-145">In the upper right corner of the Jupyter page, select **New**, and then **PySpark**.</span></span> <span data-ttu-id="6e547-146">A Python-alapú Jupyter Notebook tartalmazó új böngészőlapon nyílik meg.</span><span class="sxs-lookup"><span data-stu-id="6e547-146">A new browser tab containing a Python-based Jupyter Notebook opens.</span></span>

3. <span data-ttu-id="6e547-147">Az első mezőben (nevű egy **cella**) lapon adja meg a következő szöveget:</span><span class="sxs-lookup"><span data-stu-id="6e547-147">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```python
   sc._jsc.hadoopConfiguration().set('mapreduce.input.fileinputformat.input.dir.recursive', 'true')
   ```

    <span data-ttu-id="6e547-148">Ez a kód Spark rekurzív módon hozzáférését a könyvtárstruktúra a bemeneti adatok konfigurálja.</span><span class="sxs-lookup"><span data-stu-id="6e547-148">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="6e547-149">Application Insights telemetria kerül a hasonló könyvtárszerkezete a `/{telemetry type}/YYYY-MM-DD/{##}/`.</span><span class="sxs-lookup"><span data-stu-id="6e547-149">Application Insights telemetry is logged to a directory structure similar to the `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="6e547-150">Használjon **SHIFT + ENTER** futtatja a kódot.</span><span class="sxs-lookup"><span data-stu-id="6e547-150">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="6e547-151">A cella bal oldalán egy "\*" annak jelzésére, hogy ezt a cellát a kód végrehajtott zárójelek között jelenik meg.</span><span class="sxs-lookup"><span data-stu-id="6e547-151">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="6e547-152">Miután befejeződött, a "\*" módosítását egy számot, és a kimenet az alábbihoz hasonló a cella alább látható:</span><span class="sxs-lookup"><span data-stu-id="6e547-152">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    pyspark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="6e547-153">Egy új cella alatt az elsőt jön létre.</span><span class="sxs-lookup"><span data-stu-id="6e547-153">A new cell is created below the first one.</span></span> <span data-ttu-id="6e547-154">Adja meg a következő szöveget a új cellára.</span><span class="sxs-lookup"><span data-stu-id="6e547-154">Enter the following text in the new cell.</span></span> <span data-ttu-id="6e547-155">Cserélje le `CONTAINER` és `STORAGEACCOUNT` az Azure storage-fiók nevét és az Application Insights-adatokat tartalmazó blob-tároló neve.</span><span class="sxs-lookup"><span data-stu-id="6e547-155">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights data.</span></span>

   ```python
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="6e547-156">Használjon **SHIFT + ENTER** végrehajtani ezt a cellát.</span><span class="sxs-lookup"><span data-stu-id="6e547-156">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="6e547-157">Az alábbi hasonló eredményt látja:</span><span class="sxs-lookup"><span data-stu-id="6e547-157">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="6e547-158">A wasb visszaadott elérési út az Application Insights telemetria adatok helyét.</span><span class="sxs-lookup"><span data-stu-id="6e547-158">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="6e547-159">Módosítsa a `hdfs dfs -ls` visszaadott wasb elérési utat használja a cellában. sor, és ezután **SHIFT + ENTER** újra futtatni a cella.</span><span class="sxs-lookup"><span data-stu-id="6e547-159">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="6e547-160">Ebben az esetben az eredmények megjelenjen-e a telemetriai adatokat tartalmazó könyvtárak.</span><span class="sxs-lookup"><span data-stu-id="6e547-160">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="6e547-161">Az ebben a szakaszban a többi a `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory lett megadva.</span><span class="sxs-lookup"><span data-stu-id="6e547-161">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="6e547-162">Lehet, hogy a könyvtárstruktúra különböző.</span><span class="sxs-lookup"><span data-stu-id="6e547-162">Your directory structure may be different.</span></span>

6. <span data-ttu-id="6e547-163">A következő cellában, írja be a következő kódot: cserélje le `WASB_PATH` az előző lépésben elérési úttal.</span><span class="sxs-lookup"><span data-stu-id="6e547-163">In the next cell, enter the following code: Replace `WASB_PATH` with the path from the previous step.</span></span>

   ```python
   jsonFiles = sc.textFile('WASB_PATH')
   jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="6e547-164">Ez a kód egy dataframe a folyamatos exportálás folyamat által exportált JSON-fájlokat hoz létre.</span><span class="sxs-lookup"><span data-stu-id="6e547-164">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="6e547-165">Használjon **SHIFT + ENTER** ezt a cellát futtatásához.</span><span class="sxs-lookup"><span data-stu-id="6e547-165">Use **SHIFT+ENTER** to run this cell.</span></span>
7. <span data-ttu-id="6e547-166">A következő cellában adja meg, és futtassa a következő, a séma, Spark hozott létre a JSON-fájlok megtekintéséhez:</span><span class="sxs-lookup"><span data-stu-id="6e547-166">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```python
   jsonData.printSchema()
   ```

    <span data-ttu-id="6e547-167">Az egyes telemetriai adatokat a séma nem egyezik.</span><span class="sxs-lookup"><span data-stu-id="6e547-167">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="6e547-168">A következő példa egy a webes kérelmek az létrehozott séma (tárolt adatok a `Requests` alkönyvtár):</span><span class="sxs-lookup"><span data-stu-id="6e547-168">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)
8. <span data-ttu-id="6e547-169">Használja a következő ideiglenes táblából a dataframe regisztrálásához és a lekérdezés futtatása az adatok alapján:</span><span class="sxs-lookup"><span data-stu-id="6e547-169">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```python
   jsonData.registerTempTable("requests")
   df = sqlContext.sql("select context.location.city from requests where context.location.city is not null")
   df.show()
   ```

    <span data-ttu-id="6e547-170">Ez a lekérdezés város olyan információkat ad vissza a felső 20 rekordok ahol context.location.city értéke nem null.</span><span class="sxs-lookup"><span data-stu-id="6e547-170">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="6e547-171">A környezet struktúra megtalálható-e az Application Insights által naplózott összes telemetriai adat.</span><span class="sxs-lookup"><span data-stu-id="6e547-171">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="6e547-172">A város elem nem lehet megadni a naplókban.</span><span class="sxs-lookup"><span data-stu-id="6e547-172">The city element may not be populated in your logs.</span></span> <span data-ttu-id="6e547-173">A séma segítségével azonosíthatók a más elemeket lekérdezhető a naplók adatok szerepelhetnek.</span><span class="sxs-lookup"><span data-stu-id="6e547-173">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>

    <span data-ttu-id="6e547-174">A lekérdezés által visszaadott adatokat az alábbihoz hasonló:</span><span class="sxs-lookup"><span data-stu-id="6e547-174">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="analyze-the-data-scala"></a><span data-ttu-id="6e547-175">Az adatok elemzése: Scala</span><span class="sxs-lookup"><span data-stu-id="6e547-175">Analyze the data: Scala</span></span>

1. <span data-ttu-id="6e547-176">Az a [Azure-portálon](https://portal.azure.com), válassza ki a Spark on HDInsight-fürt.</span><span class="sxs-lookup"><span data-stu-id="6e547-176">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="6e547-177">Az a **Gyorshivatkozások** szakaszban jelölje be **fürt irányítópultok**, majd válassza ki **Jupyter Notebook** a fürt Dashboard__ paneljéről.</span><span class="sxs-lookup"><span data-stu-id="6e547-177">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![A fürt irányítópultok](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)
2. <span data-ttu-id="6e547-179">Válassza ki a Jupyter oldal jobb felső sarkában **új**, majd **Scala**.</span><span class="sxs-lookup"><span data-stu-id="6e547-179">In the upper right corner of the Jupyter page, select **New**, and then **Scala**.</span></span> <span data-ttu-id="6e547-180">Scala-alapú Jupyter Notebook tartalmazó új böngészőlapon jelenik meg.</span><span class="sxs-lookup"><span data-stu-id="6e547-180">A new browser tab containing a Scala-based Jupyter Notebook appears.</span></span>
3. <span data-ttu-id="6e547-181">Az első mezőben (nevű egy **cella**) lapon adja meg a következő szöveget:</span><span class="sxs-lookup"><span data-stu-id="6e547-181">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```scala
   sc.hadoopConfiguration.set("mapreduce.input.fileinputformat.input.dir.recursive", "true")
   ```

    <span data-ttu-id="6e547-182">Ez a kód Spark rekurzív módon hozzáférését a könyvtárstruktúra a bemeneti adatok konfigurálja.</span><span class="sxs-lookup"><span data-stu-id="6e547-182">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="6e547-183">Application Insights telemetria kerül a hasonló könyvtárstruktúra `/{telemetry type}/YYYY-MM-DD/{##}/`.</span><span class="sxs-lookup"><span data-stu-id="6e547-183">Application Insights telemetry is logged to a directory structure similar to `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="6e547-184">Használjon **SHIFT + ENTER** futtatja a kódot.</span><span class="sxs-lookup"><span data-stu-id="6e547-184">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="6e547-185">A cella bal oldalán egy "\*" annak jelzésére, hogy ezt a cellát a kód végrehajtott zárójelek között jelenik meg.</span><span class="sxs-lookup"><span data-stu-id="6e547-185">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="6e547-186">Miután befejeződött, a "\*" módosítását egy számot, és a kimenet az alábbihoz hasonló a cella alább látható:</span><span class="sxs-lookup"><span data-stu-id="6e547-186">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    spark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="6e547-187">Egy új cella alatt az elsőt jön létre.</span><span class="sxs-lookup"><span data-stu-id="6e547-187">A new cell is created below the first one.</span></span> <span data-ttu-id="6e547-188">Adja meg a következő szöveget a új cellára.</span><span class="sxs-lookup"><span data-stu-id="6e547-188">Enter the following text in the new cell.</span></span> <span data-ttu-id="6e547-189">Cserélje le `CONTAINER` és `STORAGEACCOUNT` naplózza az Azure storage-fiók nevét és az Application Insights tartalmazó blob-tároló neve.</span><span class="sxs-lookup"><span data-stu-id="6e547-189">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights logs.</span></span>

   ```scala
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="6e547-190">Használjon **SHIFT + ENTER** végrehajtani ezt a cellát.</span><span class="sxs-lookup"><span data-stu-id="6e547-190">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="6e547-191">Az alábbi hasonló eredményt látja:</span><span class="sxs-lookup"><span data-stu-id="6e547-191">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="6e547-192">A wasb visszaadott elérési út az Application Insights telemetria adatok helyét.</span><span class="sxs-lookup"><span data-stu-id="6e547-192">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="6e547-193">Módosítsa a `hdfs dfs -ls` visszaadott wasb elérési utat használja a cellában. sor, és ezután **SHIFT + ENTER** újra futtatni a cella.</span><span class="sxs-lookup"><span data-stu-id="6e547-193">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="6e547-194">Ebben az esetben az eredmények megjelenjen-e a telemetriai adatokat tartalmazó könyvtárak.</span><span class="sxs-lookup"><span data-stu-id="6e547-194">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="6e547-195">Az ebben a szakaszban a többi a `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory lett megadva.</span><span class="sxs-lookup"><span data-stu-id="6e547-195">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="6e547-196">Ez a könyvtár nem létezik, kivéve, ha a telemetriai adatok egy webalkalmazás.</span><span class="sxs-lookup"><span data-stu-id="6e547-196">This directory may not exist unless your telemetry data is for a web app.</span></span>

6. <span data-ttu-id="6e547-197">A következő cellában, írja be a következő kódot: cserélje le `WASB\_PATH` az előző lépésben elérési úttal.</span><span class="sxs-lookup"><span data-stu-id="6e547-197">In the next cell, enter the following code: Replace `WASB\_PATH` with the path from the previous step.</span></span>

   ```scala
   var jsonFiles = sc.textFile('WASB_PATH')
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   var jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="6e547-198">Ez a kód egy dataframe a folyamatos exportálás folyamat által exportált JSON-fájlokat hoz létre.</span><span class="sxs-lookup"><span data-stu-id="6e547-198">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="6e547-199">Használjon **SHIFT + ENTER** ezt a cellát futtatásához.</span><span class="sxs-lookup"><span data-stu-id="6e547-199">Use **SHIFT+ENTER** to run this cell.</span></span>

7. <span data-ttu-id="6e547-200">A következő cellában adja meg, és futtassa a következő, a séma, Spark hozott létre a JSON-fájlok megtekintéséhez:</span><span class="sxs-lookup"><span data-stu-id="6e547-200">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```scala
   jsonData.printSchema
   ```

    <span data-ttu-id="6e547-201">Az egyes telemetriai adatokat a séma nem egyezik.</span><span class="sxs-lookup"><span data-stu-id="6e547-201">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="6e547-202">A következő példa egy a webes kérelmek az létrehozott séma (tárolt adatok a `Requests` alkönyvtár):</span><span class="sxs-lookup"><span data-stu-id="6e547-202">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)

8. <span data-ttu-id="6e547-203">Használja a következő ideiglenes táblából a dataframe regisztrálásához és a lekérdezés futtatása az adatok alapján:</span><span class="sxs-lookup"><span data-stu-id="6e547-203">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```scala
   jsonData.registerTempTable("requests")
   var city = sqlContext.sql("select context.location.city from requests where context.location.city is not null limit 10").show()
   ```

    <span data-ttu-id="6e547-204">Ez a lekérdezés város olyan információkat ad vissza a felső 20 rekordok ahol context.location.city értéke nem null.</span><span class="sxs-lookup"><span data-stu-id="6e547-204">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="6e547-205">A környezet struktúra megtalálható-e az Application Insights által naplózott összes telemetriai adat.</span><span class="sxs-lookup"><span data-stu-id="6e547-205">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="6e547-206">A város elem nem lehet megadni a naplókban.</span><span class="sxs-lookup"><span data-stu-id="6e547-206">The city element may not be populated in your logs.</span></span> <span data-ttu-id="6e547-207">A séma segítségével azonosíthatók a más elemeket lekérdezhető a naplók adatok szerepelhetnek.</span><span class="sxs-lookup"><span data-stu-id="6e547-207">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>
   >
   >

    <span data-ttu-id="6e547-208">A lekérdezés által visszaadott adatokat az alábbihoz hasonló:</span><span class="sxs-lookup"><span data-stu-id="6e547-208">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="next-steps"></a><span data-ttu-id="6e547-209">Következő lépések</span><span class="sxs-lookup"><span data-stu-id="6e547-209">Next steps</span></span>

<span data-ttu-id="6e547-210">További példák a Spark használata adatokhoz és szolgáltatásokhoz az Azure-ban tekintse meg a következő dokumentumokat:</span><span class="sxs-lookup"><span data-stu-id="6e547-210">For more examples of using Spark to work with data and services in Azure, see the following documents:</span></span>

* [<span data-ttu-id="6e547-211">Spark és BI: Interaktív adatelemzés végrehajtása a Spark on HDInsight használatával, BI-eszközökkel</span><span class="sxs-lookup"><span data-stu-id="6e547-211">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="6e547-212">Spark és Machine Learning: A Spark on HDInsight használata az épület-hőmérséklet elemzésére HVAC-adatok alapján</span><span class="sxs-lookup"><span data-stu-id="6e547-212">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="6e547-213">Spark és Machine Learning: A Spark on HDInsight használata az élelmiszervizsgálati eredmények előrejelzésére</span><span class="sxs-lookup"><span data-stu-id="6e547-213">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="6e547-214">Spark Streaming: Spark on HDInsight használata az adatfolyam-továbbítási alkalmazások létrehozásához</span><span class="sxs-lookup"><span data-stu-id="6e547-214">Spark Streaming: Use Spark in HDInsight for building streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="6e547-215">A webhelynapló elemzése a Spark on HDInsight használatával</span><span class="sxs-lookup"><span data-stu-id="6e547-215">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

<span data-ttu-id="6e547-216">Létrehozása és alkalmazások futtatása Spark kapcsolatos tudnivalókat lásd: a következő dokumentumokat:</span><span class="sxs-lookup"><span data-stu-id="6e547-216">For information on creating and running Spark applications, see the following documents:</span></span>

* [<span data-ttu-id="6e547-217">Önálló alkalmazás létrehozása a Scala használatával</span><span class="sxs-lookup"><span data-stu-id="6e547-217">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="6e547-218">Feladatok távoli futtatása Spark-fürtön a Livy használatával</span><span class="sxs-lookup"><span data-stu-id="6e547-218">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)
