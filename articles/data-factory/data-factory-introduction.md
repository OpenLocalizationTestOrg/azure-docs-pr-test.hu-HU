---
title: "A Data Factory nevű adatintegrációs szolgáltatás ismertetése | Microsoft Docs"
description: "A témakör ismerteti, hogy mi is az Azure Data Factory: egy felhőalapú adatintegrációs szolgáltatás, amellyel előkészíthető és automatizálható az adatok továbbítása és átalakítása."
keywords: "adatintegrálás, felhőalapú adatintegráció, mi az az azure data factory"
services: data-factory
documentationcenter: 
author: sharonlo101
manager: jhubbard
editor: monicar
ms.assetid: cec68cb5-ca0d-473b-8ae8-35de949a009e
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: get-started-article
ms.date: 08/14/2017
ms.author: shlo
ms.openlocfilehash: bc72c4d58b98f6521dbb7420a5d05a121b0ddbda
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 08/18/2017
---
# <a name="introduction-to-azure-data-factory"></a><span data-ttu-id="7033e-104">Az Azure Data Factory bemutatása</span><span class="sxs-lookup"><span data-stu-id="7033e-104">Introduction to Azure Data Factory</span></span> 
## <a name="what-is-azure-data-factory"></a><span data-ttu-id="7033e-105">Mi az az Azure Data Factory?</span><span class="sxs-lookup"><span data-stu-id="7033e-105">What is Azure Data Factory?</span></span>
<span data-ttu-id="7033e-106">A big data világában hogyan aknázhatja ki a vállalkozás a meglévő adatait?</span><span class="sxs-lookup"><span data-stu-id="7033e-106">In the world of big data, how is existing data leveraged in business?</span></span> <span data-ttu-id="7033e-107">Lehetséges a felhőben létrehozott adatokat feldúsítani a helyi vagy más különálló adatforrásokból származó referenciaadatokkal?</span><span class="sxs-lookup"><span data-stu-id="7033e-107">Is it possible to enrich data generated in the cloud by using reference data from on-premises data sources or other disparate data sources?</span></span> <span data-ttu-id="7033e-108">Például egy játékokkal foglalkozó vállalat rengeteg, a játékok által készített naplót gyűjt össze a felhőben.</span><span class="sxs-lookup"><span data-stu-id="7033e-108">For example, a gaming company collects many logs produced by games in the cloud.</span></span> <span data-ttu-id="7033e-109">Ezen naplókat szeretné elemezni, hogy betekintést nyerhessen az ügyfelek preferenciáiba, demográfiai adataiba és felhasználói viselkedésébe, hogy ezek alapján azonosítsa az értékesítési és keresztértékesítési lehetőségeket, új funkciókat fejlesszen az üzleti növekedés elősegítése érdekében, és jobb felhasználói élményt nyújtson az ügyfeleknek.</span><span class="sxs-lookup"><span data-stu-id="7033e-109">It wants to analyze these logs to gain insights in to customer preferences, demographics, usage behavior etc. to identify up-sell and cross-sell opportunities, develop new compelling features to drive business growth, and provide a better experience to customers.</span></span> 

<span data-ttu-id="7033e-110">A naplók elemzéséhez a vállalatnak a helyszíni adattárban tárolt referenciaadatokat kell felhasználnia, mint például az ügyféladatokat, a játékadatokat és a reklámkampány-adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-110">To analyze these logs, the company needs to use the reference data such as customer information, game information, marketing campaign information that is in an on-premises data store.</span></span> <span data-ttu-id="7033e-111">Ennélfogva a vállalat szeretne hozzáférni a felhőbeli adattárban található naplóadatokhoz és a helyszíni adattárban található referenciaadatokhoz.</span><span class="sxs-lookup"><span data-stu-id="7033e-111">Therefore, the company wants to ingest log data from the cloud data store and reference data from the on-premises data store.</span></span> <span data-ttu-id="7033e-112">Ezután feldolgozza az adatokat a Hadoop használatával a felhőben (Azure HDInsight), és az eredményeket közzéteszi egy felhőbeli adattárházban, például egy Azure SQL Data Warehouse-ban, vagy egy helyszíni adattárolóban, például egy SQL Server-kiszolgálón.</span><span class="sxs-lookup"><span data-stu-id="7033e-112">Then, process the data by using Hadoop in the cloud (Azure HDInsight) and publish the result data into a cloud data warehouse such as Azure SQL Data Warehouse or an on-premises data store such as SQL Server.</span></span> <span data-ttu-id="7033e-113">Ezt a munkafolyamatot hetente egyszer szeretné futtatni.</span><span class="sxs-lookup"><span data-stu-id="7033e-113">It wants this workflow to run weekly once.</span></span> 

<span data-ttu-id="7033e-114">Mindehhez egy olyan platformra van szüksége, amellyel létrehozhat egy munkafolyamatot, amely képes kiolvasni az adatokat a helyszíni és a felhőbeli adattárolókból; átalakítani és feldolgozni az adatokat létező számítási szolgáltatások, például a Hadoop használatával; és közzétenni az eredményeket helyszíni vagy felhőbeli adattárolókon a BI-alkalmazások általi felhasználáshoz.</span><span class="sxs-lookup"><span data-stu-id="7033e-114">What is needed is a platform that allows the company to create a workflow that can ingest data from both on-premises and cloud data stores, and transform or process data by using existing compute services such as Hadoop, and publish the results to an on-premises or cloud data store for BI applications to consume.</span></span> 

![Data Factory – áttekintés](media/data-factory-introduction/what-is-azure-data-factory.png) 

<span data-ttu-id="7033e-116">Az Azure Data Factory az ilyen helyzetekben használható platform.</span><span class="sxs-lookup"><span data-stu-id="7033e-116">Azure Data Factory is the platform for this kind of scenarios.</span></span> <span data-ttu-id="7033e-117">Ez egy **felhőalapú adatintegrációs szolgáltatás, amely lehetővé teszi olyan, a felhőben futó, adatvezérelt munkafolyamatok létrehozását, amelyek irányítják és automatizálják az adatok átvitelét és átalakítását**.</span><span class="sxs-lookup"><span data-stu-id="7033e-117">It is a **cloud-based data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation**.</span></span> <span data-ttu-id="7033e-118">Az Azure Data Factory segítségével létrehozhatók és ütemezhetők a különböző adattárolókból adatokat beolvasó adatvezérelt munkafolyamatok, feldolgozhatók és átalakíthatók az adatok különböző számítási szolgáltatások használatával (pl. Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics és Azure Machine Learning), és a kimeneti adatok közzétehetők olyan adattárakban, mint például az Azure SQL Data Warehouse, ahonnan az üzleti intelligenciára épülő (BI-) alkalmazások felhasználhatják őket.</span><span class="sxs-lookup"><span data-stu-id="7033e-118">Using Azure Data Factory, you can create and schedule data-driven workflows (called pipelines) that can ingest data from disparate data stores, process/transform the data by using compute services such as Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics, and Azure Machine Learning, and publish output data to data stores such as Azure SQL Data Warehouse for business intelligence (BI) applications to consume.</span></span>  

<span data-ttu-id="7033e-119">Ez inkább egy kinyerési és betöltési (EL), majd egy átalakítási és betöltési (TL) platform, mintsem egy hagyományos kinyerési, átalakítási és betöltési (ETL) platform.</span><span class="sxs-lookup"><span data-stu-id="7033e-119">It's more of an Extract-and-Load (EL) and then Transform-and-Load (TL) platform rather than a traditional Extract-Transform-and-Load (ETL) platform.</span></span> <span data-ttu-id="7033e-120">Az elvégzett átalakítások célja az adatok számítási szolgáltatások által való átalakítása/feldolgozása, mintsem a származtatott oszlopok hozzáadásához, sorok megszámlálásához vagy adatok rendezéséhez használatos átalakítások elvégzése.</span><span class="sxs-lookup"><span data-stu-id="7033e-120">The transformations that are performed are to transform/process data by using compute services rather than to perform transformations like the ones for adding derived columns, counting number of rows, sorting data, etc.</span></span> 

<span data-ttu-id="7033e-121">Jelenleg az Azure Data Factoryben a munkafolyamatok által felhasznált és előállított adatok **időszeletekre osztott adatok** (óránként, naponta, hetente stb.).</span><span class="sxs-lookup"><span data-stu-id="7033e-121">Currently, in Azure Data Factory, the data that is consumed and produced by workflows is **time-sliced data** (hourly, daily, weekly, etc.).</span></span> <span data-ttu-id="7033e-122">Például beállítható, hogy egy folyamat naponta egyszer olvasson bemeneti adatokat, dolgozza fel őket, és hozzon létre kimeneti adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-122">For example, a pipeline may read input data, process data, and produce output data once a day.</span></span> <span data-ttu-id="7033e-123">A munkafolyamatok egyetlen alkalommal is futtathatók.</span><span class="sxs-lookup"><span data-stu-id="7033e-123">You can also run a workflow just one time.</span></span>  
  

## <a name="how-does-it-work"></a><span data-ttu-id="7033e-124">Hogyan működik?</span><span class="sxs-lookup"><span data-stu-id="7033e-124">How does it work?</span></span> 
<span data-ttu-id="7033e-125">Az adatvezérelt munkafolyamatok az Azure Data Factoryben általában a következő három lépést hajtják végre:</span><span class="sxs-lookup"><span data-stu-id="7033e-125">The pipelines (data-driven workflows) in Azure Data Factory typically perform the following three steps:</span></span>

![Az Azure Data Factory három szakasza](media/data-factory-introduction/three-information-production-stages.png)

### <a name="connect-and-collect"></a><span data-ttu-id="7033e-127">Csatlakozás és összegyűjtés</span><span class="sxs-lookup"><span data-stu-id="7033e-127">Connect and collect</span></span>
<span data-ttu-id="7033e-128">A vállalatok a legkülönfélébb adatokkal rendelkeznek a legkülönfélébb forrásokból.</span><span class="sxs-lookup"><span data-stu-id="7033e-128">Enterprises have data of various types located in disparate sources.</span></span> <span data-ttu-id="7033e-129">Az információ-előállítási rendszerek kiépítésének első lépése az összes szükséges adatforrás és feldolgozó, például az SaaS-szolgáltatások, a fájlmegosztások, az FTP-k vagy a webszolgáltatások összekapcsolása, és az adatok igényalapú átmozgatása egy központi helyre a további feldolgozás előtt.</span><span class="sxs-lookup"><span data-stu-id="7033e-129">The first step in building an information production system is to connect to all the required sources of data and processing, such as SaaS services, file shares, FTP, web services, and move the data as-needed to a centralized location for subsequent processing.</span></span>

<span data-ttu-id="7033e-130">A Data Factory nélkül a vállalatoknak egyéni adattovábbítási összetevőket kell készíteniük vagy egyéni szolgáltatásokat kell írniuk az adatforrások és feldolgozók integrálására.</span><span class="sxs-lookup"><span data-stu-id="7033e-130">Without Data Factory, enterprises must build custom data movement components or write custom services to integrate these data sources and processing.</span></span> <span data-ttu-id="7033e-131">Az ilyen rendszerek költségesek, nehezen integrálhatóak és tarthatók karban, és gyakorta nem áll rendelkezésre az a vállalati szintű felügyeleti, riasztási és vezérlési funkcionalitás, amelyet egy teljes mértékben felügyelt szolgáltatás biztosítani képes.</span><span class="sxs-lookup"><span data-stu-id="7033e-131">It is expensive and hard to integrate and maintain such systems, and it often lacks the enterprise grade monitoring and alerting, and the controls that a fully managed service can offer.</span></span>

<span data-ttu-id="7033e-132">A Data Factory segítségével a Másolási tevékenység keretében az adatok egyazon adatfolyamatban helyszíni és felhőalapú forrásadattárakból egyaránt továbbíthatóak egy, a felhőben lévő adattárba további elemzésre.</span><span class="sxs-lookup"><span data-stu-id="7033e-132">With Data Factory, you can use the Copy Activity in a data pipeline to move data from both on-premises and cloud source data stores to a centralization data store in the cloud for further analysis.</span></span> <span data-ttu-id="7033e-133">Begyűjtheti például az adatokat egy Azure Data Lake Store tárolóból, és később átalakíthatja azokat egy Azure Data Lake Analytics számítási szolgáltatás használatával.</span><span class="sxs-lookup"><span data-stu-id="7033e-133">For example, you can collect data in an Azure Data Lake Store and transform the data later by using an Azure Data Lake Analytics compute service.</span></span> <span data-ttu-id="7033e-134">Vagy begyűjtheti az adatokat egy Azure Blob Storage tárolóból, és később átalakíthatja azokat egy Azure HDInsight Hadoop-fürt használatával.</span><span class="sxs-lookup"><span data-stu-id="7033e-134">Or, collect data in an Azure Blob Storage and transform data later by using an Azure HDInsight Hadoop cluster.</span></span>

### <a name="transform-and-enrich"></a><span data-ttu-id="7033e-135">Átalakítás és bővítés</span><span class="sxs-lookup"><span data-stu-id="7033e-135">Transform and enrich</span></span>
<span data-ttu-id="7033e-136">Ha az adatok már jelen vannak egy központi adattárban a felhőben, akkor olyan számítási szolgáltatásokkal dolgozhatók fel és alakíthatók át, mint a HDInsight Hadoop, a Spark, a Data Lake Analytics és a Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="7033e-136">Once data is present in a centralized data store in the cloud, you want the collected data to be processed or transformed by using compute services such as HDInsight Hadoop, Spark, Data Lake Analytics, and Machine Learning.</span></span> <span data-ttu-id="7033e-137">Az átalakított adatok megbízhatóan állíthatók elő egy fenntartható és szabályozható séma szerint, az éles környezetek megbízható adatokkal való kiszolgálása érdekében.</span><span class="sxs-lookup"><span data-stu-id="7033e-137">You want to reliably produce transformed data on a maintainable and controlled schedule to feed production environments with trusted data.</span></span> 

### <a name="publish"></a><span data-ttu-id="7033e-138">Közzététel</span><span class="sxs-lookup"><span data-stu-id="7033e-138">Publish</span></span> 
<span data-ttu-id="7033e-139">Az átalakított adatok a felhőből áthelyezhetők egy helyszíni forrásra, például egy SQL-kiszolgálóra, vagy a felhőbeli tárolóforrásokban tartható az üzleti intelligenciaára épülő (BI-) és elemzőeszközök, illetve egyéb alkalmazások általi felhasználásra.</span><span class="sxs-lookup"><span data-stu-id="7033e-139">Deliver transformed data from the cloud to on-premises sources like SQL Server, or keep it in your cloud storage sources for consumption by business intelligence (BI) and analytics tools and other applications.</span></span>

## <a name="key-components"></a><span data-ttu-id="7033e-140">A legfontosabb összetevők</span><span class="sxs-lookup"><span data-stu-id="7033e-140">Key components</span></span>
<span data-ttu-id="7033e-141">Az Azure-előfizetések több Azure Data Factory-példányt (más néven adat-előállítókat) is tartalmazhatnak.</span><span class="sxs-lookup"><span data-stu-id="7033e-141">An Azure subscription may have one or more Azure Data Factory instances (or data factories).</span></span> <span data-ttu-id="7033e-142">Az Azure Data Factory négy fő összetevőből áll, amelyek együtt alkotják azt a platformot, amelyen létrehozhatók az adatvezérelt munkafolyamatok, és amelyeknek a lépései áthelyezik és átalakítják az adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-142">Azure Data Factory is composed of four key components that work together to provide the platform on which you can compose data-driven workflows with steps to move and transform data.</span></span> 

### <a name="pipeline"></a><span data-ttu-id="7033e-143">Folyamat</span><span class="sxs-lookup"><span data-stu-id="7033e-143">Pipeline</span></span>
<span data-ttu-id="7033e-144">Az adat-előállító egy vagy több folyamattal rendelkezhet.</span><span class="sxs-lookup"><span data-stu-id="7033e-144">A data factory may have one or more pipelines.</span></span> <span data-ttu-id="7033e-145">A folyamatok tevékenységek csoportjai.</span><span class="sxs-lookup"><span data-stu-id="7033e-145">A pipeline is a group of activities.</span></span> <span data-ttu-id="7033e-146">A folyamatban lévő tevékenységek együtt egy feladatot hajtanak végre.</span><span class="sxs-lookup"><span data-stu-id="7033e-146">Together, the activities in a pipeline perform a task.</span></span> <span data-ttu-id="7033e-147">Például a folyamat tartalmazhat egy csoportnyi műveletet, amelyek adatokat fogadnak egy Azure -blobból, majd egy Hive-lekérdezést futtatnak egy HDInsight-fürtön az adatok particionálásához.</span><span class="sxs-lookup"><span data-stu-id="7033e-147">For example, a pipeline could contain a group of activities that ingests data from an Azure blob, and then run a Hive query on an HDInsight cluster to partition the data.</span></span> <span data-ttu-id="7033e-148">A folyamatok használatának az az előnye, hogy így a tevékenységek egy készletben kezelhetők, nem pedig külön-külön.</span><span class="sxs-lookup"><span data-stu-id="7033e-148">The benefit of this is that the pipeline allows you to manage the activities as a set instead of each one individually.</span></span> <span data-ttu-id="7033e-149">Például maga a folyamat helyezhető üzembe és ütemezhető, nem a tevékenységek egymástól függetlenül.</span><span class="sxs-lookup"><span data-stu-id="7033e-149">For example, you can deploy and schedule the pipeline, instead of the activities independently.</span></span> 

### <a name="activity"></a><span data-ttu-id="7033e-150">Tevékenység</span><span class="sxs-lookup"><span data-stu-id="7033e-150">Activity</span></span>
<span data-ttu-id="7033e-151">Egy folyamat egy vagy több tevékenységgel rendelkezhet.</span><span class="sxs-lookup"><span data-stu-id="7033e-151">A pipeline may have one or more activities.</span></span> <span data-ttu-id="7033e-152">A tevékenységek meghatározzák az adatokon végrehajtandó műveleteket.</span><span class="sxs-lookup"><span data-stu-id="7033e-152">Activities define the actions to perform on your data.</span></span> <span data-ttu-id="7033e-153">A másolási tevékenység használatával például az egyik adattárból a másikba másolhatja az adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-153">For example, you may use a Copy activity to copy data from one data store to another data store.</span></span> <span data-ttu-id="7033e-154">Hasonlóképpen, egy Hive-tevékenység használatával Hive-lekérdezést futtathat egy Azure HDInsight-fürtön az adatok átalakításához és elemzéséhez.</span><span class="sxs-lookup"><span data-stu-id="7033e-154">Similarly, you may use a Hive activity, which runs a Hive query on an Azure HDInsight cluster to transform or analyze your data.</span></span> <span data-ttu-id="7033e-155">A Data Factory két típusú tevékenységet támogat: az adattovábbítási tevékenységeket és az adatátalakítási tevékenységeket.</span><span class="sxs-lookup"><span data-stu-id="7033e-155">Data Factory supports two types of activities: data movement activities and data transformation activities.</span></span>

### <a name="data-movement-activities"></a><span data-ttu-id="7033e-156">Adattovábbítási tevékenységek</span><span class="sxs-lookup"><span data-stu-id="7033e-156">Data movement activities</span></span>
<span data-ttu-id="7033e-157">A Data Factory másolási tevékenysége adatokat másol egy forrásadattárból egy fogadó adattárba.</span><span class="sxs-lookup"><span data-stu-id="7033e-157">Copy Activity in Data Factory copies data from a source data store to a sink data store.</span></span> <span data-ttu-id="7033e-158">A Data Factory a következő adattárakat támogatja.</span><span class="sxs-lookup"><span data-stu-id="7033e-158">Data Factory supports the following data stores.</span></span> <span data-ttu-id="7033e-159">Az adatok bármilyen forrásból bármilyen fogadóba másolhatók.</span><span class="sxs-lookup"><span data-stu-id="7033e-159">Data from any source can be written to any sink.</span></span> <span data-ttu-id="7033e-160">Az adattárra kattintva megtudhatja, hogy az adott tárolóba, illetve tárolóból hogyan másolhat adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-160">Click a data store to learn how to copy data to and from that store.</span></span>

[!INCLUDE [data-factory-supported-data-stores](../../includes/data-factory-supported-data-stores.md)]

<span data-ttu-id="7033e-161">További információkért tekintse meg az [adattovábbítási tevékenységekről](data-factory-data-movement-activities.md) szóló cikket.</span><span class="sxs-lookup"><span data-stu-id="7033e-161">For more information, see [Data Movement Activities](data-factory-data-movement-activities.md) article.</span></span>

### <a name="data-transformation-activities"></a><span data-ttu-id="7033e-162">Adatátalakítási tevékenységek</span><span class="sxs-lookup"><span data-stu-id="7033e-162">Data transformation activities</span></span>
[!INCLUDE [data-factory-transformation-activities](../../includes/data-factory-transformation-activities.md)]

<span data-ttu-id="7033e-163">További információkért tekintse meg az [adatátalakítási tevékenységekről](data-factory-data-transformation-activities.md) szóló cikket.</span><span class="sxs-lookup"><span data-stu-id="7033e-163">For more information, see [Data Transformation Activities](data-factory-data-transformation-activities.md) article.</span></span>

### <a name="custom-net-activities"></a><span data-ttu-id="7033e-164">Egyéni .NET-tevékenységek</span><span class="sxs-lookup"><span data-stu-id="7033e-164">Custom .NET activities</span></span>
<span data-ttu-id="7033e-165">Ha olyan adattárból/adattárba szeretne adatokat továbbítani, amely nem támogatja a másolási tevékenységet, vagy saját logika szerint szeretne adatátalakítást végezni, hozzon létre egy **egyéni .NET-tevékenységet**.</span><span class="sxs-lookup"><span data-stu-id="7033e-165">If you need to move data to/from a data store that Copy Activity doesn't support, or transform data using your own logic, create a **custom .NET activity**.</span></span> <span data-ttu-id="7033e-166">További információ az egyéni tevékenységek létrehozásával és használatával kapcsolatban: [Egyéni tevékenységek használata Azure Data Factory-folyamatban](data-factory-use-custom-activities.md).</span><span class="sxs-lookup"><span data-stu-id="7033e-166">For details on creating and using a custom activity, see [Use custom activities in an Azure Data Factory pipeline](data-factory-use-custom-activities.md).</span></span>

### <a name="datasets"></a><span data-ttu-id="7033e-167">Adathalmazok</span><span class="sxs-lookup"><span data-stu-id="7033e-167">Datasets</span></span>
<span data-ttu-id="7033e-168">Minden tevékenység nulla vagy több adatkészletet fogad bemenetként, és egy vagy több adatkészletet állít elő kimenetként.</span><span class="sxs-lookup"><span data-stu-id="7033e-168">An activity takes zero or more datasets as inputs and one or more datasets as outputs.</span></span> <span data-ttu-id="7033e-169">Az adatkészletek adatstruktúrákat jelölnek az adattárakon belül, amelyek egyszerűen rámutatnak vagy meghivatkozzák az adatokat, amelyeket a tevékenységekben be- vagy kimenetként használni szeretne.</span><span class="sxs-lookup"><span data-stu-id="7033e-169">Datasets represent data structures within the data stores, which simply point or reference the data you want to use in your activities as inputs or outputs.</span></span> <span data-ttu-id="7033e-170">Az Azure Blob-adatkészlet például meghatározza a blobtárolót és azt az Azure Blob Storage-mappát, amelyből a folyamat beolvassa az adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-170">For example, an Azure Blob dataset specifies the blob container and folder in the Azure Blob Storage from which the pipeline should read the data.</span></span> <span data-ttu-id="7033e-171">Az Azure SQL Table adatkészlet megadhatja, hogy a tevékenység melyik táblára írja a kimeneti adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-171">Or, an Azure SQL Table dataset specifies the table to which the output data is written by the activity.</span></span> 

### <a name="linked-services"></a><span data-ttu-id="7033e-172">Társított szolgáltatások</span><span class="sxs-lookup"><span data-stu-id="7033e-172">Linked services</span></span>
<span data-ttu-id="7033e-173">A társított szolgáltatások nagyon hasonlóak a kapcsolati karakterláncokhoz, amelyek meghatározzák azokat a kapcsolati információkat, amelyeket a Data Factory a külső erőforrásokhoz történő csatlakozáshoz igényel.</span><span class="sxs-lookup"><span data-stu-id="7033e-173">Linked services are much like connection strings, which define the connection information needed for Data Factory to connect to external resources.</span></span> <span data-ttu-id="7033e-174">Tulajdonképpen a társított szolgáltatás határozza meg az adatforrással való kapcsolatot, míg az adatkészlet jelöli az adatok struktúráját.</span><span class="sxs-lookup"><span data-stu-id="7033e-174">Think of it this way - a linked service defines the connection to the data source and a dataset represents the structure of the data.</span></span> <span data-ttu-id="7033e-175">Az Azure Storage társított szolgáltatása például kapcsolati karakterláncot szolgáltat az Azure Storage-fiókhoz való csatlakozáshoz.</span><span class="sxs-lookup"><span data-stu-id="7033e-175">For example, an Azure Storage linked service specifies connection string to connect to the Azure Storage account.</span></span> <span data-ttu-id="7033e-176">Az Azure Blob-adatkészlet pedig meghatározza a blobtárolót és az adatokat tartalmazó mappát.</span><span class="sxs-lookup"><span data-stu-id="7033e-176">And, an Azure Blob dataset specifies the blob container and the folder that contains the data.</span></span>   

<span data-ttu-id="7033e-177">A társított szolgáltatásokat két célból használjuk a Data Factoryban:</span><span class="sxs-lookup"><span data-stu-id="7033e-177">Linked services are used for two purposes in Data Factory:</span></span>

* <span data-ttu-id="7033e-178">Egy **adattár**, többek között például egy helyszíni SQL Server, Oracle-adatbázis, fájlmegosztás vagy egy Azure Blob Storage-fiók jelölésére.</span><span class="sxs-lookup"><span data-stu-id="7033e-178">To represent a **data store** including, but not limited to, an on-premises SQL Server, Oracle database, file share, or an Azure Blob Storage account.</span></span> <span data-ttu-id="7033e-179">A támogatott adattárak listája az [Adattovábbítási tevékenységek](#data-movement-activities) című részben található.</span><span class="sxs-lookup"><span data-stu-id="7033e-179">See the [Data movement activities](#data-movement-activities) section for a list of supported data stores.</span></span>
* <span data-ttu-id="7033e-180">Olyan **számítási erőforrás** jelölésére, amelyen végrehajtható a tevékenység.</span><span class="sxs-lookup"><span data-stu-id="7033e-180">To represent a **compute resource** that can host the execution of an activity.</span></span> <span data-ttu-id="7033e-181">A HDInsightHive-tevékenység végrehajtása például egy HDInsight Hadoop-fürtön történik.</span><span class="sxs-lookup"><span data-stu-id="7033e-181">For example, the HDInsightHive activity runs on an HDInsight Hadoop cluster.</span></span> <span data-ttu-id="7033e-182">A támogatott számítási környezetek listája az [Adatátalakítási tevékenységek](#data-transformation-activities) szakaszban található.</span><span class="sxs-lookup"><span data-stu-id="7033e-182">See [Data transformation activities](#data-transformation-activities) section for a list of supported compute environments.</span></span>

### <a name="relationship-between-data-factory-entities"></a><span data-ttu-id="7033e-183">Data Factory-entitások közötti kapcsolatok</span><span class="sxs-lookup"><span data-stu-id="7033e-183">Relationship between Data Factory entities</span></span>
<span data-ttu-id="7033e-184">![Ábra: A Data Factory áttekintése, felhőalapú adatintegrációs szolgáltatás – főbb fogalmak](./media/data-factory-introduction/data-integration-service-key-concepts.png)
**2. ábra.**</span><span class="sxs-lookup"><span data-stu-id="7033e-184">![Diagram: Data Factory, a cloud data integration service - Key Concepts](./media/data-factory-introduction/data-integration-service-key-concepts.png)
**Figure 2.**</span></span> <span data-ttu-id="7033e-185">Az adatkészlet, a tevékenység, a folyamat és a társított szolgáltatás közötti kapcsolatok</span><span class="sxs-lookup"><span data-stu-id="7033e-185">Relationships between Dataset, Activity, Pipeline, and Linked service</span></span>

## <a name="supported-regions"></a><span data-ttu-id="7033e-186">Támogatott régiók</span><span class="sxs-lookup"><span data-stu-id="7033e-186">Supported regions</span></span>
<span data-ttu-id="7033e-187">Jelenleg az **USA nyugati régiójában**, az **USA keleti régiójában** és az **észak-európai** régióban hozhat létre data factoryt.</span><span class="sxs-lookup"><span data-stu-id="7033e-187">Currently, you can create data factories in the **West US**, **East US**, and **North Europe** regions.</span></span> <span data-ttu-id="7033e-188">A data factory azonban más Azure-régiókban lévő adattárakhoz és számítási szolgáltatásokhoz is hozzáférhet az adatok adattárak közötti mozgatása vagy az adatok számítási szolgáltatásokkal történő feldolgozása érdekében.</span><span class="sxs-lookup"><span data-stu-id="7033e-188">However, a data factory can access data stores and compute services in other Azure regions to move data between data stores or process data using compute services.</span></span>

<span data-ttu-id="7033e-189">Maga az Azure Data Factory nem tárol adatokat.</span><span class="sxs-lookup"><span data-stu-id="7033e-189">Azure Data Factory itself does not store any data.</span></span> <span data-ttu-id="7033e-190">Lehetővé teszi viszont olyan adatvezérelt munkafolyamatok létrehozását, amelyekkel előkészíthető a [támogatott adattárak](#data-movement-activities) közötti adatmozgás és az adatok [számítási szolgáltatásokkal](#data-transformation-activities) történő feldolgozása más régiókban, illetve helyszíni környezetben.</span><span class="sxs-lookup"><span data-stu-id="7033e-190">It lets you create data-driven workflows to orchestrate movement of data between [supported data stores](#data-movement-activities) and processing of data using [compute services](#data-transformation-activities) in other regions or in an on-premises environment.</span></span> <span data-ttu-id="7033e-191">Lehetővé teszi továbbá a [munkafolyamatok figyelését és kezelését](data-factory-monitor-manage-pipelines.md) mind szoftveres, mind pedig felhasználói felületi mechanizmusokkal.</span><span class="sxs-lookup"><span data-stu-id="7033e-191">It also allows you to [monitor and manage workflows](data-factory-monitor-manage-pipelines.md) using both programmatic and UI mechanisms.</span></span>

<span data-ttu-id="7033e-192">Bár a Data Factory csak az **USA nyugati régiójában**, az **USA keleti régiójában** és az **észak-európai** régióban érhető el, az adatok Data Factoryval történő áthelyezését biztosító szolgáltatás [globálisan](data-factory-data-movement-activities.md#global) számos régióban elérhető.</span><span class="sxs-lookup"><span data-stu-id="7033e-192">Even though Data Factory is available in only **West US**, **East US**, and **North Europe** regions, the service powering the data movement in Data Factory is available [globally](data-factory-data-movement-activities.md#global) in several regions.</span></span> <span data-ttu-id="7033e-193">Ha az adattár tűzfal mögött található, akkor a helyszíni környezetben telepített [adatkezelési átjáró](data-factory-move-data-between-onprem-and-cloud.md) végzi az adatok áthelyezését.</span><span class="sxs-lookup"><span data-stu-id="7033e-193">If a data store is behind a firewall, then a [Data Management Gateway](data-factory-move-data-between-onprem-and-cloud.md) installed in your on-premises environment moves the data instead.</span></span>

<span data-ttu-id="7033e-194">Tegyük fel például, hogy számítási környezetei, mint például az Azure HDInsight-fürt és az Azure Machine Learning a nyugat-európai régión kívül futnak.</span><span class="sxs-lookup"><span data-stu-id="7033e-194">For an example, let us assume that your compute environments such as Azure HDInsight cluster and Azure Machine Learning are running out of West Europe region.</span></span> <span data-ttu-id="7033e-195">Létrehozhat egy Azure Data Factory-példányt Észak-Európában, és felhasználhatja a Nyugat-Európában lévő számítási környezetein futtatott feladatok ütemezéséhez.</span><span class="sxs-lookup"><span data-stu-id="7033e-195">You can create and use an Azure Data Factory instance in North Europe and use it to schedule jobs on your compute environments in West Europe.</span></span> <span data-ttu-id="7033e-196">A Data Factory néhány ezredmásodperc alatt aktiválja a feladatot a számítási környezetben, a feladatnak a számítási környezetben való futtatásához szükséges idő viszont nem változik.</span><span class="sxs-lookup"><span data-stu-id="7033e-196">It takes a few milliseconds for Data Factory to trigger the job on your compute environment but the time for running the job on your computing environment does not change.</span></span>

## <a name="get-started-with-creating-a-pipeline"></a><span data-ttu-id="7033e-197">Bevezetés a folyamatok létrehozásába</span><span class="sxs-lookup"><span data-stu-id="7033e-197">Get started with creating a pipeline</span></span>
<span data-ttu-id="7033e-198">Az Azure Data Factoryben a következő eszközök és API-k használhatók adatfolyamatok létrehozására:</span><span class="sxs-lookup"><span data-stu-id="7033e-198">You can use one of these tools or APIs to create data pipelines in Azure Data Factory:</span></span> 

- <span data-ttu-id="7033e-199">Azure Portal</span><span class="sxs-lookup"><span data-stu-id="7033e-199">Azure portal</span></span>
- <span data-ttu-id="7033e-200">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="7033e-200">Visual Studio</span></span>
- <span data-ttu-id="7033e-201">PowerShell</span><span class="sxs-lookup"><span data-stu-id="7033e-201">PowerShell</span></span>
- <span data-ttu-id="7033e-202">.NET API</span><span class="sxs-lookup"><span data-stu-id="7033e-202">.NET API</span></span>
- <span data-ttu-id="7033e-203">REST API</span><span class="sxs-lookup"><span data-stu-id="7033e-203">REST API</span></span>
- <span data-ttu-id="7033e-204">Azure Resource Manager-sablon</span><span class="sxs-lookup"><span data-stu-id="7033e-204">Azure Resource Manager template.</span></span> 

<span data-ttu-id="7033e-205">A következő oktatóanyagok részletes utasításait követve megtudhatja, hogyan építhet ki adatfolyamatokkal rendelkező data factorykat:</span><span class="sxs-lookup"><span data-stu-id="7033e-205">To learn how to build data factories with data pipelines, follow step-by-step instructions in the following tutorials:</span></span>

| <span data-ttu-id="7033e-206">Oktatóanyag</span><span class="sxs-lookup"><span data-stu-id="7033e-206">Tutorial</span></span> | <span data-ttu-id="7033e-207">Leírás</span><span class="sxs-lookup"><span data-stu-id="7033e-207">Description</span></span> |
| --- | --- |
| [<span data-ttu-id="7033e-208">Két felhőalapú adattár közötti adatáthelyezés</span><span class="sxs-lookup"><span data-stu-id="7033e-208">Move data between two cloud data stores</span></span>](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) |<span data-ttu-id="7033e-209">Ebben az oktatóanyagban olyan folyamattal rendelkező data factoryt hoz létre, amely Blob Storage-ból SQL-adatbázisba **helyez át adatokat**.</span><span class="sxs-lookup"><span data-stu-id="7033e-209">In this tutorial, you create a data factory with a pipeline that **moves data** from Blob storage to SQL database.</span></span> |
| [<span data-ttu-id="7033e-210">Adatok átalakítása Hadoop-fürttel</span><span class="sxs-lookup"><span data-stu-id="7033e-210">Transform data using Hadoop cluster</span></span>](data-factory-build-your-first-pipeline.md) |<span data-ttu-id="7033e-211">Az oktatóanyag során kiépíti az első Azure data factoryját egy olyan adatfolyamattal, amely egy Azure HDInsight (Hadoop) fürtön futtatott Hive-parancsprogrammal **dolgozza fel az adatokat**.</span><span class="sxs-lookup"><span data-stu-id="7033e-211">In this tutorial, you build your first Azure data factory with a data pipeline that **processes data** by running Hive script on an Azure HDInsight (Hadoop) cluster.</span></span> |
| [<span data-ttu-id="7033e-212">Egy helyszíni és egy felhőalapú adattár közötti adatáthelyezés adatkezelési átjáró segítségével</span><span class="sxs-lookup"><span data-stu-id="7033e-212">Move data between an on-premises data store and a cloud data store using Data Management Gateway</span></span>](data-factory-move-data-between-onprem-and-cloud.md) |<span data-ttu-id="7033e-213">Ebben az oktatóanyagban olyan folyamattal rendelkező data factoryt épít ki, amely egy **helyszíni** SQL Server-adatbázisból Azure-blobba **helyez át adatokat**.</span><span class="sxs-lookup"><span data-stu-id="7033e-213">In this tutorial, you build a data factory with a pipeline that **moves data** from an **on-premises** SQL Server database to an Azure blob.</span></span> <span data-ttu-id="7033e-214">A bemutató részeként telepíti és konfigurálja az adatkezelési átjárót a gépen.</span><span class="sxs-lookup"><span data-stu-id="7033e-214">As part of the walkthrough, you install and configure the Data Management Gateway on your machine.</span></span> |
