---
title: "a Microsoft Azure Service Fabric aaaCommon kérdésekre |} Microsoft Docs"
description: "A Service Fabric és a válaszok kapcsolatos gyakori kérdések"
services: service-fabric
documentationcenter: .net
author: chackdan
manager: timlt
editor: 
ms.assetid: 5a179703-ff0c-4b8e-98cd-377253295d12
ms.service: service-fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: na
ms.date: 08/18/2017
ms.author: chackdan
ms.openlocfilehash: 4cbe92d2a03f7a1ea5d077807fdc982288220a7e
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 10/06/2017
---
# <a name="commonly-asked-service-fabric-questions"></a><span data-ttu-id="0af79-103">A Service Fabric gyakori kérdések</span><span class="sxs-lookup"><span data-stu-id="0af79-103">Commonly asked Service Fabric questions</span></span>

<span data-ttu-id="0af79-104">Nincsenek számos gyakran feltett kérdésekre a Service Fabric Teendők, és hogyan kell használni.</span><span class="sxs-lookup"><span data-stu-id="0af79-104">There are many commonly asked questions about what Service Fabric can do and how it should be used.</span></span> <span data-ttu-id="0af79-105">Ez a dokumentum ismerteti a fenti gyakori kérdések és a válaszok.</span><span class="sxs-lookup"><span data-stu-id="0af79-105">This document covers many of those common questions and their answers.</span></span>

## <a name="cluster-setup-and-management"></a><span data-ttu-id="0af79-106">Fürt beállítása és kezelése</span><span class="sxs-lookup"><span data-stu-id="0af79-106">Cluster setup and management</span></span>

### <a name="can-i-create-a-cluster-that-spans-multiple-azure-regions-or-my-own-datacenters"></a><span data-ttu-id="0af79-107">Egy fürt, amely több Azure-régiók és a saját adatközpontját hozhat létre?</span><span class="sxs-lookup"><span data-stu-id="0af79-107">Can I create a cluster that spans multiple Azure regions or my own datacenters?</span></span>

<span data-ttu-id="0af79-108">Igen.</span><span class="sxs-lookup"><span data-stu-id="0af79-108">Yes.</span></span> 

<span data-ttu-id="0af79-109">Service Fabric fürtözési technológia hello core lehet működő bárhol hello world használt toocombine gépek, feltéve, hogy rendelkeznek-e hálózati kapcsolat tooeach más.</span><span class="sxs-lookup"><span data-stu-id="0af79-109">hello core Service Fabric clustering technology can be used toocombine machines running anywhere in hello world, so long as they have network connectivity tooeach other.</span></span> <span data-ttu-id="0af79-110">Azonban kialakításához és futtatásához az ilyen fürt bonyolult lehet.</span><span class="sxs-lookup"><span data-stu-id="0af79-110">However, building and running such a cluster can be complicated.</span></span>

<span data-ttu-id="0af79-111">Ha érdekli, ebben a forgatókönyvben, javasoljuk az forduljon tooget keresztül hello [Service Fabric Github problémák listájába](https://github.com/azure/service-fabric-issues) vagy a támogató szolgálat képviselője rendelés tooobtain további útmutatást a keresztül.</span><span class="sxs-lookup"><span data-stu-id="0af79-111">If you are interested in this scenario, we encourage you tooget in contact either through hello [Service Fabric Github Issues List](https://github.com/azure/service-fabric-issues) or through your support representative in order tooobtain additional guidance.</span></span> <span data-ttu-id="0af79-112">a Service Fabric-csapat hello működik-e tooprovide további egyértelműség, útmutatást és javaslatokat ehhez a forgatókönyvhöz.</span><span class="sxs-lookup"><span data-stu-id="0af79-112">hello Service Fabric team is working tooprovide additional clarity, guidance, and recommendations for this scenario.</span></span> 

<span data-ttu-id="0af79-113">Néhány dolgot tooconsider:</span><span class="sxs-lookup"><span data-stu-id="0af79-113">Some things tooconsider:</span></span> 

1. <span data-ttu-id="0af79-114">hello Azure Service Fabric fürt erőforrás regionális ma, mert hello virtulálisgép-skálázási készletekben hello fürthöz épül.</span><span class="sxs-lookup"><span data-stu-id="0af79-114">hello Service Fabric cluster resource in Azure is regional today, as are hello virtual machine scale sets that hello cluster is built on.</span></span> <span data-ttu-id="0af79-115">Ez azt jelenti, hogy hello eseményben regionális hiba előfordulhat, hogy elveszti hello képességét toomanage hello fürt hello Azure Resource Manageren keresztül vagy hello Azure portálon.</span><span class="sxs-lookup"><span data-stu-id="0af79-115">This means that in hello event of a regional failure you may lose hello ability toomanage hello cluster via hello Azure Resource Manager or hello Azure Portal.</span></span> <span data-ttu-id="0af79-116">Ez akkor fordulhat elő, annak ellenére, hogy hello fürt továbbra is futni fog, és hozzá tud toointeract közvetlenül lenne.</span><span class="sxs-lookup"><span data-stu-id="0af79-116">This can happen even though hello cluster remains running and you'd be able toointeract with it directly.</span></span> <span data-ttu-id="0af79-117">Emellett Azure ma nem biztosít hello képességét toohave egyetlen virtuális hálózaton használható régiók között.</span><span class="sxs-lookup"><span data-stu-id="0af79-117">In addition, Azure today does not offer hello ability toohave a single virtual network that is usable across regions.</span></span> <span data-ttu-id="0af79-118">Ez azt jelenti, hogy az Azure-ban több területi fürt szükséges [nyilvános IP-címeket az egyes virtuális gépek, a Virtuálisgép-méretezési készlet hello](../virtual-machine-scale-sets/virtual-machine-scale-sets-networking.md#public-ipv4-per-virtual-machine) vagy [Azure VPN Gatewayek](../vpn-gateway/vpn-gateway-about-vpngateways.md).</span><span class="sxs-lookup"><span data-stu-id="0af79-118">This means that a multi-region cluster in Azure requires either [Public IP Addresses for each VM in hello VM Scale Sets](../virtual-machine-scale-sets/virtual-machine-scale-sets-networking.md#public-ipv4-per-virtual-machine) or [Azure VPN Gateways](../vpn-gateway/vpn-gateway-about-vpngateways.md).</span></span> <span data-ttu-id="0af79-119">A fenti hálózatkezelési lehetőségek hatással különböző van a költségek, a teljesítmény és a toosome mértékben alkalmazás tervét, így gondos elemzés és tervezése előtt meg kell adni egy ilyen környezet állandó.</span><span class="sxs-lookup"><span data-stu-id="0af79-119">These networking choices have different impacts on costs, performance, and toosome degree application design, so careful analysis and planning is required before standing up such an environment.</span></span>
2. <span data-ttu-id="0af79-120">hello karbantartási, felügyeleti és figyelési ezeknek a gépeknek válhat bonyolult, különösen akkor, ha átnyúlhatnak _típusok_ a környezetek, például különböző szolgáltatók közötti vagy a helyszíni erőforrások és az Azure közötti .</span><span class="sxs-lookup"><span data-stu-id="0af79-120">hello maintenance, management, and monitoring of these machines can become complicated, especially when spanned across _types_ of environments, such as between different cloud providers or between on-premises resources and Azure.</span></span> <span data-ttu-id="0af79-121">Gondot kell fordítani, tooensure, amely frissíti, a figyelés, a felügyeleti és diagnosztikai értendők hello fürt-vagy hello alkalmazások, az ilyen környezetekben termelési számítási feladatokhoz futtatása előtt.</span><span class="sxs-lookup"><span data-stu-id="0af79-121">Care must be taken tooensure that upgrades, monitoring, management, and diagnostics are understood for both hello cluster and hello applications before running production workloads in such an environment.</span></span> <span data-ttu-id="0af79-122">Ha már rendelkezik Azure-ban vagy a saját adatközpontját belül problémák megoldásához élmény rengeteg, akkor valószínű, hogy ezek azonos megoldások is alkalmazható, ha épület kimenő, vagy a Service Fabric-fürt fut.</span><span class="sxs-lookup"><span data-stu-id="0af79-122">If you already have lots of experience solving these problems in Azure or within your own datacenters, then it is likely that those same solutions can be applied when building out or running your Service Fabric cluster.</span></span> 

### <a name="do-service-fabric-nodes-automatically-receive-os-updates"></a><span data-ttu-id="0af79-123">Hajtsa végre a Service Fabric-csomópontok automatikusan frissítését az operációs rendszer?</span><span class="sxs-lookup"><span data-stu-id="0af79-123">Do Service Fabric nodes automatically receive OS updates?</span></span>

<span data-ttu-id="0af79-124">Nem ma de ez történik akkor is, hogy Azure százalékát toodeliver közös kérelmet.</span><span class="sxs-lookup"><span data-stu-id="0af79-124">Not today, but this is also a common request that Azure intends toodeliver.</span></span>

<span data-ttu-id="0af79-125">Az ideiglenes hello tudunk [alkalmazás megadott](service-fabric-patch-orchestration-application.md) , hogy hello operációs rendszerek a Service Fabric-csomópont alatt maradnak javított és toodate fel.</span><span class="sxs-lookup"><span data-stu-id="0af79-125">In hello interim, we have [provided an application](service-fabric-patch-orchestration-application.md) that hello operating systems underneath your Service Fabric nodes stay patched and up toodate.</span></span>

<span data-ttu-id="0af79-126">hello kihívás az operációs rendszer frissítése érdekében, hogy ezek általában a számítógép újraindítása szükséges hello gép, amely ideiglenes rendelkezésre állást eredményez.</span><span class="sxs-lookup"><span data-stu-id="0af79-126">hello challenge with OS updates is that they typically require a reboot of hello machine, which results in temporary availability loss.</span></span> <span data-ttu-id="0af79-127">Önmagában ez nem probléma, mivel a Service Fabric automatikusan átirányítja a forgalmat ezen szolgáltatások tooother csomópontok.</span><span class="sxs-lookup"><span data-stu-id="0af79-127">By itself, that is not a problem, since Service Fabric will automatically redirect traffic for those services tooother nodes.</span></span> <span data-ttu-id="0af79-128">Azonban ha operációs rendszer frissítése érdekében van megfelelő koordináció hiányában hello fürtön, nincs hello lehetséges, hogy sok csomópont egyszerre leáll.</span><span class="sxs-lookup"><span data-stu-id="0af79-128">However, if OS updates are not coordinated across hello cluster, there is hello potential that many nodes go down at once.</span></span> <span data-ttu-id="0af79-129">Ilyen egyidejű újraindítások okozhat a teljes rendelkezésre állást egy szolgáltatás, vagy legalább egy adott partícióra (az állapotalapú szolgáltatás).</span><span class="sxs-lookup"><span data-stu-id="0af79-129">Such simultaneous reboots can cause complete availability loss for a service, or at least for a specific partition (for a stateful service).</span></span>

<span data-ttu-id="0af79-130">A jövőbeli hello hogy terv toosupport egy operációs rendszer frissítési házirend teljesen automatizált és frissítési tartományokon, koordinált győződjön meg arról, hogy rendelkezésre állás annak ellenére, hogy az újraindítások és más váratlan meghibásodások esetén.</span><span class="sxs-lookup"><span data-stu-id="0af79-130">In hello future, we plan toosupport an OS update policy that is fully automated and coordinated across update domains, ensuring that availability is maintained despite reboots and other unexpected failures.</span></span>

### <a name="can-i-use-large-virtual-machine-scale-sets-in-my-sf-cluster"></a><span data-ttu-id="0af79-131">Használható a ú fürt nagy virtuálisgép-méretezési csoportok?</span><span class="sxs-lookup"><span data-stu-id="0af79-131">Can I use Large Virtual Machine Scale Sets in my SF cluster?</span></span> 

<span data-ttu-id="0af79-132">**Válasz rövid** – nem</span><span class="sxs-lookup"><span data-stu-id="0af79-132">**Short answer** - No.</span></span> 

<span data-ttu-id="0af79-133">**Hosszú válasz** – bár hello nagy virtuálisgép-méretezési csoportok lehetővé teszik tooscale egy virtuálisgép-méretezési csoport legfeljebb 1000 Virtuálisgép-példányok, elhelyezési csoportok (PGs) használatával hello ilyeneket.</span><span class="sxs-lookup"><span data-stu-id="0af79-133">**Long Answer** - Although hello Large Virtual Machine Scale Sets allow you tooscale a virtual machine scale set upto 1000 VM instances, it does so by hello use of Placement Groups (PGs).</span></span> <span data-ttu-id="0af79-134">Tartalék tartományok (FDs) és a frissítési tartományok (UDs) belül egy elhelyezési csoport Service fabric használ FDs és UDs toomake elhelyezési döntéseit a szolgáltatáspéldány replikák/szolgáltatás csak megegyeznek.</span><span class="sxs-lookup"><span data-stu-id="0af79-134">Fault domains (FDs) and upgrade domains (UDs) are only consistent within a placement group Service fabric uses FDs and UDs toomake placement decisions of your service replicas/Service instances.</span></span> <span data-ttu-id="0af79-135">Mivel a hello FDs és UDs összehasonlítható elhelyezési csoporton belül csak ú nem tudja azt használni.</span><span class="sxs-lookup"><span data-stu-id="0af79-135">Since hello FDs  and UDs are comparable only within a placement group SF cannot use it.</span></span> <span data-ttu-id="0af79-136">Például, ha a PG1 VM1 a FD-topológia = 0, és a PG2 VM9 a FD-topológia = 4, ez nem jelenti azt, hogy VM1 és vm2 virtuális gépnek a két különböző hardver Rackszekrények, ezért ú nem használható hello FD értékek az az eset toomake elhelyezési kapcsolatos döntések meghozásakor.</span><span class="sxs-lookup"><span data-stu-id="0af79-136">For example, If VM1 in PG1 has a topology of FD=0 and VM9 in PG2 has a topology of FD=4 , it does not mean that VM1 and VM2 are on two different Hardware Racks, hence SF cannot use hello FD values in this case toomake placement decisions.</span></span>

<span data-ttu-id="0af79-137">Jelenleg más problémákat nagy virtuálisgép-méretezési csoportok, például a 4. szint hello hiánya betölteni a terheléselosztási támogatását.</span><span class="sxs-lookup"><span data-stu-id="0af79-137">There are other issues with Large virtual machine scale sets currently, like hello lack of level-4 Load balancing support.</span></span> <span data-ttu-id="0af79-138">Tekintse meg a toofor [nagy részleteinek méretezése beállítása](../virtual-machine-scale-sets/virtual-machine-scale-sets-placement-groups.md)</span><span class="sxs-lookup"><span data-stu-id="0af79-138">Refer toofor [details on Large scale sets](../virtual-machine-scale-sets/virtual-machine-scale-sets-placement-groups.md)</span></span>



### <a name="what-is-hello-minimum-size-of-a-service-fabric-cluster-why-cant-it-be-smaller"></a><span data-ttu-id="0af79-139">Mi az a Service Fabric-fürt hello minimális mérete?</span><span class="sxs-lookup"><span data-stu-id="0af79-139">What is hello minimum size of a Service Fabric cluster?</span></span> <span data-ttu-id="0af79-140">Miért nem legyen kisebb?</span><span class="sxs-lookup"><span data-stu-id="0af79-140">Why can't it be smaller?</span></span>

<span data-ttu-id="0af79-141">hello minimális támogatott termelési számítási feladatokhoz futó Service Fabric-fürt mérete az öt csomóponttal.</span><span class="sxs-lookup"><span data-stu-id="0af79-141">hello minimum supported size for a Service Fabric cluster running production workloads is five nodes.</span></span> <span data-ttu-id="0af79-142">Fejlesztési/Tesztelési forgatókönyvek esetén három csomópontos fürt támogatott.</span><span class="sxs-lookup"><span data-stu-id="0af79-142">For dev/test scenarios, we support three node clusters.</span></span>

<span data-ttu-id="0af79-143">Ezek minimumértékének létezik, mert a Service Fabric-fürt hello lefuttat egy állapotalapú rendszerszolgáltatások, beleértve a naming service hello és hello Feladatátvevőfürt-kezelő.</span><span class="sxs-lookup"><span data-stu-id="0af79-143">These minimums exist because hello Service Fabric cluster runs a set of stateful system services, including hello naming service and hello failover manager.</span></span> <span data-ttu-id="0af79-144">Ezeket a szolgáltatásokat, amelyek nyomon követheti, mi le lett telepítve toohello fürt, és ha azok még jelenleg üzemel, az erős konzisztencia függenek.</span><span class="sxs-lookup"><span data-stu-id="0af79-144">These services, which track what services have been deployed toohello cluster and where they're currently hosted, depend on strong consistency.</span></span> <span data-ttu-id="0af79-145">Hello képességét tooacquire viszont függ, hogy az erős konzisztencia egy *kvórum* bármely adott frissítés toohello állapotának azokat szolgáltatási, ahol a kvórum jelenti szigorú többsége hello replikák (N/2 + 1) egy adott szolgáltatáshoz.</span><span class="sxs-lookup"><span data-stu-id="0af79-145">That strong consistency, in turn, depends on hello ability tooacquire a *quorum* for any given update toohello state of those services, where a quorum represents a strict majority of hello replicas (N/2 +1) for a given service.</span></span>

<span data-ttu-id="0af79-146">A háttérben történő vizsgáljuk meg néhány lehetséges fürtkonfigurációk:</span><span class="sxs-lookup"><span data-stu-id="0af79-146">With that background, let's examine some possible cluster configurations:</span></span>

<span data-ttu-id="0af79-147">**Egy csomópont**: ezt a beállítást nem biztosít magas rendelkezésre állású, mivel hello megszűnését hello egyetlen csomópont bármilyen okból azt jelenti, hogy a hello fürtözési hello megszűnését.</span><span class="sxs-lookup"><span data-stu-id="0af79-147">**One node**: this option does not provide high availability since hello loss of hello single node for any reason means hello loss of hello entire cluster.</span></span>

<span data-ttu-id="0af79-148">**Két csomópont**: két csomópont között telepített szolgáltatáshoz a kvórum (N = 2) 2 (2/2 + 1 = 2).</span><span class="sxs-lookup"><span data-stu-id="0af79-148">**Two nodes**: a quorum for a service deployed across two nodes (N = 2) is 2 (2/2 + 1 = 2).</span></span> <span data-ttu-id="0af79-149">Egy replikát nem vesztek el, esetén nem lehetséges toocreate kvórumához.</span><span class="sxs-lookup"><span data-stu-id="0af79-149">When a single replica is lost, it is impossible toocreate a quorum.</span></span> <span data-ttu-id="0af79-150">Az szolgáltatás frissítés végrehajtása szükséges egy replika le ideiglenesen véve, ez a beállítás nem hasznos.</span><span class="sxs-lookup"><span data-stu-id="0af79-150">Since performing a service upgrade requires temporarily taking down a replica, this is not a useful configuration.</span></span>

<span data-ttu-id="0af79-151">**Három csomópont**: a három csomópont (N = 3) révén hello követelmény toocreate egy kvórum még két csomópont (3/2 + 1 = 2).</span><span class="sxs-lookup"><span data-stu-id="0af79-151">**Three nodes**: with three nodes (N=3), hello requirement toocreate a quorum is still two nodes (3/2 + 1 = 2).</span></span> <span data-ttu-id="0af79-152">Ez azt jelenti, hogy az egyes csomópontok elvesznek, és továbbra is a kvórum fenntartása.</span><span class="sxs-lookup"><span data-stu-id="0af79-152">This means that you can lose an individual node and still maintain quorum.</span></span>

<span data-ttu-id="0af79-153">három csomópont fürtkonfiguráció hello támogatott fejlesztési és tesztelési célú biztonságosan frissítések végrehajtása és után is megmaradnak az egyes csomópontok hibáit, mert mindaddig, amíg azok nem fordulhat elő, egyidejűleg.</span><span class="sxs-lookup"><span data-stu-id="0af79-153">hello three node cluster configuration is supported for dev/test because you can safely perform upgrades and survive individual node failures, as long as they don't happen simultaneously.</span></span> <span data-ttu-id="0af79-154">A termelési számítási feladatokhoz rugalmas toosuch egyidejű hibája esetén, kell lennie, ezért a öt csomópontra szükség.</span><span class="sxs-lookup"><span data-stu-id="0af79-154">For production workloads, you must be resilient toosuch a simultaneous failure, so five nodes are required.</span></span>

### <a name="can-i-turn-off-my-cluster-at-nightweekends-toosave-costs"></a><span data-ttu-id="0af79-155">Lehet kikapcsolni a fürt éjszakai/hétvégéket toosave költségek?</span><span class="sxs-lookup"><span data-stu-id="0af79-155">Can I turn off my cluster at night/weekends toosave costs?</span></span>

<span data-ttu-id="0af79-156">Általában nem.</span><span class="sxs-lookup"><span data-stu-id="0af79-156">In general, no.</span></span> <span data-ttu-id="0af79-157">A Service Fabric tárolja állapot helyi, rövid élettartamú lemezeken, ami azt jelenti, hogy ha hello virtuális gép áthelyezett tooa másik gazdagépet, hello adatok nem helyezi át a vele.</span><span class="sxs-lookup"><span data-stu-id="0af79-157">Service Fabric stores state on local, ephemeral disks, meaning that if hello virtual machine is moved tooa different host, hello data does not move with it.</span></span> <span data-ttu-id="0af79-158">A normál működés, amely nincs probléma, hello új csomópont jelenik toodate más csomópontok.</span><span class="sxs-lookup"><span data-stu-id="0af79-158">In normal operation, that is not a problem as hello new node is brought up toodate by other nodes.</span></span> <span data-ttu-id="0af79-159">Azonban ha állítson le minden csomópontot, és azokat később újraindíthatja a rendszert, nincs jelentős esély arra, hogy hello csomópontok többsége indítsa el az új gazdagépeknek, és ellenőrizze a rendszer nem toorecover hello.</span><span class="sxs-lookup"><span data-stu-id="0af79-159">However, if you stop all nodes and restart them later, there is a significant possibility that most of hello nodes start on new hosts and make hello system unable toorecover.</span></span>

<span data-ttu-id="0af79-160">Ha szeretné, hogy az alkalmazás teszteléséhez, mielőtt telepítené toocreate fürtök, azt javasoljuk, hogy dinamikusan hozzon létre ezeket a fürt részeként a [folyamatos integráció/folyamatos központi telepítési folyamat](service-fabric-set-up-continuous-integration.md).</span><span class="sxs-lookup"><span data-stu-id="0af79-160">If you would like toocreate clusters for testing your application before it is deployed, we recommend that you dynamically create those clusters as part of your [continuous integration/continuous deployment pipeline](service-fabric-set-up-continuous-integration.md).</span></span>


### <a name="how-do-i-upgrade-my-operating-system-for-example-from-windows-server-2012-toowindows-server-2016"></a><span data-ttu-id="0af79-161">Hogyan lehet frissíteni az operációs rendszer (például a Windows Server 2012 tooWindows Server 2016-os)?</span><span class="sxs-lookup"><span data-stu-id="0af79-161">How do I upgrade my Operating System (for example from Windows Server 2012 tooWindows Server 2016)?</span></span>

<span data-ttu-id="0af79-162">Napjainkban fejlesztjük fejlett élményt, amíg való telepítésért felelős hello frissítését.</span><span class="sxs-lookup"><span data-stu-id="0af79-162">While we're working on an improved experience, today, you are responsible for hello upgrade.</span></span> <span data-ttu-id="0af79-163">Hello hello az operációs rendszer lemezképét frissítenie kell a fürt egy virtuális gép virtuális gépeinek hello egyszerre.</span><span class="sxs-lookup"><span data-stu-id="0af79-163">You must upgrade hello OS image on hello virtual machines of hello cluster one VM at a time.</span></span> 

## <a name="container-support"></a><span data-ttu-id="0af79-164">Tároló-támogatás</span><span class="sxs-lookup"><span data-stu-id="0af79-164">Container Support</span></span>

### <a name="why-are-my-containers-that-are-deployed-toosf-unable-tooresolve-dns-addresses"></a><span data-ttu-id="0af79-165">Miért tárolói a telepített tooSF nem tooresolve DNS képező címeinek?</span><span class="sxs-lookup"><span data-stu-id="0af79-165">Why are my containers that are deployed tooSF unable tooresolve DNS addresses?</span></span>

<span data-ttu-id="0af79-166">A probléma 5.6.204.9494 lévő fürtökön érkezett jelentés verziója</span><span class="sxs-lookup"><span data-stu-id="0af79-166">This issue has been reported on clusters that are on 5.6.204.9494 version</span></span> 

<span data-ttu-id="0af79-167">**Megoldás** : kövesse [Ez a dokumentum](service-fabric-dnsservice.md) tooenable hello DNS service fabric-szolgáltatás a fürtön.</span><span class="sxs-lookup"><span data-stu-id="0af79-167">**Mitigation** :  Follow [this document](service-fabric-dnsservice.md) tooenable hello DNS service fabric service in your cluster.</span></span>

<span data-ttu-id="0af79-168">**Javítsa ki** : támogatott frissítési tooa fürt verziója, amely nagyobb, mint 5.6.204.9494, ha azok elérhetők.</span><span class="sxs-lookup"><span data-stu-id="0af79-168">**Fix** :  Upgrade tooa supported cluster version that is higher than 5.6.204.9494, when it is available.</span></span> <span data-ttu-id="0af79-169">Ha a fürt tooautomatic frissítéseket, majd hello fürt automatikusan frissíti a rögzített probléma toohello verziót.</span><span class="sxs-lookup"><span data-stu-id="0af79-169">If your cluster is set tooautomatic upgrades, then hello cluster will automatically upgrade toohello version that has this issue fixed.</span></span>

  
## <a name="application-design"></a><span data-ttu-id="0af79-170">Alkalmazás tervezése</span><span class="sxs-lookup"><span data-stu-id="0af79-170">Application Design</span></span>

### <a name="whats-hello-best-way-tooquery-data-across-partitions-of-a-reliable-collection"></a><span data-ttu-id="0af79-171">Mi az a hello legjobb módja tooquery adatok megbízható gyűjtemény partíciók között?</span><span class="sxs-lookup"><span data-stu-id="0af79-171">What's hello best way tooquery data across partitions of a Reliable Collection?</span></span>

<span data-ttu-id="0af79-172">Megbízható gyűjtemények jellemzően [particionált](service-fabric-concepts-partitioning.md) a nagyobb teljesítmény és átviteli kibővítési tooenable.</span><span class="sxs-lookup"><span data-stu-id="0af79-172">Reliable collections are typically [partitioned](service-fabric-concepts-partitioning.md) tooenable scale out for greater performance and throughput.</span></span> <span data-ttu-id="0af79-173">Ez azt jelenti, hogy egy adott szolgáltatáshoz hello állapot terjedhetnek 10 egység vagy 100-as egység gépek között.</span><span class="sxs-lookup"><span data-stu-id="0af79-173">That means that hello state for a given service may be spread across 10s or 100s of machines.</span></span> <span data-ttu-id="0af79-174">tooperform műveletek során a teljes adatkészlet, közül néhány:</span><span class="sxs-lookup"><span data-stu-id="0af79-174">tooperform operations over that full data set, you have a few options:</span></span>

- <span data-ttu-id="0af79-175">Hozzon létre egy másik szolgáltatás toopull hello szükséges adatok minden partíciója lekérdező szolgáltatást.</span><span class="sxs-lookup"><span data-stu-id="0af79-175">Create a service that queries all partitions of another service toopull in hello required data.</span></span>
- <span data-ttu-id="0af79-176">Hozzon létre egy szolgáltatás, amely egy másik szolgáltatás minden partíciója fogadhat adatokat.</span><span class="sxs-lookup"><span data-stu-id="0af79-176">Create a service that can receive data from all partitions of another service.</span></span>
- <span data-ttu-id="0af79-177">Minden szolgáltatás tooan külső store-ból adatokat rendszeres időközönként.</span><span class="sxs-lookup"><span data-stu-id="0af79-177">Periodically push data from each service tooan external store.</span></span> <span data-ttu-id="0af79-178">Ezt a módszert használja, csak ha hello lekérdezések végrehajtásához nem szerepelnek a fő üzleti logika.</span><span class="sxs-lookup"><span data-stu-id="0af79-178">This approach is only appropriate if hello queries you're performing are not part of your core business logic.</span></span>


### <a name="whats-hello-best-way-tooquery-data-across-my-actors"></a><span data-ttu-id="0af79-179">Mi az a hello legjobb módja tooquery adatok a szereplője között?</span><span class="sxs-lookup"><span data-stu-id="0af79-179">What's hello best way tooquery data across my actors?</span></span>

<span data-ttu-id="0af79-180">Szereplője tervezett toobe független egység állapot és számítási, így nem ajánlott tooperform aktorállapot futásidőben széleskörű lekérdezését.</span><span class="sxs-lookup"><span data-stu-id="0af79-180">Actors are designed toobe independent units of state and compute, so it is not recommended tooperform broad queries of actor state at runtime.</span></span> <span data-ttu-id="0af79-181">Ha egy szükséges tooquery keresztül hello aktorállapot teljes készletét, érdemes vagy:</span><span class="sxs-lookup"><span data-stu-id="0af79-181">If you have a need tooquery across hello full set of actor state, you should consider either:</span></span>

- <span data-ttu-id="0af79-182">A aktorszolgáltatások cseréje az állapot-nyilvántartó megbízható szolgáltatásokat, úgy, hogy a hálózati hello száma toogather összes adatokat kér hello szereplője toohello száma a szolgáltatás a partíciók száma.</span><span class="sxs-lookup"><span data-stu-id="0af79-182">Replacing your actor services with stateful reliable services, such that hello number of network requests toogather all data from hello number of actors toohello number of partitions in your service.</span></span>
- <span data-ttu-id="0af79-183">A szereplője tooperiodically leküldéses tervezése a tooan külső állapottárolóhoz könnyebb lekérdezése.</span><span class="sxs-lookup"><span data-stu-id="0af79-183">Designing your actors tooperiodically push their state tooan external store for easier querying.</span></span> <span data-ttu-id="0af79-184">Újabb verzióiban ez a megközelítés lesz csak kivitelezhető, ha a helyreállítást hajt végre hello lekérdezések esetén nincs szükség a működését.</span><span class="sxs-lookup"><span data-stu-id="0af79-184">As above, this approach is only viable if hello queries you're performing are not required for your runtime behavior.</span></span>

### <a name="how-much-data-can-i-store-in-a-reliable-collection"></a><span data-ttu-id="0af79-185">Mennyi adatot tud tárolni egy megbízható gyűjtemény?</span><span class="sxs-lookup"><span data-stu-id="0af79-185">How much data can I store in a Reliable Collection?</span></span>

<span data-ttu-id="0af79-186">Megbízható szolgáltatások általában particionáltak, így hello tárolhatja csak korlátozza, és a hello ezeken a számítógépeken elérhető memória mennyiségének hello fürt rendelkezik gépek hello száma.</span><span class="sxs-lookup"><span data-stu-id="0af79-186">Reliable services are typically partitioned, so hello amount you can store is only limited by hello number of machines you have in hello cluster, and hello amount of memory available on those machines.</span></span>

<span data-ttu-id="0af79-187">Tegyük fel tegyük fel, hogy egy megbízható gyűjtemény 100 partíciók 3 replikák egy szolgáltatást a 1kb méretű átlagos objektumok tárolására.</span><span class="sxs-lookup"><span data-stu-id="0af79-187">As an example, suppose that you have a reliable collection in a service with 100 partitions and 3 replicas, storing objects that average 1kb in size.</span></span> <span data-ttu-id="0af79-188">Most tegyük fel, hogy rendelkezik-e a 16gb memóriája gépenként 10 gép fürtben.</span><span class="sxs-lookup"><span data-stu-id="0af79-188">Now suppose that you have a 10 machine cluster with 16gb of memory per machine.</span></span> <span data-ttu-id="0af79-189">Az egyszerűség és nagyon óvatos toobe azt feltételezik, hogy hello operációs rendszerek és rendszerszolgáltatások hello Service Fabric-futtatókörnyezet és a szolgáltatások felhasználásához 6gb, így a 10 GB-os gépenként érhető el, vagy 100gb hello fürt.</span><span class="sxs-lookup"><span data-stu-id="0af79-189">For simplicity and toobe very conservative, assume that hello operating system and system services, hello Service Fabric runtime, and your services consume 6gb of that, leaving 10gb available per machine, or 100gb for hello cluster.</span></span>

<span data-ttu-id="0af79-190">Figyelembe vételével kell lennie a minden objektumon tárolt három időpontokban (egy elsődleges és két replika), akkor egy körülbelül 35 millió objektumok elegendő memória áll rendelkezésre a gyűjteményben teljes kapacitás üzemi.</span><span class="sxs-lookup"><span data-stu-id="0af79-190">Keeping in mind that each object must be stored three times (one primary and two replicas), you would have sufficient memory for approximately 35 million objects in your collection when operating at full capacity.</span></span> <span data-ttu-id="0af79-191">Azt javasoljuk azonban egy hiba tartományban, és egy frissítési tartomány, amely körülbelül 1/3 kapacitás, és a csökkenne hello számú tooroughly 23 millió egyidejű megszűnését rugalmas toohello alatt.</span><span class="sxs-lookup"><span data-stu-id="0af79-191">However, we recommend being resilient toohello simultaneous loss of a failure domain and an upgrade domain, which represents about 1/3 of capacity, and would reduce hello number tooroughly 23 million.</span></span>

<span data-ttu-id="0af79-192">Vegye figyelembe, hogy a számítási is feltételezi, hogy:</span><span class="sxs-lookup"><span data-stu-id="0af79-192">Note that this calculation also assumes:</span></span>

- <span data-ttu-id="0af79-193">Hogy az adatok így vannak elrendezve hello partíciók hello terjesztési többé-kevésbé egységes, és, hogy van-e reporting terhelési metrika toohello fürt erőforrás-kezelő.</span><span class="sxs-lookup"><span data-stu-id="0af79-193">That hello distribution of data across hello partitions is roughly uniform or that you're reporting load metrics toohello Cluster Resource Manager.</span></span> <span data-ttu-id="0af79-194">Alapértelmezés szerint a Service Fabric rendszer terheléselosztásához replikáinak száma alapján.</span><span class="sxs-lookup"><span data-stu-id="0af79-194">By default, Service Fabric will load balance based on replica count.</span></span> <span data-ttu-id="0af79-195">A fenti példában, célszerű helyezni 10 elsődleges replika és 20 másodlagos replikák hello fürt minden csomópontjának.</span><span class="sxs-lookup"><span data-stu-id="0af79-195">In our example above, that would put 10 primary replicas and 20 secondary replicas on each node in hello cluster.</span></span> <span data-ttu-id="0af79-196">Betöltési, amely egyenlően elosztva hello partíciók esetén, amelyek működik.</span><span class="sxs-lookup"><span data-stu-id="0af79-196">That works well for load that is evenly distributed across hello partitions.</span></span> <span data-ttu-id="0af79-197">Betöltési nincs még akkor is, ha jelenteniük kell terhelést, hogy hello erőforrás-kezelő is kisebb replikák együtt csomag és annak engedélyezése, hogy nagyobb replikák tooconsume több memóriát az egyes csomópontok.</span><span class="sxs-lookup"><span data-stu-id="0af79-197">If load is not even, you must report load so that hello Resource Manager can pack smaller replicas together and allow larger replicas tooconsume more memory on an individual node.</span></span>

- <span data-ttu-id="0af79-198">A szóban forgó hello megbízható szolgáltatás hello fürt tárolni állapot csak egy hello.</span><span class="sxs-lookup"><span data-stu-id="0af79-198">That hello reliable service in question is hello only one storing state in hello cluster.</span></span> <span data-ttu-id="0af79-199">Mivel több szolgáltatás tooa fürtök telepítése, kell toobe szem előtt tartva hello erőforrásokat, hogy minden egyes fog toorun kell, és az állapot kezelése.</span><span class="sxs-lookup"><span data-stu-id="0af79-199">Since you can deploy multiple services tooa cluster, you need toobe mindful of hello resources that each will need toorun and manage its state.</span></span>

- <span data-ttu-id="0af79-200">Adott hello fürtnek nincs növekvő vagy zsugorítását.</span><span class="sxs-lookup"><span data-stu-id="0af79-200">That hello cluster itself is not growing or shrinking.</span></span> <span data-ttu-id="0af79-201">További gépek hozzáadásakor a Service Fabric egyensúlyba a replikák tooleverage hello további kapacitást, amíg gépek hello száma meghaladja a hello száma a szolgáltatás a partíciók, mivel az egyes replika gépek nem terjedhet ki.</span><span class="sxs-lookup"><span data-stu-id="0af79-201">If you add more machines, Service Fabric will rebalance your replicas tooleverage hello additional capacity until hello number of machines surpasses hello number of partitions in your service, since an individual replica cannot span machines.</span></span> <span data-ttu-id="0af79-202">Ezzel szemben ha gépek eltávolításával csökkentenie hello fürt hello méretét, a replikák szigorúbban csomagolt tartalmaz, és teljes kapacitásának rendelkezik.</span><span class="sxs-lookup"><span data-stu-id="0af79-202">By contrast, if you reduce hello size of hello cluster by removing machines, your replicas will be packed more tightly and have less overall capacity.</span></span>

### <a name="how-much-data-can-i-store-in-an-actor"></a><span data-ttu-id="0af79-203">Mennyi adatot is tárolnak a egy szereplő?</span><span class="sxs-lookup"><span data-stu-id="0af79-203">How much data can I store in an actor?</span></span>

<span data-ttu-id="0af79-204">Csakúgy, mint a megbízható szolgáltatások hello adatmennyiséget szereplő szolgáltatásnak tárolhat csak korlátozza hello teljes lemezterület és a rendelkezésre álló memória hello a fürtben található csomópontok között.</span><span class="sxs-lookup"><span data-stu-id="0af79-204">As with reliable services, hello amount of data that you can store in an actor service is only limited by hello total disk space and memory available across hello nodes in your cluster.</span></span> <span data-ttu-id="0af79-205">Azonban egyedi szereplője esetén leghatékonyabb használt tooencapsulate állapot és a kapcsolódó üzleti logika kis mennyiségű.</span><span class="sxs-lookup"><span data-stu-id="0af79-205">However, individual actors are most effective when they are used tooencapsulate a small amount of state and associated business logic.</span></span> <span data-ttu-id="0af79-206">Általános szabályként elmondható egy egyéni szereplő, amelyet a rendszer kilobájtban kell rendelkeznie.</span><span class="sxs-lookup"><span data-stu-id="0af79-206">As a general rule, an individual actor should have state that is measured in kilobytes.</span></span>

## <a name="other-questions"></a><span data-ttu-id="0af79-207">Egyéb kérdések</span><span class="sxs-lookup"><span data-stu-id="0af79-207">Other questions</span></span>

### <a name="how-does-service-fabric-relate-toocontainers"></a><span data-ttu-id="0af79-208">Milyen kapcsolatban áll a Service Fabric toocontainers?</span><span class="sxs-lookup"><span data-stu-id="0af79-208">How does Service Fabric relate toocontainers?</span></span>

<span data-ttu-id="0af79-209">Tárolók ajánlatot egy egyszerű módon toopackage szolgáltatások és függőségi viszonyaikat, úgy, hogy következetesen minden környezetben futtatni, és egy gépen csak elkülönített módon kapnak.</span><span class="sxs-lookup"><span data-stu-id="0af79-209">Containers offer a simple way toopackage services and their dependencies such that they run consistently in all environments and can operate in an isolated fashion on a single machine.</span></span> <span data-ttu-id="0af79-210">A Service Fabric egy módon toodeploy nyújt, és szolgáltatásaink, így kezelése [, amely rendelkezik egy tárolóba van csomagolva szolgáltatások](service-fabric-containers-overview.md).</span><span class="sxs-lookup"><span data-stu-id="0af79-210">Service Fabric offers a way toodeploy and manage services, including [services that have been packaged in a container](service-fabric-containers-overview.md).</span></span>

### <a name="are-you-planning-tooopen-source-service-fabric"></a><span data-ttu-id="0af79-211">A Service Fabric tooopen forrás tervezési?</span><span class="sxs-lookup"><span data-stu-id="0af79-211">Are you planning tooopen source Service Fabric?</span></span>

<span data-ttu-id="0af79-212">Azt tervezi, tooopen forrás hello reliable services és a megbízható szereplője keretrendszerek a Githubon, és elfogadja a közösségi hozzájárulásokat toothose projektek.</span><span class="sxs-lookup"><span data-stu-id="0af79-212">We intend tooopen source hello reliable services and reliable actors frameworks on GitHub and will accept community contributions toothose projects.</span></span> <span data-ttu-id="0af79-213">Kövesse a hello [Service Fabric blog](https://blogs.msdn.microsoft.com/azureservicefabric/) azok még jelent további részletekért.</span><span class="sxs-lookup"><span data-stu-id="0af79-213">Please follow hello [Service Fabric blog](https://blogs.msdn.microsoft.com/azureservicefabric/) for more details as they're announced.</span></span>

<span data-ttu-id="0af79-214">hello jelenleg nincs tervek tooopen forrás hello Service Fabric-futtatókörnyezet.</span><span class="sxs-lookup"><span data-stu-id="0af79-214">hello are currently no plans tooopen source hello Service Fabric runtime.</span></span>

## <a name="next-steps"></a><span data-ttu-id="0af79-215">Következő lépések</span><span class="sxs-lookup"><span data-stu-id="0af79-215">Next steps</span></span>

- [<span data-ttu-id="0af79-216">További tudnivalók a Service Fabric Alapfogalmak és ajánlott eljárások</span><span class="sxs-lookup"><span data-stu-id="0af79-216">Learn about core Service Fabric concepts and best practices</span></span>](https://mva.microsoft.com/en-us/training-courses/building-microservices-applications-on-azure-service-fabric-16747?l=tbuZM46yC_5206218965)
