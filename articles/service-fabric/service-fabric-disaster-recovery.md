---
title: "a Service Fabric vész-helyreállítási aaaAzure |} Microsoft Docs"
description: "Az Azure Service Fabric hello képességek szükséges toodeal katasztrófák minden típusú kínál. Ez a cikk ismerteti a hello típusú katasztrófák, amik akkor léphetnek fel, és hogyan toodeal velük."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 04b8348fb63e8a1c76a8f722c4c8255b339908e2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 10/06/2017
---
# <a name="disaster-recovery-in-azure-service-fabric"></a><span data-ttu-id="c8831-104">Az Azure Service Fabric katasztrófa utáni helyreállítás</span><span class="sxs-lookup"><span data-stu-id="c8831-104">Disaster recovery in Azure Service Fabric</span></span>
<span data-ttu-id="c8831-105">A magas rendelkezésre állású kézbesítéséhez kritikus része annak ellenőrzése, hogy a szolgáltatások hibatűrését összes különböző típusú hibák.</span><span class="sxs-lookup"><span data-stu-id="c8831-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span></span> <span data-ttu-id="c8831-106">Ez különösen fontos a nem tervezett hibák és a vezérlőn kívül.</span><span class="sxs-lookup"><span data-stu-id="c8831-106">This is especially important for failures that are unplanned and outside of your control.</span></span> <span data-ttu-id="c8831-107">Ez a cikk ismerteti az egyes közös függően katasztrófák lehet, ha nem modellezve, és a megfelelő felügyelt.</span><span class="sxs-lookup"><span data-stu-id="c8831-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span></span> <span data-ttu-id="c8831-108">Is tárgyalja azok mérséklési és műveletek tootake, ha egy olyan vészhelyzet esetén mégis történt.</span><span class="sxs-lookup"><span data-stu-id="c8831-108">It also discuss mitigations and actions tootake if a disaster happened anyway.</span></span> <span data-ttu-id="c8831-109">hello cél toolimit vagy leállás vagy adatvesztés kockázatát hello kiküszöbölheti a hibák, a tervezett fordulhat elő, vagy ellenkező esetben fordulhat elő, amikor.</span><span class="sxs-lookup"><span data-stu-id="c8831-109">hello goal is toolimit or eliminate hello risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span></span>

## <a name="avoiding-disaster"></a><span data-ttu-id="c8831-110">Vészhelyreállítás elkerülése</span><span class="sxs-lookup"><span data-stu-id="c8831-110">Avoiding disaster</span></span>
<span data-ttu-id="c8831-111">A Service Fabric elsődleges célja, mind a környezettől, és a szolgáltatások úgy, hogy általános hiba típusok nincsenek katasztrófák modell toohelp.</span><span class="sxs-lookup"><span data-stu-id="c8831-111">Service Fabric's primary goal is toohelp you model both your environment and your services in such a way that common failure types are not disasters.</span></span> 

<span data-ttu-id="c8831-112">Általános vész és sikertelen forgatókönyvek két típusa van:</span><span class="sxs-lookup"><span data-stu-id="c8831-112">In general there are two types of disaster/failure scenarios:</span></span>

1. <span data-ttu-id="c8831-113">Hardver- vagy hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-113">Hardware or software faults</span></span>
2. <span data-ttu-id="c8831-114">Működési hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-114">Operational faults</span></span>

### <a name="hardware-and-software-faults"></a><span data-ttu-id="c8831-115">Hardver- és hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-115">Hardware and software faults</span></span>
<span data-ttu-id="c8831-116">Hardver- és hibák előre nem látható.</span><span class="sxs-lookup"><span data-stu-id="c8831-116">Hardware and software faults are unpredictable.</span></span> <span data-ttu-id="c8831-117">hello legegyszerűbb módja toosurvive hibák hardver- vagy tartalék határain túlnyúló ölel hello szolgáltatás több példánya fut.</span><span class="sxs-lookup"><span data-stu-id="c8831-117">hello easiest way toosurvive faults is running more copies of hello service  spanned across hardware or software fault boundaries.</span></span> <span data-ttu-id="c8831-118">Például ha a szolgáltatás csak egy adott számítógép fut, majd hello, hogy egy géphez hibája, hogy a szolgáltatás egy olyan vészhelyzet esetén.</span><span class="sxs-lookup"><span data-stu-id="c8831-118">For example, if your service is running only on one particular machine, then hello failure of that one machine is a disaster for that service.</span></span> <span data-ttu-id="c8831-119">hello egyszerűen tooavoid a katasztrófa tooensure, amely hello szolgáltatás ténylegesen fut a több számítógép.</span><span class="sxs-lookup"><span data-stu-id="c8831-119">hello simple way tooavoid this disaster is tooensure that hello service is actually running on multiple machines.</span></span> <span data-ttu-id="c8831-120">Tesztelés az is szükséges tooensure hello sikertelen volt-e egy számítógép nem zavarja hello szolgáltatást futtató.</span><span class="sxs-lookup"><span data-stu-id="c8831-120">Testing is also necessary tooensure hello failure of one machine doesn't disrupt hello running service.</span></span> <span data-ttu-id="c8831-121">Egy helyettesítő példányt is létrehozható máshol és, hogy kapacitásának csökkentése nem túlterhelés fennmaradó szolgáltatások hello kapacitástervezés biztosítja.</span><span class="sxs-lookup"><span data-stu-id="c8831-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload hello remaining services.</span></span> <span data-ttu-id="c8831-122">hello ugyanilyen mintájú dolog, amit tooavoid hello meghibásodása függetlenül működik.</span><span class="sxs-lookup"><span data-stu-id="c8831-122">hello same pattern works regardless of what you're trying tooavoid hello failure of.</span></span> <span data-ttu-id="c8831-123">Például.</span><span class="sxs-lookup"><span data-stu-id="c8831-123">For example.</span></span> <span data-ttu-id="c8831-124">Ha aggódik hello sikertelen volt-e a TÁROLÓHÁLÓZAT, több San-okkal keresztül futtatja.</span><span class="sxs-lookup"><span data-stu-id="c8831-124">if you're concerned about hello failure of a SAN, you run across multiple SANs.</span></span> <span data-ttu-id="c8831-125">Ha aggódik kiszolgálók állvány hello elvesztését, akkor több állványra futtatása.</span><span class="sxs-lookup"><span data-stu-id="c8831-125">If you're concerned about hello loss of a rack of servers, you run across multiple racks.</span></span> <span data-ttu-id="c8831-126">Ha aggódik adatközpontok hello adatvesztés, a szolgáltatás a több Azure-régiók vagy adatközpontok futhat.</span><span class="sxs-lookup"><span data-stu-id="c8831-126">If you're worried about hello loss of datacenters, your service should run across multiple Azure regions or datacenters.</span></span> 

<span data-ttu-id="c8831-127">Ha ilyen típusú átnyúló módban fut, még mindig tulajdonos toosome típusú egyidejű hibák, de egyetlen és még több hibák számát egy adott típusának (például: egyetlen virtuális gép vagy a hálózati kapcsolat sikertelen) kezeli automatikusan (és már nem a "vész").</span><span class="sxs-lookup"><span data-stu-id="c8831-127">When running in this type of spanned mode, you're still subject toosome types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span></span> <span data-ttu-id="c8831-128">A Service Fabric a bővített hello fürt és kezeli a hogy vissza a meghibásodott csomópontok és a szolgáltatások számos mechanizmust biztosít.</span><span class="sxs-lookup"><span data-stu-id="c8831-128">Service Fabric provides many mechanisms for expanding hello cluster and handles bringing failed nodes and services back.</span></span> <span data-ttu-id="c8831-129">A Service Fabric is lehetővé teszi, hogy a szolgáltatások több példányát rendelés tooavoid a nem tervezett leállások ilyen típusú forduljon rendszert futtató valódi katasztrófa.</span><span class="sxs-lookup"><span data-stu-id="c8831-129">Service Fabric also allows running many instances of your services in order tooavoid these types of unplanned failures from turning into real disasters.</span></span>

<span data-ttu-id="c8831-130">Miért egy központi telepítési elég nagy toospan hibák az állomásokon futó esetén nem valósítható meg oka lehet.</span><span class="sxs-lookup"><span data-stu-id="c8831-130">There may be reasons why running a deployment large enough toospan over failures is not feasible.</span></span> <span data-ttu-id="c8831-131">Például további hardver-erőforrások, mint a most nem kívánok eltarthat toopay a hiba relatív toohello esélyét.</span><span class="sxs-lookup"><span data-stu-id="c8831-131">For example, it may take more hardware resources than you're not willing toopay for relative toohello chance of failure.</span></span> <span data-ttu-id="c8831-132">Elosztott alkalmazások a meghatározásakor, hogy további kommunikációs ugrások vagy állapot replikációs költségek földrajzi távolság okok elfogadhatatlan késleltetés között lehet.</span><span class="sxs-lookup"><span data-stu-id="c8831-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span></span> <span data-ttu-id="c8831-133">Ha ezt a sort megrajzolása eltér az egyes alkalmazásokhoz.</span><span class="sxs-lookup"><span data-stu-id="c8831-133">Where this line is drawn differs for each application.</span></span> <span data-ttu-id="c8831-134">Szoftver hibák pontosabban hello hiba lehet hello szolgáltatásban tooscale próbált.</span><span class="sxs-lookup"><span data-stu-id="c8831-134">For software faults specifically, hello fault could be in hello service that you are trying tooscale.</span></span> <span data-ttu-id="c8831-135">Ebben az esetben több példányt nem akadályozzák meg a hello katasztrófa, mivel hello hibafeltétel minden hello példányára visszamenőleges korrelációban állnak.</span><span class="sxs-lookup"><span data-stu-id="c8831-135">In this case more copies don't prevent hello disaster, since hello failure condition is correlated across all hello instances.</span></span>

### <a name="operational-faults"></a><span data-ttu-id="c8831-136">Működési hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-136">Operational faults</span></span>
<span data-ttu-id="c8831-137">Akkor is, ha a szolgáltatás a sok létszámcsökkentések hello földgömb van átnyúlhatnak, továbbra is fedezheti katasztrofális esemény.</span><span class="sxs-lookup"><span data-stu-id="c8831-137">Even if your service is spanned across hello globe with many redundancies, it can still experience disastrous events.</span></span> <span data-ttu-id="c8831-138">Ha például valaki véletlenül újrakonfigurálja hello szolgáltatás hello DNS-nevét, vagy törlése végleges.</span><span class="sxs-lookup"><span data-stu-id="c8831-138">For example, if someone accidentally reconfigures hello dns name for hello service, or deletes it outright.</span></span> <span data-ttu-id="c8831-139">Tegyük fel tegyük fel, az állapotalapú Service Fabric-szolgáltatás volt, és valaki az adott szolgáltatás véletlenül törölt.</span><span class="sxs-lookup"><span data-stu-id="c8831-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span></span> <span data-ttu-id="c8831-140">Kivéve, ha egy másik megoldás, hogy a szolgáltatás és az összes hello állapotban volt van most már meg.</span><span class="sxs-lookup"><span data-stu-id="c8831-140">Unless there's some other mitigation, that service and all of hello state it had is now gone.</span></span> <span data-ttu-id="c8831-141">Működési katasztrófák ilyen típusú ("sajnos") különböző megoldást és lépéseket helyreállításához szükséges rendszeres nem tervezett leállások-nál.</span><span class="sxs-lookup"><span data-stu-id="c8831-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span></span> 

<span data-ttu-id="c8831-142">Ezek a típusok működési hibáinak éppen hello legjobb módjai tooavoid</span><span class="sxs-lookup"><span data-stu-id="c8831-142">hello best ways tooavoid these types of operational faults are to</span></span>
1. <span data-ttu-id="c8831-143">működési access toohello környezet korlátozása</span><span class="sxs-lookup"><span data-stu-id="c8831-143">restrict operational access toohello environment</span></span>
2. <span data-ttu-id="c8831-144">szigorúan naplózási veszélyes műveletek</span><span class="sxs-lookup"><span data-stu-id="c8831-144">strictly audit dangerous operations</span></span>
3. <span data-ttu-id="c8831-145">automatizálási ugyanazok, megakadályozza a manuális és a módosítások sávon kívüli és hello tényleges környezet ellen adott módosítások érvényesítése előtt életbe őket</span><span class="sxs-lookup"><span data-stu-id="c8831-145">impose automation, prevent manual or out of band changes, and validate specific changes against hello actual environment before enacting them</span></span>
4. <span data-ttu-id="c8831-146">Gondoskodjon arról, hogy ilyen beállítások mellett pusztító műveletek "Soft szót".</span><span class="sxs-lookup"><span data-stu-id="c8831-146">ensure that destructive operations are "soft".</span></span> <span data-ttu-id="c8831-147">Szoftveres műveletek nem azonnal lépnek érvénybe, vagy visszavonható néhány időkerete belül</span><span class="sxs-lookup"><span data-stu-id="c8831-147">Soft operations don't take effect immediately or can be undone within some time window</span></span>

<span data-ttu-id="c8831-148">A Service Fabric biztosít bizonyos mechanizmusok tooprevent működési hibák, így például [szerepköralapú](service-fabric-cluster-security-roles.md) hozzáférés-vezérlés a fürt működését.</span><span class="sxs-lookup"><span data-stu-id="c8831-148">Service Fabric provides some mechanisms tooprevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span></span> <span data-ttu-id="c8831-149">A működési hibák többségét kérhetnek szervezeti erőfeszítéseket és más rendszerek.</span><span class="sxs-lookup"><span data-stu-id="c8831-149">However, most of these operational faults require organizational efforts and other systems.</span></span> <span data-ttu-id="c8831-150">A Service Fabric egyes fennmaradó működési hibák, ilyen például a biztonsági mentési mechanizmus biztosítása, és állítsa vissza az állapotalapú szolgáltatások.</span><span class="sxs-lookup"><span data-stu-id="c8831-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span></span>

## <a name="managing-failures"></a><span data-ttu-id="c8831-151">Hibák kezelése</span><span class="sxs-lookup"><span data-stu-id="c8831-151">Managing failures</span></span>
<span data-ttu-id="c8831-152">hello Service Fabric célja szinte mindig automatikus hiba felügyeletét.</span><span class="sxs-lookup"><span data-stu-id="c8831-152">hello goal of Service Fabric is almost always automatic management of failures.</span></span> <span data-ttu-id="c8831-153">Azonban a rendezés toohandle hibák bizonyos típusú, a szolgáltatások további kódot kell rendelkeznie.</span><span class="sxs-lookup"><span data-stu-id="c8831-153">However, in order toohandle some types of failures, services must have additional code.</span></span> <span data-ttu-id="c8831-154">Más típusú hibák kell _nem_ automatikusan címezhető biztonsági és üzleti folytonossági okok miatt.</span><span class="sxs-lookup"><span data-stu-id="c8831-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span></span> 

### <a name="handling-single-failures"></a><span data-ttu-id="c8831-155">Egyetlen hibák kezelése</span><span class="sxs-lookup"><span data-stu-id="c8831-155">Handling single failures</span></span>
<span data-ttu-id="c8831-156">Egyetlen gépek mindenféle okból sikertelen lehet.</span><span class="sxs-lookup"><span data-stu-id="c8831-156">Single machines can fail for all sorts of reasons.</span></span> <span data-ttu-id="c8831-157">Ezek némelyike hardver oka van, például a áramforrások és a hálózati hardver meghibásodása.</span><span class="sxs-lookup"><span data-stu-id="c8831-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span></span> <span data-ttu-id="c8831-158">Szoftver más hibák vannak.</span><span class="sxs-lookup"><span data-stu-id="c8831-158">Other failures are in software.</span></span> <span data-ttu-id="c8831-159">Ezek közé tartozik a hibák hello tényleges operációs rendszer és hello szolgáltatást.</span><span class="sxs-lookup"><span data-stu-id="c8831-159">These include failures of hello actual operating system and hello service itself.</span></span> <span data-ttu-id="c8831-160">A Service Fabric automatikusan észleli a hibákat, beleértve azokat az eseteket, ahol hello gép válik különítve a többi gép toonetwork problémák miatt az ilyen típusú.</span><span class="sxs-lookup"><span data-stu-id="c8831-160">Service Fabric automatically detects these types of failures, including cases where hello machine becomes isolated from other machines due toonetwork issues.</span></span>

<span data-ttu-id="c8831-161">Hello szolgáltatás típusa, függetlenül fut egy példányban eredmények állásidőt, hogy a szolgáltatás Ha hello kódot, hogy egyetlen példányát bármilyen okból nem sikerül.</span><span class="sxs-lookup"><span data-stu-id="c8831-161">Regardless of hello type of service, running a single instance results in downtime for that service if that single copy of hello code fails for any reason.</span></span> 

<span data-ttu-id="c8831-162">Rendelés toohandle minden egyetlen hiba hello legegyszerűbb művelet, amelyet, amely alapértelmezés szerint több csomópontján futtatni a szolgáltatások tooensure.</span><span class="sxs-lookup"><span data-stu-id="c8831-162">In order toohandle any single failure, hello simplest thing you can do is tooensure that your services run on more than one node by default.</span></span> <span data-ttu-id="c8831-163">Az állapotmentes szolgáltatások ehhez azzal, hogy egy `InstanceCount` 1-nél nagyobb.</span><span class="sxs-lookup"><span data-stu-id="c8831-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span></span> <span data-ttu-id="c8831-164">Állapotalapú szolgáltatások hello minimális ajánljuk, mindig egy `TargetReplicaSetSize` és `MinReplicaSetSize` legalább 3.</span><span class="sxs-lookup"><span data-stu-id="c8831-164">For stateful services, hello minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span></span> <span data-ttu-id="c8831-165">A szolgáltatáskód hibáit további példányait futtató biztosítja, hogy a szolgáltatás képes automatikusan kezelni minden egyetlen hiba.</span><span class="sxs-lookup"><span data-stu-id="c8831-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span></span> 

### <a name="handling-coordinated-failures"></a><span data-ttu-id="c8831-166">Kezelési koordinált hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-166">Handling coordinated failures</span></span>
<span data-ttu-id="c8831-167">Koordinált hiba akkor fordulhat elő, a fürt megfelelő tooeither tervezett vagy nem tervezett infrastruktúra hibák és módosításokat, vagy tervezett szoftver változásainak.</span><span class="sxs-lookup"><span data-stu-id="c8831-167">Coordinated failures can happen in a cluster due tooeither planned or unplanned infrastructure failures and changes, or planned software changes.</span></span> <span data-ttu-id="c8831-168">A Service Fabric modellek infrastruktúra zónák, hogy a tartalék tartományok, koordinált hibák.</span><span class="sxs-lookup"><span data-stu-id="c8831-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span></span> <span data-ttu-id="c8831-169">Frissítési tartományok, amelyeknek a koordinált szoftvermódosítások területek van modellezve.</span><span class="sxs-lookup"><span data-stu-id="c8831-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span></span> <span data-ttu-id="c8831-170">További információ a tartalék és a frissítési tartományok van [Ez a dokumentum](service-fabric-cluster-resource-manager-cluster-description.md) , amely leírja a fürt topológia és definíciója.</span><span class="sxs-lookup"><span data-stu-id="c8831-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span></span>

<span data-ttu-id="c8831-171">Alapértelmezés szerint a Service Fabric hiba és a frissítési tartományok tekinti, ahol kell futtatni a szolgáltatások tervezése során.</span><span class="sxs-lookup"><span data-stu-id="c8831-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span></span> <span data-ttu-id="c8831-172">Alapértelmezés szerint a Service Fabric megpróbál tooensure, amely a szolgáltatások több hiba és a frissítési tartományok közötti futtatni, hogy a tervezett vagy nem tervezett módosítások fordulhat elő, ha a szolgáltatások elérhetők maradnak.</span><span class="sxs-lookup"><span data-stu-id="c8831-172">By default, Service Fabric tries tooensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span></span> 

<span data-ttu-id="c8831-173">Tételezzük fel például, hogy sikertelen volt-e az áramforrás egyidejűleg okozza-e a gépek toofail rögzítve.</span><span class="sxs-lookup"><span data-stu-id="c8831-173">For example, let's say that failure of a power source causes a rack of machines toofail simultaneously.</span></span> <span data-ttu-id="c8831-174">Több gép hello megszűnését futó és tartalék tartomány hibát hello szolgáltatás több példányával kapcsolja be egy másik példa egy adott szolgáltatáshoz egyetlen hiba.</span><span class="sxs-lookup"><span data-stu-id="c8831-174">With multiple copies of hello service running hello loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span></span> <span data-ttu-id="c8831-175">Éppen ezért a tartalék tartományok kezelése többé már kritikus tooensuring magas rendelkezésre állású a szolgáltatásokat.</span><span class="sxs-lookup"><span data-stu-id="c8831-175">This is why managing fault domains is critical tooensuring high availability of your services.</span></span> <span data-ttu-id="c8831-176">Az Azure Service Fabric futtatásakor tartalék tartományok automatikusan kezeli.</span><span class="sxs-lookup"><span data-stu-id="c8831-176">When running Service Fabric in Azure, fault domains are managed automatically.</span></span> <span data-ttu-id="c8831-177">Más környezetekben nem lehetnek.</span><span class="sxs-lookup"><span data-stu-id="c8831-177">In other environments they may not be.</span></span> <span data-ttu-id="c8831-178">Ha a saját telephelyükön található fürtök most létre, meg arról, hogy toomap és a tartalék tartomány elrendezését tervezik megfelelően.</span><span class="sxs-lookup"><span data-stu-id="c8831-178">If you're building your own clusters on premises, be sure toomap and plan your fault domain layout correctly.</span></span>

<span data-ttu-id="c8831-179">Frissítési tartományok akkor hasznosak, ahol szoftver érintetlen toobe frissítheti hello azonos területek modellezési idő.</span><span class="sxs-lookup"><span data-stu-id="c8831-179">Upgrade Domains are useful for modeling areas where software is going toobe upgraded at hello same time.</span></span> <span data-ttu-id="c8831-180">Ebből kifolyólag frissítési tartományok is gyakran hello határokat, ahol szoftvereket van leállítaná tervezett frissítéskor határozza meg.</span><span class="sxs-lookup"><span data-stu-id="c8831-180">Because of this, Upgrade Domains also often define hello boundaries where software is taken down during planned upgrades.</span></span> <span data-ttu-id="c8831-181">A Service Fabric- és a szolgáltatások frissítéseket kövesse hello ugyanannak a modellnek.</span><span class="sxs-lookup"><span data-stu-id="c8831-181">Upgrades of both Service Fabric and your services follow hello same model.</span></span> <span data-ttu-id="c8831-182">A működés közbeni frissítés frissítési tartományok és hello Service Fabric állapotmodell, amelyek segítségével a nemkívánatos módosítások megakadályozása hello fürt és a szolgáltatást érintő kapcsolatban bővebben lásd: a dokumentumok:</span><span class="sxs-lookup"><span data-stu-id="c8831-182">For more on rolling upgrades, upgrade domains, and hello Service Fabric health model that helps prevent unintended changes from impacting hello cluster and your service, see these documents:</span></span>

 - [<span data-ttu-id="c8831-183">Az alkalmazásfrissítés</span><span class="sxs-lookup"><span data-stu-id="c8831-183">Application Upgrade</span></span>](service-fabric-application-upgrade.md)
 - [<span data-ttu-id="c8831-184">Frissítési vonatkozó oktatóanyag</span><span class="sxs-lookup"><span data-stu-id="c8831-184">Application Upgrade Tutorial</span></span>](service-fabric-application-upgrade-tutorial.md)
 - [<span data-ttu-id="c8831-185">Service Fabric Állapotmodell</span><span class="sxs-lookup"><span data-stu-id="c8831-185">Service Fabric Health Model</span></span>](service-fabric-health-introduction.md)

<span data-ttu-id="c8831-186">A megadott hello betűcsoport-leképezés használatával fürt hello elrendezés jelenítheti meg [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span><span class="sxs-lookup"><span data-stu-id="c8831-186">You can visualize hello layout of your cluster using hello cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span></span>

<span data-ttu-id="c8831-187"><center>
![A Service Fabric Explorerben tartalék tartományok elosztva csomópontok][sfx-cluster-map]
</center></span><span class="sxs-lookup"><span data-stu-id="c8831-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span></span>

> [!NOTE]
> <span data-ttu-id="c8831-188">Hiba: a működés közbeni frissítés, a szolgáltatáskód hibáit és a állapot több példányát futtató területéhez modellezési elhelyezési szabályokat tooensure a szolgáltatások futnak hiba és a frissítési tartományok között, és beépített állapotfigyelés csak **néhány** a hello Service Fabric rendelés tookeep normál működési problémák és hibák forduljon az katasztrófák biztosító funkciókat.</span><span class="sxs-lookup"><span data-stu-id="c8831-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules tooensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of hello features that Service Fabric provides in order tookeep normal operational issues and failures from turning into disasters.</span></span> 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a><span data-ttu-id="c8831-189">Egyidejű hardver- vagy hibák kezelése</span><span class="sxs-lookup"><span data-stu-id="c8831-189">Handling simultaneous hardware or software failures</span></span>
<span data-ttu-id="c8831-190">Újabb megtartásról egyetlen hibákról.</span><span class="sxs-lookup"><span data-stu-id="c8831-190">Above we talked about single failures.</span></span> <span data-ttu-id="c8831-191">Látható, a rendszer mind az állapotmentes és állapotalapú szolgáltatások könnyen toohandle csak tartalék és verziófrissítési futtatásáért hello kódot (és állapot) további példányait tartja.</span><span class="sxs-lookup"><span data-stu-id="c8831-191">As you can see, are easy toohandle for both stateless and stateful services just by keeping more copies of hello code (and state) running across fault and upgrade domains.</span></span> <span data-ttu-id="c8831-192">Több egyidejű véletlen hibákat is megtörténhet.</span><span class="sxs-lookup"><span data-stu-id="c8831-192">Multiple simultaneous random failures can also happen.</span></span> <span data-ttu-id="c8831-193">Ezek a tényleges katasztrófa nagy valószínűséggel toolead tooan.</span><span class="sxs-lookup"><span data-stu-id="c8831-193">These are more likely toolead tooan actual disaster.</span></span>


### <a name="random-failures-leading-tooservice-failures"></a><span data-ttu-id="c8831-194">Véletlenszerűen hibák vezető tooservice hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-194">Random failures leading tooservice failures</span></span>
<span data-ttu-id="c8831-195">Tegyük fel, hogy hello szolgáltatást kellett egy `InstanceCount` 5, és több csomópontok azokat a példányokat futtató összes sikertelen a következő hello azonos idő.</span><span class="sxs-lookup"><span data-stu-id="c8831-195">Let's say that hello service had an `InstanceCount` of 5, and several nodes running those instances all failed at hello same time.</span></span> <span data-ttu-id="c8831-196">A Service Fabric válaszol a többi csomóponton helyettesítő példányok automatikusan létrehozásával.</span><span class="sxs-lookup"><span data-stu-id="c8831-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span></span> <span data-ttu-id="c8831-197">Csere létrehozását, amíg hello szolgáltatás vissza tooits szükségeskonfiguráció-példányok száma továbbra is.</span><span class="sxs-lookup"><span data-stu-id="c8831-197">It will continue creating replacements until hello service is back tooits desired instance count.</span></span> <span data-ttu-id="c8831-198">Másik példaként, tegyük fel, az állapot nélküli szolgáltatás történt egy `InstanceCount`-1, ami azt jelenti, az érvényes hello fürt csomópontjaihoz fut.</span><span class="sxs-lookup"><span data-stu-id="c8831-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in hello cluster.</span></span> <span data-ttu-id="c8831-199">Tegyük fel, hogy a logikailemez némelyike volt toofail.</span><span class="sxs-lookup"><span data-stu-id="c8831-199">Let's say that some of those instances were toofail.</span></span> <span data-ttu-id="c8831-200">Ebben az esetben a Service Fabric megjegyzések hello szolgáltatás nem a megfelelő állapotban van, és megpróbál toocreate hello példányok hello csomópontján, ha hiányoznak.</span><span class="sxs-lookup"><span data-stu-id="c8831-200">In this case, Service Fabric notices that hello service is not in its desired state, and tries toocreate hello instances on hello nodes where they are missing.</span></span> 

<span data-ttu-id="c8831-201">Az állapotalapú szolgáltatások hello helyzet attól függ, hogy hello szolgáltatást rendelkezik megőrzött állapot vagy nem.</span><span class="sxs-lookup"><span data-stu-id="c8831-201">For stateful services hello situation depends on whether hello service has persisted state or not.</span></span> <span data-ttu-id="c8831-202">Is függ, hogy hány replikák hello szolgáltatást kellett, és hány sikertelen.</span><span class="sxs-lookup"><span data-stu-id="c8831-202">It also depends on how many replicas hello service had and how many failed.</span></span> <span data-ttu-id="c8831-203">Hogy egy olyan vészhelyzet esetén egy állapotalapú szolgáltatás történt, ezért a kezelése, a következő három szakaszban meghatározása:</span><span class="sxs-lookup"><span data-stu-id="c8831-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span></span>

1. <span data-ttu-id="c8831-204">Ha történt kvórum elvesztése vagy nem meghatározása</span><span class="sxs-lookup"><span data-stu-id="c8831-204">Determining if there has been quorum loss or not</span></span>
 - <span data-ttu-id="c8831-205">A kvórum elvesztése egy állapotalapú szolgáltatás hello replikák többsége nem működnek a hello bármikor egy időben, ideértve a elsődleges hello.</span><span class="sxs-lookup"><span data-stu-id="c8831-205">A quorum loss is any time a majority of hello replicas of a stateful service are down at hello same time, including hello Primary.</span></span>
2. <span data-ttu-id="c8831-206">Ha hello kvórum elvesztése állandó van-e vagy sem</span><span class="sxs-lookup"><span data-stu-id="c8831-206">Determining if hello quorum loss is permanent or not</span></span>
 - <span data-ttu-id="c8831-207">Legtöbbször ennek hello hibák átmeneti jellegűek.</span><span class="sxs-lookup"><span data-stu-id="c8831-207">Most of hello time, failures are transient.</span></span> <span data-ttu-id="c8831-208">Folyamatok újraindul, csomópont újraindul, virtuális gépek vannak s, hálózati partíciók javítandó.</span><span class="sxs-lookup"><span data-stu-id="c8831-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span></span> <span data-ttu-id="c8831-209">Egyes esetekben azonban hibák sem lesznek állandó.</span><span class="sxs-lookup"><span data-stu-id="c8831-209">Sometimes though, failures are permanent.</span></span> 
    - <span data-ttu-id="c8831-210">A szolgáltatások nélkül megőrzött állapot legalább egy kvórum-replikák eredmények hibát _azonnal_ állandó kvórumveszteségben.</span><span class="sxs-lookup"><span data-stu-id="c8831-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span></span> <span data-ttu-id="c8831-211">Service Fabric kvórum elvesztése állapot-nyilvántartó nem állandó szolgáltatás észleli, ha 3 toostep is deklarálni kell (esetleges) dataloss azonnal folytatódik.</span><span class="sxs-lookup"><span data-stu-id="c8831-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds toostep 3 by declaring (potential) dataloss.</span></span> <span data-ttu-id="c8831-212">A Folytatás toodataloss szabálykészletében, mert a Service Fabric ismeri az, hogy nincs-e a Várakozás hello replikák toocome vissza, a pont, mert még akkor is, ha helyre lett állítva azok lenne üres.</span><span class="sxs-lookup"><span data-stu-id="c8831-212">Proceeding toodataloss makes sense because Service Fabric knows that there's no point in waiting for hello replicas toocome back, because even if they were recovered they would be empty.</span></span>
    - <span data-ttu-id="c8831-213">Állapot-nyilvántartó állandó szolgáltatásokhoz legalább egy kvórum-replikák hiba hatására a Service Fabric toostart hello replikák toocome vissza és visszaállítási kvórum várakozás.</span><span class="sxs-lookup"><span data-stu-id="c8831-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric toostart waiting for hello replicas toocome back and restore quorum.</span></span> <span data-ttu-id="c8831-214">Bármely ennek eredményeként a szolgáltatáskimaradás _ír_ hatással toohello hello szolgáltatás partíció (vagy "replikakészlethez").</span><span class="sxs-lookup"><span data-stu-id="c8831-214">This results in a service outage for any _writes_ toohello affected partition (or "replica set") of hello service.</span></span> <span data-ttu-id="c8831-215">Előfordulhat azonban, olvasási továbbra is lehetséges a csökkentett konzisztencia biztosítja.</span><span class="sxs-lookup"><span data-stu-id="c8831-215">However, reads may still be possible with reduced consistency guarantees.</span></span> <span data-ttu-id="c8831-216">hello alapértelmezett, hogy mennyi ideig Service Fabric megvárja-e a kvórum toobe vissza azért végtelen, mert a Folytatás az (esetleges) dataloss esemény, és más kockázatokat hordoz magában.</span><span class="sxs-lookup"><span data-stu-id="c8831-216">hello default amount of time that Service Fabric waits for quorum toobe restored is infinite, since proceeding is a (potential) dataloss event and carries other risks.</span></span> <span data-ttu-id="c8831-217">Hello alapértelmezett felülbírálása `QuorumLossWaitDuration` érték lehet, de nem ajánlott.</span><span class="sxs-lookup"><span data-stu-id="c8831-217">Overriding hello default `QuorumLossWaitDuration` value is possible but is not recommended.</span></span> <span data-ttu-id="c8831-218">Ehelyett jelenleg összes kell erőfeszítéseket toorestore hello replikák le.</span><span class="sxs-lookup"><span data-stu-id="c8831-218">Instead at this time, all efforts should be made toorestore hello down replicas.</span></span> <span data-ttu-id="c8831-219">Ehhez a gépidőt hello csomópontot, amely le a biztonsági mentést, és győződjön meg arról, hogy azok is újracsatlakoztatni hello meghajtók hello helyi állandó állapot tárolásához.</span><span class="sxs-lookup"><span data-stu-id="c8831-219">This requires bringing hello nodes that are down back up, and ensuring that they can remount hello drives where they stored hello local persistent state.</span></span> <span data-ttu-id="c8831-220">Hello kvórum elvesztése folyamat hibája okozza, ha a Service Fabric automatikusan megpróbál toorecreate hello folyamatokat, és indítsa újra a bennük hello replikák.</span><span class="sxs-lookup"><span data-stu-id="c8831-220">If hello quorum loss is caused by process failure, Service Fabric automatically tries toorecreate hello processes and restart hello replicas inside them.</span></span> <span data-ttu-id="c8831-221">Ha ez nem sikerül, a Service Fabric állapot hibát jelez.</span><span class="sxs-lookup"><span data-stu-id="c8831-221">If this fails, Service Fabric reports health errors.</span></span> <span data-ttu-id="c8831-222">Ha ezek orvosolni tudja majd hello replikák általában térjen vissza.</span><span class="sxs-lookup"><span data-stu-id="c8831-222">If these can be resolved then hello replicas usually come back.</span></span> <span data-ttu-id="c8831-223">Egyes esetekben azonban hello replikák nem térhetnek vissza.</span><span class="sxs-lookup"><span data-stu-id="c8831-223">Sometimes, though, hello replicas can't be brought back.</span></span> <span data-ttu-id="c8831-224">Például hello meghajtók összes sikertelen volt, vagy hello gépek fizikailag megsemmisül valamilyen módon.</span><span class="sxs-lookup"><span data-stu-id="c8831-224">For example, hello drives may all have failed, or hello machines physically destroyed somehow.</span></span> <span data-ttu-id="c8831-225">Ebben az esetben egy állandó kvórum adatvesztés most van.</span><span class="sxs-lookup"><span data-stu-id="c8831-225">In these cases, we now have a permanent quorum loss event.</span></span> <span data-ttu-id="c8831-226">hello le ismét, replikák toocome a fürt rendszergazdája Várakozás tootell Service Fabric toostop meg kell határoznia, amelyek szolgáltatásokat érinti, és hívja meg hello partíciók `Repair-ServiceFabricPartition -PartitionId` vagy ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span><span class="sxs-lookup"><span data-stu-id="c8831-226">tootell Service Fabric toostop waiting for hello down replicas toocome back, a cluster administrator must determine which partitions of which services are affected and call hello `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span></span>  <span data-ttu-id="c8831-227">Ez az API lehetővé teszi, hogy hello partíció toomove QuorumLoss kívül, és a potenciális dataloss hello Azonosítójának megadása.</span><span class="sxs-lookup"><span data-stu-id="c8831-227">This API allows specifying hello ID of hello partition toomove out of QuorumLoss and into potential dataloss.</span></span>

> [!NOTE]
> <span data-ttu-id="c8831-228">Az _soha nem_ biztonságos toouse az API nem adott partíciókra elleni célzott módon.</span><span class="sxs-lookup"><span data-stu-id="c8831-228">It is _never_ safe toouse this API other than in a targeted way against specific partitions.</span></span> 
>

3. <span data-ttu-id="c8831-229">Meghatározhatja, hogy a tényleges adatvesztés lett, és a biztonsági mentésekből visszaállítása</span><span class="sxs-lookup"><span data-stu-id="c8831-229">Determining if there has been actual data loss, and restoring from backups</span></span>
  - <span data-ttu-id="c8831-230">Ha a Service Fabric meghívja a hello `OnDataLossAsync` miatt a rendszer mindig metódus _gyanús_ dataloss.</span><span class="sxs-lookup"><span data-stu-id="c8831-230">When Service Fabric calls hello `OnDataLossAsync` method it is always because of _suspected_ dataloss.</span></span> <span data-ttu-id="c8831-231">A Service Fabric biztosítja, hogy a hívás érkezik toohello _legjobb_ fennmaradó replika.</span><span class="sxs-lookup"><span data-stu-id="c8831-231">Service Fabric ensures that this call is delivered toohello _best_ remaining replica.</span></span> <span data-ttu-id="c8831-232">Ez a tetszőleges kiegészítendő replika tett hello legtöbb folyamatban van.</span><span class="sxs-lookup"><span data-stu-id="c8831-232">This is whichever replica has made hello most progress.</span></span> <span data-ttu-id="c8831-233">mindig fel ok hello _gyanús_ dataloss, lehetséges, hogy hello fennmaradó replika rendelkezik-e minden olyan állapotban, ahogy hello elsődleges amikor csökkent.</span><span class="sxs-lookup"><span data-stu-id="c8831-233">hello reason we always say _suspected_ dataloss is that it is possible that hello remaining replica actually has all same state as hello Primary did when it went down.</span></span> <span data-ttu-id="c8831-234">Azonban, hogy állapot toocompare nélkül azt, hogy nincs megfelelő mód a Service Fabric vagy operátorok tooknow biztosan.</span><span class="sxs-lookup"><span data-stu-id="c8831-234">However, without that state toocompare it to, there's no good way for Service Fabric or operators tooknow for sure.</span></span> <span data-ttu-id="c8831-235">Ezen a ponton a Service Fabric is tudja hello más replikák nem érkeznek vissza.</span><span class="sxs-lookup"><span data-stu-id="c8831-235">At this point, Service Fabric also knows hello other replicas are not coming back.</span></span> <span data-ttu-id="c8831-236">Hello döntés, ha azt várakozik hello kvórum elvesztése tooresolve magát, amely volt.</span><span class="sxs-lookup"><span data-stu-id="c8831-236">That was hello decision made when we stopped waiting for hello quorum loss tooresolve itself.</span></span> <span data-ttu-id="c8831-237">hello helyes intézkedéseket hello szolgáltatás az általában toofreeze és a megadott rendszergazdai beavatkozás várakozás.</span><span class="sxs-lookup"><span data-stu-id="c8831-237">hello best course of action for hello service is usually toofreeze and wait for specific administrative intervention.</span></span> <span data-ttu-id="c8831-238">Ezért funkciója hello tipikus végrehajtásának `OnDataLossAsync` metódus tegye?</span><span class="sxs-lookup"><span data-stu-id="c8831-238">So what does a typical implementation of hello `OnDataLossAsync` method do?</span></span>
  - <span data-ttu-id="c8831-239">Első lépésként jelentkezzen, amely `OnDataLossAsync` lett elindítva, és minden szükséges felügyeleti riasztások megkezdéséhez.</span><span class="sxs-lookup"><span data-stu-id="c8831-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span></span>
   - <span data-ttu-id="c8831-240">Általában ezen a ponton toopause és további döntések és manuális műveletek toobe végrehajtott várakozás.</span><span class="sxs-lookup"><span data-stu-id="c8831-240">Usually at this point, toopause and wait for further decisions and manual actions toobe taken.</span></span> <span data-ttu-id="c8831-241">Ennek oka az, akkor is, ha a biztonsági másolatok szükségük toobe előkészítve.</span><span class="sxs-lookup"><span data-stu-id="c8831-241">This is because even if backups are available they may need toobe prepared.</span></span> <span data-ttu-id="c8831-242">Például két különböző szolgáltatások koordinálja az adatokat, ha azokat a biztonsági mentések esetleg toobe, amely után a hello visszaállítás történik, hogy hello információt e két szolgáltatás érdeklik megfelelő sorrendben tooensure módosítva.</span><span class="sxs-lookup"><span data-stu-id="c8831-242">For example, if two different services coordinate information, those backups may need toobe modified in order tooensure that once hello restore happens that hello information those two services care about is consistent.</span></span> 
  - <span data-ttu-id="c8831-243">Gyakran van is néhány egyéb telemetriai vagy származó hello szolgáltatást.</span><span class="sxs-lookup"><span data-stu-id="c8831-243">Often there is also some other telemetry or exhaust from hello service.</span></span> <span data-ttu-id="c8831-244">A metaadatok tartalmazhatja az egyéb szolgáltatások vagy a naplókban.</span><span class="sxs-lookup"><span data-stu-id="c8831-244">This metadata may be contained in other services or in logs.</span></span> <span data-ttu-id="c8831-245">Ezeket az információkat lehet használt szükséges toodetermine, ha bármely hívások kapott és elsődleges hello feldolgozni, amely nincs jelen volt a biztonsági mentési vagy replikált toothis adott replika hello történt.</span><span class="sxs-lookup"><span data-stu-id="c8831-245">This information can be used needed toodetermine if there were any calls received and processed at hello primary that were not present in hello backup or replicated toothis particular replica.</span></span> <span data-ttu-id="c8831-246">Ezek esetleg toobe megismételt vagy hozzáadott toohello biztonsági másolat, előtt visszaállítás esetén valósítható meg.</span><span class="sxs-lookup"><span data-stu-id="c8831-246">These may need toobe replayed or added toohello backup before restoration is feasible.</span></span>  
   - <span data-ttu-id="c8831-247">A replika tartozó állapot toothat minden biztonsági másolatának elérhető szereplő fennmaradó hello összehasonlítást.</span><span class="sxs-lookup"><span data-stu-id="c8831-247">Comparisons of hello remaining replica's state toothat contained in any backups that are available.</span></span> <span data-ttu-id="c8831-248">Hello Service Fabric megbízható gyűjtemények használatával, akkor vannak eszközökhöz, és feldolgozza a úgy érhető el, ha ismertetett [Ez a cikk](service-fabric-reliable-services-backup-restore.md).</span><span class="sxs-lookup"><span data-stu-id="c8831-248">If using hello Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span></span> <span data-ttu-id="c8831-249">hello célja toosee Ha hello belül hello a replika állapota megfelelő, vagy milyen hello biztonsági mentés hiányozhat is.</span><span class="sxs-lookup"><span data-stu-id="c8831-249">hello goal is toosee if hello state within hello replica is sufficient, or also what hello backup may be missing.</span></span>
  - <span data-ttu-id="c8831-250">Egyszer hello összehasonlítás történik, és szükséges hello visszaállítás befejeződött, ha a hello szolgáltatást kód értéke true, ha az állapot módosítások kell visszaadnia.</span><span class="sxs-lookup"><span data-stu-id="c8831-250">Once hello comparison is done, and if necessary hello restore completed, hello service code should return true if any state changes were made.</span></span> <span data-ttu-id="c8831-251">Ha hello replika határozza meg, hogy hello legjobb elérhető példányát hello állapotban volt, és nem módosítja, majd térjen vissza hamis.</span><span class="sxs-lookup"><span data-stu-id="c8831-251">If hello replica determined that it was hello best available copy of hello state and made no changes, then return false.</span></span> <span data-ttu-id="c8831-252">Igaz érték azt jelenti, hogy minden _más_ replikák fennmaradó most lehet a erre.</span><span class="sxs-lookup"><span data-stu-id="c8831-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span></span> <span data-ttu-id="c8831-253">Azok lesz dobva, újraépíti a replikából.</span><span class="sxs-lookup"><span data-stu-id="c8831-253">They will be dropped and rebuilt from this replica.</span></span> <span data-ttu-id="c8831-254">Hamis azt jelzi, hogy nincs állapot változások, így hello más replikák biztosítható, azok rendelkeznek.</span><span class="sxs-lookup"><span data-stu-id="c8831-254">False indicates that no state changes were made, so hello other replicas can keep what they have.</span></span> 

<span data-ttu-id="c8831-255">Különösen fontos, hogy szolgáltatás szerzők gyakorlatban lehetséges dataloss és a hibát jelentő forgatókönyvek előtt szolgáltatás telepítve van a termelési.</span><span class="sxs-lookup"><span data-stu-id="c8831-255">It is critically important that service authors practice potential dataloss and failure scenarios before services are ever deployed in production.</span></span> <span data-ttu-id="c8831-256">tooprotect elleni dataloss hello lehetőségét, fontos tooperiodically [készítsen biztonsági másolatot hello állapot](service-fabric-reliable-services-backup-restore.md) tooa georedundáns tárolt állapotalapú szolgáltatások egyikét sem.</span><span class="sxs-lookup"><span data-stu-id="c8831-256">tooprotect against hello possibility of dataloss, it is important tooperiodically [back up hello state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services tooa geo-redundant store.</span></span> <span data-ttu-id="c8831-257">Azt is ellenőrizze, hogy rendelkezik-e hello képességét toorestore azt.</span><span class="sxs-lookup"><span data-stu-id="c8831-257">You must also ensure that you have hello ability toorestore it.</span></span> <span data-ttu-id="c8831-258">Mivel számos különböző szolgáltatások biztonsági mentést készít a különböző időpontokban, kell, hogy a visszaállítást követően a szolgáltatások rendelkeznek egymástól egységes megjelenítése tooensure.</span><span class="sxs-lookup"><span data-stu-id="c8831-258">Since backups of many different services are taken at different times, you need tooensure that after a restore your services have a consistent view of each other.</span></span> <span data-ttu-id="c8831-259">Vegye figyelembe például olyan helyzet, ahol egy szolgáltatás állít elő, egy szám és tárolja, majd elküldi tooanother szolgáltatás, amely is tárolja.</span><span class="sxs-lookup"><span data-stu-id="c8831-259">For example, consider a situation where one service generates a number and stores it, then sends it tooanother service that also stores it.</span></span> <span data-ttu-id="c8831-260">A visszaállítást követően felfedezheti, hogy rendelkezik hello második hello szolgáltatást, de hello először viszont nem, mert annak biztonsági mentési művelet nem tartozik.</span><span class="sxs-lookup"><span data-stu-id="c8831-260">After a restore, you might discover that hello second service has hello number but hello first does not, because it's backup didn't include that operation.</span></span>

<span data-ttu-id="c8831-261">Megállapítja, hogy hello fennmaradó replikák dataloss esetén a megfelelő toocontinue, és telemetriai vagy kipufogógáz szolgáltatás állapota nem helyreállítására, ha a biztonsági mentések gyakoriságát hello meghatározza, hogy a legjobb lehetséges helyreállítási időkorlát (RPO) .</span><span class="sxs-lookup"><span data-stu-id="c8831-261">If you find out that hello remaining replicas are insufficient toocontinue from in a dataloss scenario, and you can't reconstruct service state from telemetry or exhaust, hello frequency of your backups determines your best possible recovery point objective (RPO).</span></span> <span data-ttu-id="c8831-262">A Service Fabric számos eszközt kínál a tesztelési különböző sikertelen forgatókönyvek, köztük a végleges kvórum és dataloss van szükség egy biztonsági másolatból.</span><span class="sxs-lookup"><span data-stu-id="c8831-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and dataloss requiring restoration from a backup.</span></span> <span data-ttu-id="c8831-263">Ezek a forgatókönyvek szerepelnek a Service Fabric tesztelhetőségi eszközök hello hiba az Analysis Services által kezelt részeként.</span><span class="sxs-lookup"><span data-stu-id="c8831-263">These scenarios are included as a part of Service Fabric's testability tools, managed by hello Fault Analysis Service.</span></span> <span data-ttu-id="c8831-264">További információ ezen eszközök és a minták érhető [Itt](service-fabric-testability-overview.md).</span><span class="sxs-lookup"><span data-stu-id="c8831-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span></span> 

> [!NOTE]
> <span data-ttu-id="c8831-265">Rendszerszolgáltatások is hello terheli adott toohello szolgáltatás az adott a kvórum elvesztése esetén csökkenhet.</span><span class="sxs-lookup"><span data-stu-id="c8831-265">System services can also suffer quorum loss, with hello impact being specific toohello service in question.</span></span> <span data-ttu-id="c8831-266">Például hello a naming service a kvórum elvesztése hatással van a névfeloldás, mivel a kvórum elvesztése a Feladatátvevőfürt-kezelő szolgáltatás hello blokkolja az új szolgáltatás létrehozása és a feladatátvétel.</span><span class="sxs-lookup"><span data-stu-id="c8831-266">For instance, quorum loss in hello naming service impacts name resolution, whereas quorum loss in hello failover manager service blocks new service creation and failovers.</span></span> <span data-ttu-id="c8831-267">Hello Service Fabric rendszerszolgáltatások hello ugyanaz az állapotkezelést a szolgáltatásként mintát követi, amíg nem ajánlott, hogy meg kell próbálni toomove kívül a kvórum elvesztése, valamint a potenciális dataloss őket.</span><span class="sxs-lookup"><span data-stu-id="c8831-267">While hello Service Fabric system services follow hello same pattern as your services for state management, it is not recommended that you should attempt toomove them out of Quorum Loss and into potential dataloss.</span></span> <span data-ttu-id="c8831-268">hello javasoljuk túl, hanem[támogatási pozícionálni](service-fabric-support.md) olyan megoldás, amely toodetermine céloz tooyour adott helyzet.</span><span class="sxs-lookup"><span data-stu-id="c8831-268">hello recommendation is instead too[seek support](service-fabric-support.md) toodetermine a solution that is targeted tooyour specific situation.</span></span>  <span data-ttu-id="c8831-269">Általában akkor előnyösebb toosimply várakozási amíg hello visszatérési replikák le.</span><span class="sxs-lookup"><span data-stu-id="c8831-269">Usually it is preferable toosimply wait until hello down replicas return.</span></span>
>

## <a name="availability-of-hello-service-fabric-cluster"></a><span data-ttu-id="c8831-270">Service Fabric-fürt hello rendelkezésre állása</span><span class="sxs-lookup"><span data-stu-id="c8831-270">Availability of hello Service Fabric cluster</span></span>
<span data-ttu-id="c8831-271">Hello Service Fabric-fürt maga általánosságban véve nem meghibásodási ponttal rendelkező magas elosztott környezetben.</span><span class="sxs-lookup"><span data-stu-id="c8831-271">Generally speaking, hello Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span></span> <span data-ttu-id="c8831-272">Bármely egy csomópont hiba nem okoz rendelkezésre állás vagy a megbízhatósági hibákat elhárító hello fürt elsősorban, mert hello Service Fabric rendszerszolgáltatások hajtsa végre a korábban megadott azonos irányelvek hello: mindig hibaüzenettel három vagy több replikák alapértelmezés szerint, és azokat állapot nélküli rendszer-szolgáltatásokat futtatni az összes olyan csomóponton.</span><span class="sxs-lookup"><span data-stu-id="c8831-272">A failure of any one node will not cause availability or reliability issues for hello cluster, primarily because hello Service Fabric system services follow hello same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span></span> <span data-ttu-id="c8831-273">hello alapul szolgáló Service Fabric hálózatkezelési és hiba észlelése rétegek teljesen terjesztése.</span><span class="sxs-lookup"><span data-stu-id="c8831-273">hello underlying Service Fabric networking and failure detection layers are fully distributed.</span></span> <span data-ttu-id="c8831-274">A legtöbb rendszerszolgáltatások úgy is, a metaadatok hello fürt, vagy tudja, hogyan tooresynchronize állapotukra más helyeiről.</span><span class="sxs-lookup"><span data-stu-id="c8831-274">Most system services can be rebuilt from metadata in hello cluster, or know how tooresynchronize their state from other places.</span></span> <span data-ttu-id="c8831-275">hello fürt hello rendelkezésre állását képes lesz biztonsága sérül, ha rendszerszolgáltatások feltölti a kvórum elvesztése helyzetek például szerepel a fenti.</span><span class="sxs-lookup"><span data-stu-id="c8831-275">hello availability of hello cluster can become compromised if system services get into quorum loss situations like those described above.</span></span> <span data-ttu-id="c8831-276">Ezekben az esetekben nem lehet bizonyos hello fürtön műveletek, például egy frissítés elindítása vagy új szolgáltatások telepítése, de hello fürt továbbra is fel tud tooperform.</span><span class="sxs-lookup"><span data-stu-id="c8831-276">In these cases you may not be able tooperform certain operations on hello cluster like starting an upgrade or deploying new services, but hello cluster itself is still up.</span></span> <span data-ttu-id="c8831-277">Már fut a szolgáltatás fut, ezek a feltételek a marad, kivéve, ha szükségük van az írási műveletek toohello rendszer szolgáltatások toocontinue működik.</span><span class="sxs-lookup"><span data-stu-id="c8831-277">Services on already running will remain running in these conditions unless they require writes toohello system services toocontinue functioning.</span></span> <span data-ttu-id="c8831-278">Például ha a Feladatátvevőfürt-kezelő hello kvórumveszteségben van minden szolgáltatás toorun folytatódik, de bármely eleget nem tevő szolgáltatás nem lesz képes tooautomatically újraindítást, mivel ehhez a Feladatátvevőfürt-kezelő hello hello bevonása.</span><span class="sxs-lookup"><span data-stu-id="c8831-278">For example, if hello Failover Manager is in quorum loss all services will continue toorun, but any services that fail will not be able tooautomatically restart, since this requires hello involvement of hello Failover Manager.</span></span> 

### <a name="failures-of-a-datacenter-or-azure-region"></a><span data-ttu-id="c8831-279">A datacenter vagy az Azure-régió hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-279">Failures of a datacenter or Azure region</span></span>
<span data-ttu-id="c8831-280">Bizonyos ritkán előforduló esetekben, átmenetileg nem érhető el, miatt válhat a fizikai adatközpont tooloss a teljesítmény vagy a hálózati kapcsolat.</span><span class="sxs-lookup"><span data-stu-id="c8831-280">In rare cases, a physical data center can become temporarily unavailable due tooloss of power or network connectivity.</span></span> <span data-ttu-id="c8831-281">Ezekben az esetekben a Service Fabric-fürtök és a szolgáltatások, az adott datacenter vagy az Azure-régió nem lesz elérhető.</span><span class="sxs-lookup"><span data-stu-id="c8831-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span></span> <span data-ttu-id="c8831-282">Azonban _az adatok megmaradjanak_.</span><span class="sxs-lookup"><span data-stu-id="c8831-282">However, _your data is preserved_.</span></span> <span data-ttu-id="c8831-283">Az Azure-ban futó fürtök esetén a hello kimaradások megtekintheti a frissítések [Azure állapotlapon][azure-status-dashboard].</span><span class="sxs-lookup"><span data-stu-id="c8831-283">For clusters running in Azure, you can view updates on outages on hello [Azure status page][azure-status-dashboard].</span></span> <span data-ttu-id="c8831-284">A hello nagyon valószínű esemény, amely a fizikai adatközpont részlegesen vagy teljesen megsemmisül, a Service Fabric-fürtök üzemeltetett van, vagy bennük hello szolgáltatások elveszhet.</span><span class="sxs-lookup"><span data-stu-id="c8831-284">In hello highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or hello services inside them could be lost.</span></span> <span data-ttu-id="c8831-285">Ez magában foglalja a bármely kívül adatközpontját, illetve a régió nem biztonsági állapotát.</span><span class="sxs-lookup"><span data-stu-id="c8831-285">This includes any state not backed up outside of that datacenter or region.</span></span>

<span data-ttu-id="c8831-286">Nincs a két különböző stratégiák a még működő hello egyetlen datacenter vagy terület végleges vagy tartós hiba.</span><span class="sxs-lookup"><span data-stu-id="c8831-286">There's two different strategies for surviving hello permanent or sustained failure of a single datacenter or region.</span></span> 

1. <span data-ttu-id="c8831-287">Különálló Service Fabric-fürtök futtassa több régióba, és a feladatátvétel és visszavenni a feladatokat a különböző környezetek között valamilyen mód használatára.</span><span class="sxs-lookup"><span data-stu-id="c8831-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span></span> <span data-ttu-id="c8831-288">Az ilyen több fürt aktív-aktív vagy aktív-passzív modell felügyelete és műveletei kód szükséges.</span><span class="sxs-lookup"><span data-stu-id="c8831-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span></span> <span data-ttu-id="c8831-289">Ehhez is készített biztonsági másolat egy adatközpontban, illetve a régió hello szolgáltatások összehangolását, hogy elérhetők más adatközpontok vagy régiókban, ha egy sikertelen lesz.</span><span class="sxs-lookup"><span data-stu-id="c8831-289">This also requires coordination of backups from hello services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span></span> 
2. <span data-ttu-id="c8831-290">Futtassa egy egyetlen Service Fabric-fürt több adatközpontban vagy régiókból is.</span><span class="sxs-lookup"><span data-stu-id="c8831-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span></span> <span data-ttu-id="c8831-291">hello támogatott konfigurációkra vonatkozó minimális ez három adatközpontban vagy régióban.</span><span class="sxs-lookup"><span data-stu-id="c8831-291">hello minimum supported configuration for this is three datacenters or regions.</span></span> <span data-ttu-id="c8831-292">hello javasolt régiók száma vagy adatközpontok öt.</span><span class="sxs-lookup"><span data-stu-id="c8831-292">hello recommended number of regions or datacenters is five.</span></span> <span data-ttu-id="c8831-293">Ehhez egy összetettebb fürtjének topológiája.</span><span class="sxs-lookup"><span data-stu-id="c8831-293">This requires a more complex cluster topology.</span></span> <span data-ttu-id="c8831-294">Azonban hello Ez a modell előnye, hogy egy datacenter vagy régió sikertelen alakul át egy olyan vészhelyzet esetén a normál hiba.</span><span class="sxs-lookup"><span data-stu-id="c8831-294">However, hello benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span></span> <span data-ttu-id="c8831-295">Ezek a hibák kezelhetik hello mechanizmusokat, amelyek működnek a fürtök egyetlen régión belül.</span><span class="sxs-lookup"><span data-stu-id="c8831-295">These failures can be handled by hello mechanisms that work for clusters within a single region.</span></span> <span data-ttu-id="c8831-296">Tartalék tartományok, a frissítési tartományok és a Service Fabric elhelyezési szabályokat győződjön meg arról, munkaterhelések terjesztése, hogy azok működését normál hibák.</span><span class="sxs-lookup"><span data-stu-id="c8831-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span></span> <span data-ttu-id="c8831-297">Amely segít az ilyen típusú fürt szolgáltatások működtetéséhez házirendekkel kapcsolatos további információkért olvassa a [elhelyezési házirendeket](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span><span class="sxs-lookup"><span data-stu-id="c8831-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span></span>

### <a name="random-failures-leading-toocluster-failures"></a><span data-ttu-id="c8831-298">Véletlenszerűen hibák vezető toocluster hibák</span><span class="sxs-lookup"><span data-stu-id="c8831-298">Random failures leading toocluster failures</span></span>
<span data-ttu-id="c8831-299">A Service Fabric rendelkezik kezdőérték csomópontok hello fogalmát.</span><span class="sxs-lookup"><span data-stu-id="c8831-299">Service Fabric has hello concept of Seed Nodes.</span></span> <span data-ttu-id="c8831-300">Ezek a csomópontokra, amelyeket az alapul szolgáló fürt hello hello rendelkezésre állását fenntartásához.</span><span class="sxs-lookup"><span data-stu-id="c8831-300">These are nodes that maintain hello availability of hello underlying cluster.</span></span> <span data-ttu-id="c8831-301">Ezek a csomópontok segíthet az tooensure hello fürt továbbra is másolatot létrehozó további csomópontokkal bérleteket és bizonyos típusú hálózati hibák során tiebreakers szolgál.</span><span class="sxs-lookup"><span data-stu-id="c8831-301">These nodes help tooensure hello cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span></span> <span data-ttu-id="c8831-302">Ha a véletlenszerűen hibák hello kezdőérték csomópontok többsége eltávolítása hello fürt, és azok nem teszik vissza, hello fürt automatikusan leáll.</span><span class="sxs-lookup"><span data-stu-id="c8831-302">If random failures remove a majority of hello seed nodes in hello cluster and they are not brought back, hello cluster automatically shuts down.</span></span> <span data-ttu-id="c8831-303">Az Azure kezdőérték csomópontok automatikusan kezelt: hello elérhető tartalék és verziófrissítési keresztül továbbítja, és egyetlen kezdőérték csomópont eltávolításakor hello fürtből létrejön-e egy másik helyén.</span><span class="sxs-lookup"><span data-stu-id="c8831-303">In Azure, Seed Nodes are automatically managed: they are distributed over hello available fault and upgrade domains, and if a single seed node is removed from hello cluster another one will be created in its place.</span></span> 

<span data-ttu-id="c8831-304">A különálló Service Fabric-fürtök és a Azure "Elsődleges csomópont Type" hello egy hello magok futtató hello.</span><span class="sxs-lookup"><span data-stu-id="c8831-304">In both standalone Service Fabric clusters and Azure, hello "Primary Node Type" is hello one that runs hello seeds.</span></span> <span data-ttu-id="c8831-305">Egy elsődleges csomóponttípusok meghatározásakor a Service Fabric lesz automatikusan előnyeit hello too9 kezdőérték csomópontok és az egyes hello rendszerszolgáltatások 9 replikák létrehozott csomópontok száma.</span><span class="sxs-lookup"><span data-stu-id="c8831-305">When defining a primary node type, Service Fabric will automatically take advantage of hello number of nodes provided by creating up too9 seed nodes and 9 replicas of each of hello system services.</span></span> <span data-ttu-id="c8831-306">Ha egy véletlenszerűen hibák készletét ki ezeket a rendszer szolgáltatás replikák többsége fogad adatokat egyidejűleg, hello rendszerszolgáltatások lép kvórum elvesztése, azt a fent leírt módon.</span><span class="sxs-lookup"><span data-stu-id="c8831-306">If a set of random failures takes out a majority of those system service replicas simultaneously, hello system services will enter quorum loss, as we described above.</span></span> <span data-ttu-id="c8831-307">Hello kezdőérték csomópontok többsége elvesznek, ha hello fürt le fog állni hamarosan után.</span><span class="sxs-lookup"><span data-stu-id="c8831-307">If a majority of hello seed nodes are lost, hello cluster will shut down soon after.</span></span>

## <a name="next-steps"></a><span data-ttu-id="c8831-308">Következő lépések</span><span class="sxs-lookup"><span data-stu-id="c8831-308">Next steps</span></span>
- <span data-ttu-id="c8831-309">Megtudhatja, hogyan toosimulate hello segítségével különféle hibák [tesztelhetőségi keretrendszer](service-fabric-testability-overview.md)</span><span class="sxs-lookup"><span data-stu-id="c8831-309">Learn how toosimulate various failures using hello [testability framework](service-fabric-testability-overview.md)</span></span>
- <span data-ttu-id="c8831-310">Olvassa el a vész-helyreállítási és magas rendelkezésre állású erőforrását.</span><span class="sxs-lookup"><span data-stu-id="c8831-310">Read other disaster-recovery and high-availability resources.</span></span> <span data-ttu-id="c8831-311">A Microsoft tett közzé útmutatást nagy mennyiségű ezekben a kérdésekben.</span><span class="sxs-lookup"><span data-stu-id="c8831-311">Microsoft has published a large amount of guidance on these topics.</span></span> <span data-ttu-id="c8831-312">Közben ezeket a dokumentumokat némelyike toospecific technikák használható egyéb termékek, sok általános gyakorlati tanácsok hello Service Fabric környezetben is alkalmazhat tartalmazzák:</span><span class="sxs-lookup"><span data-stu-id="c8831-312">While some of these documents refer toospecific techniques for use in other products, they contain many general best practices you can apply in hello Service Fabric context as well:</span></span>
  - [<span data-ttu-id="c8831-313">Rendelkezésre állási ellenőrzőlista</span><span class="sxs-lookup"><span data-stu-id="c8831-313">Availability checklist</span></span>](../best-practices-availability-checklist.md)
  - [<span data-ttu-id="c8831-314">A vész-helyreállítási részletezési végrehajtása</span><span class="sxs-lookup"><span data-stu-id="c8831-314">Performing a disaster recovery drill</span></span>](../sql-database/sql-database-disaster-recovery-drills.md)
  - <span data-ttu-id="c8831-315">[Vész-helyreállítási és magas rendelkezésre állás a Azure-alkalmazások][dr-ha-guide]</span><span class="sxs-lookup"><span data-stu-id="c8831-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span></span>
- <span data-ttu-id="c8831-316">A [Service Fabric támogatási lehetőségeinek](service-fabric-support.md) ismertetése</span><span class="sxs-lookup"><span data-stu-id="c8831-316">Learn about [Service Fabric support options](service-fabric-support.md)</span></span>

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
